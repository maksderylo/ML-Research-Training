{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SAE implementations",
   "id": "c36299e8cc347f22"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-20T19:26:49.230301Z",
     "start_time": "2025-10-20T19:26:49.223537Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=64, k_top=20):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.training = True\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.k_top = k_top\n",
    "        self.name = \"Default Sparse Autoencoder\"\n",
    "\n",
    "        # Encoder maps input to hidden representation\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Decoder maps hidden representation back to input space\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def _topk_mask(self, activations: torch.Tensor) -> torch.Tensor:\n",
    "        # activations: (batch, hidden)\n",
    "        k = max(0, min(self.k_top, activations.size(1)))\n",
    "        _, idx = torch.topk(activations, k, dim=1)\n",
    "        mask = torch.zeros_like(activations)\n",
    "        mask.scatter_(1, idx, 1.0)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        pre_activations = self.encoder(x)\n",
    "        pre_activations = F.relu(pre_activations)\n",
    "        mask = self._topk_mask(pre_activations)\n",
    "        h = pre_activations * mask\n",
    "        x_hat = self.decoder(h)\n",
    "        return h, x_hat\n",
    "\n",
    "\n",
    "    def compute_loss(self, x, h, x_hat):\n",
    "        # We compute sum of squares and normalize by input dimension 'd'\n",
    "        recon_loss = torch.sum((x - x_hat) ** 2) / (x.size(0) * self.input_size)\n",
    "\n",
    "        return recon_loss"
   ],
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class SparseAutoencoderInit(SparseAutoencoder):\n",
    "    def __init__(self, input_size=784, hidden_size=64, k_top=20):\n",
    "        super(SparseAutoencoderInit, self).__init__(input_size, hidden_size, k_top)\n",
    "\n",
    "        self.name = \"Sparse Autoencoder with just weight initialization\"\n",
    "        # Initialize encoder weights first with random directions\n",
    "        nn.init.kaiming_uniform_(self.encoder.weight, a=math.sqrt(5))\n",
    "        # Initialize the decoder to be the transpose of the encoder weights\n",
    "        with torch.no_grad():\n",
    "            self.decoder.weight.copy_(self.encoder.weight.t())\n"
   ],
   "id": "a70c6493dc195d4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class SparseAutoencoderJumpReLU(SparseAutoencoder):\n",
    "    def __init__(self, input_size=784, hidden_size=64, k_top=20, jump_value=0.1):\n",
    "        super(SparseAutoencoderJumpReLU, self).__init__(input_size, hidden_size, k_top)\n",
    "        self.name = \"Sparse Autoencoder with Jump ReLU\"\n",
    "        self.jump_value = jump_value\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h_raw = self.encoder(x)\n",
    "        mask = self._topk_mask(h_raw)\n",
    "        h = h_raw * mask\n",
    "        # Apply JumpReLU\n",
    "        h = torch.where(h > self.jump_value, h, torch.zeros_like(h))\n",
    "        x_hat = self.decoder(h)\n",
    "        return h, x_hat"
   ],
   "id": "5d8d293203ac3d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class SparseAutoencoderInitJumpReLU(SparseAutoencoder):\n",
    "    def __init__(self, input_size=784, hidden_size=64, k_top=20, jump_value=0.1):\n",
    "        super(SparseAutoencoderInitJumpReLU, self).__init__(input_size, hidden_size, k_top)\n",
    "        self.name = \"Sparse Autoencoder with Initialization and Jump ReLU\"\n",
    "        self.jump_value = jump_value\n",
    "\n",
    "        # Initialize encoder weights first with random directions\n",
    "        nn.init.kaiming_uniform_(self.encoder.weight, a=math.sqrt(5))\n",
    "        # Initialize the decoder to be the transpose of the encoder weights\n",
    "        with torch.no_grad():\n",
    "            self.decoder.weight.copy_(self.encoder.weight.t())\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h_raw = self.encoder(x)\n",
    "        mask = self._topk_mask(h_raw)\n",
    "        h = h_raw * mask\n",
    "        # Apply JumpReLU\n",
    "        h = torch.where(h > self.jump_value, h, torch.zeros_like(h))\n",
    "        x_hat = self.decoder(h)\n",
    "        return h, x_hat"
   ],
   "id": "aa80385d49115bf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Implementing auxiliary loss SAE",
   "id": "b565b8d2503ed656"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T18:01:16.936310Z",
     "start_time": "2025-10-20T18:01:16.912256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class SparseAutoencoderAuxLoss(SparseAutoencoder):\n",
    "    def __init__(self, input_size, hidden_size, k_top, k_aux, k_aux_param, dead_feature_threshold):\n",
    "        super(SparseAutoencoderAuxLoss, self).__init__(input_size, hidden_size, k_top)\n",
    "        self.name = \"Sparse Autoencoder with Auxiliary Loss\"\n",
    "        # k_aux is typically 2*k or more to revive dead features\n",
    "        self.k_aux = k_aux if k_aux is not None else 2 * k_top\n",
    "        self.k_aux_param = k_aux_param\n",
    "        # Track dead features: count steps since each feature was last active\n",
    "        self.register_buffer('steps_since_active', torch.zeros(hidden_size))\n",
    "        self.dead_feature_threshold = dead_feature_threshold\n",
    "\n",
    "    # Function to track which features are dead\n",
    "    def _update_dead_features(self, h: torch.Tensor):\n",
    "        # Feature is active if ANY sample in batch activates it\n",
    "        active_mask = (h.abs() > 1e-8).any(dim=0)\n",
    "\n",
    "        # Increment counter for inactive features, reset for active ones\n",
    "        self.steps_since_active += 1\n",
    "        self.steps_since_active[active_mask] = 0\n",
    "\n",
    "    def _get_dead_feature_mask(self) -> torch.Tensor:\n",
    "        \"\"\"Return boolean mask of dead features\"\"\"\n",
    "        return self.steps_since_active > self.dead_feature_threshold\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h_raw = self.encoder(x)\n",
    "        mask = self._topk_mask(h_raw)\n",
    "        h = h_raw * mask\n",
    "        x_hat = self.decoder(h)\n",
    "\n",
    "        # Track dead features during training\n",
    "        if self.training:\n",
    "            self._update_dead_features(h)\n",
    "\n",
    "        return h, x_hat\n",
    "\n",
    "    def compute_loss(self, x, h, x_hat):\n",
    "        # Main reconstruction loss\n",
    "        recon_error = torch.sum((x - x_hat) ** 2)\n",
    "        recon_loss = recon_error / self.input_size\n",
    "\n",
    "        # Auxiliary loss using dead features only\n",
    "        aux_loss = torch.tensor(0.0, device=x.device)\n",
    "\n",
    "        if self.training:\n",
    "            dead_mask = self._get_dead_feature_mask()  # (hidden_size,)\n",
    "            n_dead = dead_mask.sum().item()\n",
    "\n",
    "            if n_dead > 0:\n",
    "                # Compute reconstruction error: e = x - x_hat\n",
    "                recon_error_vec = x - x_hat  # (batch, input_size)\n",
    "\n",
    "                # Get raw activations again (before TopK masking)\n",
    "                with torch.no_grad():\n",
    "                    h_raw = self.encoder(x)\n",
    "\n",
    "                # Select only dead features\n",
    "                h_dead = h_raw * dead_mask.float().unsqueeze(0)  # (batch, hidden_size)\n",
    "\n",
    "                # Select top-k_aux dead features\n",
    "                k_aux_features = min(self.k_aux, n_dead)\n",
    "                _, idx_aux = torch.topk(h_dead, k_aux_features, dim=1)\n",
    "                mask_aux = torch.zeros_like(h_dead)\n",
    "                mask_aux.scatter_(1, idx_aux, 1.0)\n",
    "\n",
    "                # Sparse activations using only dead features\n",
    "                z_aux = h_raw * mask_aux  # (batch, hidden_size)\n",
    "\n",
    "                # Reconstruct error using dead features\n",
    "                e_hat = self.decoder(z_aux)  # (batch, input_size)\n",
    "\n",
    "                # Auxiliary loss: ||e - e_hat||^2\n",
    "                aux_loss = torch.sum((recon_error_vec - e_hat) ** 2) / self.input_size\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = recon_loss + self.k_aux_param * aux_loss\n",
    "\n",
    "        return total_loss, recon_loss, aux_loss"
   ],
   "id": "72f4004d2c87ef37",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Complete with relu, init and aux loss implementation.",
   "id": "bcf91031d67e0628"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T18:01:16.973663Z",
     "start_time": "2025-10-20T18:01:16.963015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class SparseAutoencoderComplete(SparseAutoencoder):\n",
    "    def __init__(self, input_size, hidden_size, k_top, k_aux, k_aux_param, dead_feature_threshold, jump_value):\n",
    "        super(SparseAutoencoderComplete, self).__init__(input_size, hidden_size, k_top)\n",
    "        self.name = \"Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss\"\n",
    "        self.jump_value = jump_value\n",
    "\n",
    "        # k_aux is typically 2*k or more to revive dead features\n",
    "        self.k_aux = k_aux if k_aux is not None else 2 * k_top\n",
    "        self.k_aux_param = k_aux_param\n",
    "        # Track dead features: count steps since each feature was last active\n",
    "        self.register_buffer('steps_since_active', torch.zeros(hidden_size))\n",
    "        self.dead_feature_threshold = dead_feature_threshold\n",
    "\n",
    "        # Initialize encoder weights first with random directions\n",
    "        nn.init.kaiming_uniform_(self.encoder.weight, a=math.sqrt(5))\n",
    "        # Initialize the decoder to be the transpose of the encoder weights\n",
    "        with torch.no_grad():\n",
    "            self.decoder.weight.copy_(self.encoder.weight.t())\n",
    "\n",
    "    # Function to track which features are dead\n",
    "    def _update_dead_features(self, h: torch.Tensor):\n",
    "        # Feature is active if ANY sample in batch activates it\n",
    "        active_mask = (h.abs() > 1e-8).any(dim=0)\n",
    "\n",
    "        # Increment counter for inactive features, reset for active ones\n",
    "        self.steps_since_active += 1\n",
    "        self.steps_since_active[active_mask] = 0\n",
    "\n",
    "    def _get_dead_feature_mask(self) -> torch.Tensor:\n",
    "        \"\"\"Return boolean mask of dead features\"\"\"\n",
    "        return self.steps_since_active > self.dead_feature_threshold\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h_raw = self.encoder(x)\n",
    "        mask = self._topk_mask(h_raw)\n",
    "        h = h_raw * mask\n",
    "        # Apply JumpReLU\n",
    "        h = torch.where(h > self.jump_value, h, torch.zeros_like(h))\n",
    "        x_hat = self.decoder(h)\n",
    "\n",
    "        # Track dead features during training\n",
    "        if self.training:\n",
    "            self._update_dead_features(h)\n",
    "\n",
    "        return h, x_hat\n",
    "\n",
    "    def compute_loss(self, x, h, x_hat):\n",
    "        # Main reconstruction loss\n",
    "        recon_error = torch.sum((x - x_hat) ** 2)\n",
    "        recon_loss = recon_error / self.input_size\n",
    "\n",
    "        # Auxiliary loss using dead features only\n",
    "        aux_loss = torch.tensor(0.0, device=x.device)\n",
    "\n",
    "        if self.training:\n",
    "            dead_mask = self._get_dead_feature_mask()  # (hidden_size,)\n",
    "            n_dead = dead_mask.sum().item()\n",
    "\n",
    "            if n_dead > 0:\n",
    "                # Compute reconstruction error: e = x - x_hat\n",
    "                recon_error_vec = x - x_hat  # (batch, input_size)\n",
    "\n",
    "                # Get raw activations again (before TopK masking)\n",
    "                with torch.no_grad():\n",
    "                    h_raw = self.encoder(x)\n",
    "\n",
    "                # Select only dead features\n",
    "                h_dead = h_raw * dead_mask.float().unsqueeze(0)  # (batch, hidden_size)\n",
    "\n",
    "                # Select top-k_aux dead features\n",
    "                k_aux_features = min(self.k_aux, n_dead)\n",
    "                _, idx_aux = torch.topk(h_dead, k_aux_features, dim=1)\n",
    "                mask_aux = torch.zeros_like(h_dead)\n",
    "                mask_aux.scatter_(1, idx_aux, 1.0)\n",
    "\n",
    "                # Sparse activations using only dead features\n",
    "                z_aux = h_raw * mask_aux  # (batch, hidden_size)\n",
    "\n",
    "                # Reconstruct error using dead features\n",
    "                e_hat = self.decoder(z_aux)  # (batch, input_size)\n",
    "\n",
    "                # Auxiliary loss: ||e - e_hat||^2\n",
    "                aux_loss = torch.sum((recon_error_vec - e_hat) ** 2) / self.input_size\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = recon_loss + self.k_aux_param * aux_loss\n",
    "\n",
    "        return total_loss, recon_loss, aux_loss"
   ],
   "id": "6b95f44957f5b99f",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Loading and Preprocessing",
   "id": "1c8bfa281ef7655d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T18:01:17.026542Z",
     "start_time": "2025-10-20T18:01:17.018489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import optim\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, Subset\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def load_mnist_data(batch_size=256):\n",
    "    # First load raw data to compute mean\n",
    "    raw_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Converts to [0,1] and creates tensor\n",
    "    ])\n",
    "\n",
    "    # Load training set to compute mean\n",
    "    trainset_raw = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                              download=True, transform=raw_transform)\n",
    "\n",
    "    # Compute mean over entire training set\n",
    "    train_loader_temp = DataLoader(trainset_raw, batch_size=len(trainset_raw), shuffle=False)\n",
    "    all_data = next(iter(train_loader_temp))[0]\n",
    "    all_data = all_data.view(all_data.size(0), -1)  # Flatten to (N, 784)\n",
    "    dataset_mean = all_data.mean(dim=0)  # Mean across samples, shape (784,)\n",
    "\n",
    "    # Define preprocessing transform with mean subtraction and normalization\n",
    "    def preprocess(x):\n",
    "        x_flat = x.view(-1)  # Flatten from (1, 28, 28) to (784,)\n",
    "        x_centered = x_flat - dataset_mean  # Subtract mean\n",
    "        x_norm = x_centered / (torch.norm(x_centered) + 1e-8)  # Normalize to unit norm\n",
    "        return x_norm\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(preprocess)\n",
    "    ])\n",
    "\n",
    "    # Load datasets with proper preprocessing\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                          download=True, transform=transform)\n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, dataset_mean"
   ],
   "id": "9dffec9ed378ec9c",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T18:01:17.080639Z",
     "start_time": "2025-10-20T18:01:17.073888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def load_olivetti_data(batch_size=32, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Load Olivetti Faces dataset (400 images, 64x64 grayscale)\n",
    "    Returns data with shape (N, 4096) after flattening\n",
    "    \"\"\"\n",
    "    # Download Olivetti Faces using sklearn\n",
    "    faces = fetch_olivetti_faces(shuffle=True, random_state=42)\n",
    "    data = faces.data  # Already normalized to [0, 1], shape (400, 4096)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    data_tensor = torch.FloatTensor(data)  # Shape: (400, 4096)\n",
    "\n",
    "    # Compute mean over entire dataset\n",
    "    dataset_mean = data_tensor.mean(dim=0)  # Shape: (4096,)\n",
    "\n",
    "    # Define preprocessing function\n",
    "    def preprocess(x):\n",
    "        x_centered = x - dataset_mean  # Subtract mean\n",
    "        x_norm = x_centered / (torch.norm(x_centered) + 1e-8)  # Unit norm\n",
    "        return x_norm\n",
    "\n",
    "    # Apply preprocessing to all data\n",
    "    preprocessed_data = torch.stack([preprocess(x) for x in data_tensor])\n",
    "\n",
    "    # Create dataset (no labels needed for autoencoder)\n",
    "    dataset = TensorDataset(preprocessed_data)\n",
    "\n",
    "    # Split into train/test\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size],\n",
    "                                     generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, dataset_mean"
   ],
   "id": "d600891fa864567",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T18:01:17.136362Z",
     "start_time": "2025-10-20T18:01:17.127018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def load_imagenet_subset(batch_size=128, subset_size=10000, img_size=64,\n",
    "                         data_root='./data/imagenet'):\n",
    "    \"\"\"\n",
    "    Load a subset of ImageNet with preprocessing\n",
    "\n",
    "    Args:\n",
    "        batch_size: Batch size for DataLoader\n",
    "        subset_size: Number of images to use (None for full dataset)\n",
    "        img_size: Resize images to (img_size, img_size)\n",
    "        data_root: Path to ImageNet data directory\n",
    "\n",
    "    Returns:\n",
    "        train_loader, test_loader, dataset_mean\n",
    "    \"\"\"\n",
    "\n",
    "    # Raw transform for computing mean\n",
    "    raw_transform = transforms.Compose([\n",
    "        transforms.Resize(img_size + 8),  # Slightly larger for center crop\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.Grayscale(),  # Convert to grayscale for consistency\n",
    "        transforms.ToTensor(),  # Converts to [0,1]\n",
    "    ])\n",
    "\n",
    "    # Load training set\n",
    "    try:\n",
    "        trainset_raw = torchvision.datasets.ImageNet(\n",
    "            root=data_root,\n",
    "            split='train',\n",
    "            transform=raw_transform\n",
    "        )\n",
    "    except:\n",
    "        # Alternative: Use ImageFolder if you have custom subset\n",
    "        trainset_raw = torchvision.datasets.ImageFolder(\n",
    "            root=os.path.join(data_root, 'train'),\n",
    "            transform=raw_transform\n",
    "        )\n",
    "\n",
    "    # Create subset if specified\n",
    "    if subset_size is not None and subset_size < len(trainset_raw):\n",
    "        indices = torch.randperm(len(trainset_raw))[:subset_size].tolist()\n",
    "        trainset_raw = Subset(trainset_raw, indices)\n",
    "\n",
    "    # Compute mean over subset (use smaller batch for memory efficiency)\n",
    "    print(\"Computing dataset mean...\")\n",
    "    temp_loader = DataLoader(trainset_raw, batch_size=min(1000, len(trainset_raw)),\n",
    "                            shuffle=False, num_workers=4)\n",
    "\n",
    "    # Accumulate mean\n",
    "    mean_accumulator = None\n",
    "    count = 0\n",
    "    for batch_data, _ in temp_loader:\n",
    "        batch_flat = batch_data.view(batch_data.size(0), -1)  # (B, img_size²)\n",
    "        if mean_accumulator is None:\n",
    "            mean_accumulator = batch_flat.sum(dim=0)\n",
    "        else:\n",
    "            mean_accumulator += batch_flat.sum(dim=0)\n",
    "        count += batch_data.size(0)\n",
    "\n",
    "    dataset_mean = mean_accumulator / count  # Shape: (img_size²,)\n",
    "    print(f\"Mean computed over {count} images\")\n",
    "\n",
    "    # Define preprocessing function\n",
    "    input_dim = img_size * img_size  # For grayscale\n",
    "    def preprocess(x):\n",
    "        x_flat = x.view(-1)  # Flatten\n",
    "        x_centered = x_flat - dataset_mean  # Subtract mean\n",
    "        x_norm = x_centered / (torch.norm(x_centered) + 1e-8)  # Unit norm\n",
    "        return x_norm\n",
    "\n",
    "    # Final transform with preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(img_size + 8),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(preprocess)\n",
    "    ])\n",
    "\n",
    "    # Load datasets with proper preprocessing\n",
    "    try:\n",
    "        trainset = torchvision.datasets.ImageNet(\n",
    "            root=data_root,\n",
    "            split='train',\n",
    "            transform=transform\n",
    "        )\n",
    "        testset = torchvision.datasets.ImageNet(\n",
    "            root=data_root,\n",
    "            split='val',  # Use validation set as test\n",
    "            transform=transform\n",
    "        )\n",
    "    except:\n",
    "        trainset = torchvision.datasets.ImageFolder(\n",
    "            root=os.path.join(data_root, 'train'),\n",
    "            transform=transform\n",
    "        )\n",
    "        testset = torchvision.datasets.ImageFolder(\n",
    "            root=os.path.join(data_root, 'val'),\n",
    "            transform=transform\n",
    "        )\n",
    "\n",
    "    # Apply subset to preprocessed data\n",
    "    if subset_size is not None and subset_size < len(trainset):\n",
    "        trainset = Subset(trainset, indices)\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True,\n",
    "                             num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader, dataset_mean"
   ],
   "id": "3ccddad406245d3b",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T18:01:17.197899Z",
     "start_time": "2025-10-20T18:01:17.187304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "\n",
    "def load_lfw_data(batch_size=128, img_size=64, min_faces_per_person=20):\n",
    "    \"\"\"\n",
    "    Load Labeled Faces in the Wild dataset with proper resizing\n",
    "\n",
    "    Args:\n",
    "        batch_size: Batch size\n",
    "        img_size: Resize to (img_size, img_size) - actual pixels\n",
    "        min_faces_per_person: Filter people with fewer images\n",
    "    \"\"\"\n",
    "    # Download LFW with original size\n",
    "    lfw_people = fetch_lfw_people(\n",
    "        min_faces_per_person=min_faces_per_person,\n",
    "        resize=1.0,  # Keep original size, we'll resize manually\n",
    "        color=False\n",
    "    )\n",
    "\n",
    "    print(f\"Original LFW shape: {lfw_people.images.shape}\")\n",
    "\n",
    "    # Manually resize to exact dimensions\n",
    "    resized_images = []\n",
    "    for img in lfw_people.images:\n",
    "        # Convert to PIL Image for proper resizing\n",
    "        pil_img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "        # Resize to exact target size\n",
    "        pil_img = pil_img.resize((img_size, img_size), Image.LANCZOS)\n",
    "        # Back to normalized array\n",
    "        resized = np.array(pil_img).astype(np.float32) / 255.0\n",
    "        resized_images.append(resized.flatten())\n",
    "\n",
    "    data_flat = np.array(resized_images)\n",
    "    print(f\"Resized LFW shape: {data_flat.shape}\")  # Should be (n_samples, img_size²)\n",
    "\n",
    "    # Convert to torch\n",
    "    data_tensor = torch.FloatTensor(data_flat)\n",
    "\n",
    "    # Compute mean\n",
    "    dataset_mean = data_tensor.mean(dim=0)\n",
    "\n",
    "    # Preprocess\n",
    "    def preprocess(x):\n",
    "        x_centered = x - dataset_mean\n",
    "        x_norm = x_centered / (torch.norm(x_centered) + 1e-8)\n",
    "        return x_norm\n",
    "\n",
    "    preprocessed_data = torch.stack([preprocess(x) for x in data_tensor])\n",
    "\n",
    "    # Create dataset\n",
    "    class LFWDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return (self.data[idx],)\n",
    "\n",
    "    dataset = LFWDataset(preprocessed_data)\n",
    "\n",
    "    # Split 80/20\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size],\n",
    "                                     generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    print(f\"LFW Dataset loaded: {len(trainset)} train, {len(testset)} test\")\n",
    "    print(f\"Image size: {img_size}×{img_size}, Input dimension: {img_size**2}\")\n",
    "\n",
    "    return train_loader, test_loader, dataset_mean"
   ],
   "id": "3c3c819edc1dfd03",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:15:37.503083Z",
     "start_time": "2025-10-20T19:15:37.489682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.datasets import fetch_olivetti_faces, fetch_lfw_people\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def load_data(dataset_name, batch_size=128, img_size=64, **kwargs):\n",
    "    \"\"\"\n",
    "    Unified data loading function for multiple datasets.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: One of ['mnist', 'olivetti', 'lfw', 'imagenet']\n",
    "        batch_size: Batch size for DataLoader\n",
    "        img_size: Image size for face datasets (default 64)\n",
    "        **kwargs: Additional dataset-specific arguments\n",
    "\n",
    "    Returns:\n",
    "        train_loader: DataLoader for training\n",
    "        test_loader: DataLoader for testing\n",
    "        dataset_mean: Mean vector used for preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset_name.lower() == 'mnist':\n",
    "        return load_mnist_data(batch_size)\n",
    "\n",
    "    elif dataset_name.lower() == 'olivetti':\n",
    "        train_split = kwargs.get('train_split', 0.8)\n",
    "        return load_olivetti_data(batch_size, train_split)\n",
    "\n",
    "    elif dataset_name.lower() == 'lfw':\n",
    "        min_faces_per_person = kwargs.get('min_faces_per_person', 20)\n",
    "        return load_lfw_data(batch_size, img_size, min_faces_per_person)\n",
    "\n",
    "    elif dataset_name.lower() == 'imagenet':\n",
    "        subset_size = kwargs.get('subset_size', 10000)\n",
    "        data_root = kwargs.get('data_root', './data/imagenet')\n",
    "        return load_imagenet_subset(batch_size, subset_size, img_size, data_root)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}. Choose from ['mnist', 'olivetti', 'lfw', 'imagenet']\")\n"
   ],
   "id": "beab04ad03948608",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training function",
   "id": "50475f90ab2b747d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T18:01:17.250626Z",
     "start_time": "2025-10-20T18:01:17.243482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "def train_sparse_autoencoder(train_loader, num_epochs=50, learning_rate=0.001,\n",
    "                            input_size=784, hidden_size=64, k_top=20,\n",
    "                            JumpReLU=0.1, k_aux=None, k_aux_param=1/32,\n",
    "                            dead_feature_threshold=1000, modelType=\"SAE\",\n",
    "                            dataset_type=\"mnist\"):\n",
    "    \"\"\"\n",
    "    Train sparse autoencoder with support for different datasets\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader for training data\n",
    "        dataset_type: 'mnist', 'olivetti', or 'imagenet' to handle different unpacking\n",
    "        ... (other args as before)\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if modelType == \"SAE\":\n",
    "        model = SparseAutoencoder(input_size=input_size, hidden_size=hidden_size, k_top=k_top).to(device)\n",
    "    elif modelType == \"SAE_Init_JumpReLU\":\n",
    "        model = SparseAutoencoderInitJumpReLU(input_size=input_size, hidden_size=hidden_size, k_top=k_top, jump_value=JumpReLU).to(device)\n",
    "    elif modelType == \"SAE_JumpReLU\":\n",
    "        model = SparseAutoencoderJumpReLU(input_size=input_size, hidden_size=hidden_size, k_top=k_top, jump_value=JumpReLU).to(device)\n",
    "    elif modelType == \"SAE_Init\":\n",
    "        model = SparseAutoencoderInit(input_size=input_size, hidden_size=hidden_size, k_top=k_top).to(device)\n",
    "    elif modelType == \"SAE_AuxLoss\":\n",
    "        model = SparseAutoencoderAuxLoss(input_size=input_size, hidden_size=hidden_size, k_top=k_top, k_aux=k_aux,\n",
    "                                         k_aux_param=k_aux_param, dead_feature_threshold=dead_feature_threshold).to(device)\n",
    "    elif modelType == \"Complete\":\n",
    "        model = SparseAutoencoderComplete(input_size=input_size, hidden_size=hidden_size, k_top=k_top, k_aux=k_aux,\n",
    "                                         k_aux_param=k_aux_param, dead_feature_threshold=dead_feature_threshold, jump_value=JumpReLU).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid modelType specified.\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            # Handle different data loader formats\n",
    "            if dataset_type == 'olivetti':\n",
    "                # Olivetti returns single-element tuple: (inputs,)\n",
    "                inputs, = data  # Note the comma - unpacks single element\n",
    "                inputs = inputs.to(device)\n",
    "            elif dataset_type in ['mnist', 'imagenet']:\n",
    "                # MNIST and ImageNet return (inputs, labels)\n",
    "                inputs, _ = data\n",
    "                # No need to reshape - already preprocessed to correct shape\n",
    "                inputs = inputs.to(device)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown dataset_type: {dataset_type}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            h, outputs = model(inputs)\n",
    "\n",
    "            if modelType == \"SAE_AuxLoss\" or modelType == \"Complete\":\n",
    "                loss, mse_loss, aux_loss = model.compute_loss(inputs, h, outputs)\n",
    "            else:\n",
    "                loss = model.compute_loss(inputs, h, outputs)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clamp weights to enforce non-negativity\n",
    "            with torch.no_grad():\n",
    "                model.decoder.weight.clamp_(0.0)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    print('Finished Training')\n",
    "    return model"
   ],
   "id": "ec162a61e1221868",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualization functions",
   "id": "e0b92ca01859d5eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T18:01:17.313559Z",
     "start_time": "2025-10-20T18:01:17.298756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def visualize_weights_decoder(model, num_features=64):\n",
    "    \"\"\"\n",
    "    Visualize decoder weights - AUTO-DETECTS dimensions\n",
    "    \"\"\"\n",
    "    # Auto-detect input size from model\n",
    "    input_size = model.decoder.weight.shape[0]\n",
    "    print(f\"Auto-detected input_size: {input_size}\")\n",
    "\n",
    "    # Determine image shape\n",
    "    if input_size == 784:\n",
    "        img_shape = (28, 28)\n",
    "        dataset_type = 'mnist'\n",
    "    elif input_size == 4096:\n",
    "        img_shape = (64, 64)\n",
    "    else:\n",
    "        # Non-square or unusual size - try square root\n",
    "        side = int(np.sqrt(input_size))\n",
    "        if side * side == input_size:\n",
    "            img_shape = (side, side)\n",
    "        else:\n",
    "            # Non-square - find factors\n",
    "            for h in range(int(np.sqrt(input_size)), 0, -1):\n",
    "                if input_size % h == 0:\n",
    "                    w = input_size // h\n",
    "                    img_shape = (h, w)\n",
    "                    break\n",
    "\n",
    "    print(f\"Using image shape: {img_shape}\")\n",
    "\n",
    "    weights = model.decoder.weight.data.cpu().numpy().T\n",
    "    num_features = min(num_features, weights.shape[0])\n",
    "\n",
    "    # Grid dimensions\n",
    "    x_images = int(math.ceil(math.sqrt(num_features)))\n",
    "    y_images = int(math.ceil(num_features / x_images))\n",
    "\n",
    "    plt.figure(figsize=(x_images * 2, y_images * 2))\n",
    "    model_name = getattr(model, 'name', 'SAE')\n",
    "    plt.suptitle(f'{model_name} Decoder Weights ({img_shape[0]}×{img_shape[1]})',\n",
    "                 fontsize=14, y=0.995)\n",
    "\n",
    "    for i in range(num_features):\n",
    "        plt.subplot(y_images, x_images, i + 1)\n",
    "        weight_img = weights[i].reshape(img_shape)\n",
    "\n",
    "        # Normalize\n",
    "        weight_img = (weight_img - weight_img.min()) / (weight_img.max() - weight_img.min() + 1e-8)\n",
    "\n",
    "        plt.imshow(weight_img, cmap='gray', interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'F{i}', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_weights_encoder(model, num_features=64):\n",
    "    \"\"\"\n",
    "    Visualize encoder weights - AUTO-DETECTS dimensions\n",
    "    \"\"\"\n",
    "    # Get weights\n",
    "    if hasattr(model.encoder, 'weight'):\n",
    "        weights = model.encoder.weight.data.cpu().numpy()\n",
    "    elif isinstance(model.encoder, torch.nn.Sequential):\n",
    "        weights = model.encoder[0].weight.data.cpu().numpy()\n",
    "    else:\n",
    "        raise ValueError(\"Unknown encoder structure\")\n",
    "\n",
    "    # Auto-detect input size\n",
    "    input_size = weights.shape[1]\n",
    "    print(f\"Auto-detected input_size: {input_size}\")\n",
    "\n",
    "    # Determine image shape\n",
    "    if input_size == 784:\n",
    "        img_shape = (28, 28)\n",
    "    elif input_size == 4096:\n",
    "        img_shape = (64, 64)\n",
    "    else:\n",
    "        side = int(np.sqrt(input_size))\n",
    "        if side * side == input_size:\n",
    "            img_shape = (side, side)\n",
    "        else:\n",
    "            for h in range(int(np.sqrt(input_size)), 0, -1):\n",
    "                if input_size % h == 0:\n",
    "                    w = input_size // h\n",
    "                    img_shape = (h, w)\n",
    "                    break\n",
    "\n",
    "    print(f\"Using image shape: {img_shape}\")\n",
    "\n",
    "    num_features = min(num_features, weights.shape[0])\n",
    "\n",
    "    x_images = int(math.ceil(math.sqrt(num_features)))\n",
    "    y_images = int(math.ceil(num_features / x_images))\n",
    "\n",
    "    plt.figure(figsize=(x_images * 2, y_images * 2))\n",
    "    model_name = getattr(model, 'name', 'SAE')\n",
    "    plt.suptitle(f'{model_name} Encoder Weights ({img_shape[0]}×{img_shape[1]})',\n",
    "                 fontsize=14, y=0.995)\n",
    "\n",
    "    for i in range(num_features):\n",
    "        plt.subplot(y_images, x_images, i + 1)\n",
    "        weight_img = weights[i].reshape(img_shape)\n",
    "        weight_img = (weight_img - weight_img.min()) / (weight_img.max() - weight_img.min() + 1e-8)\n",
    "        plt.imshow(weight_img, cmap='gray', interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'F{i}', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_reconstructions(model, data_loader, num_samples=10, dataset_type='olivetti'):\n",
    "    \"\"\"\n",
    "    Visualize reconstructions - AUTO-DETECTS dimensions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Get data\n",
    "    data_iter = iter(data_loader)\n",
    "    data = next(data_iter)\n",
    "\n",
    "    if dataset_type == 'olivetti' or len(data) == 1:\n",
    "        inputs, = data\n",
    "    else:\n",
    "        inputs, _ = data\n",
    "\n",
    "    inputs = inputs[:num_samples].to(device)\n",
    "\n",
    "    # Auto-detect dimensions\n",
    "    input_size = inputs.shape[1]\n",
    "    if input_size == 784:\n",
    "        img_shape = (28, 28)\n",
    "    elif input_size == 4096:\n",
    "        img_shape = (64, 64)\n",
    "    else:\n",
    "        side = int(np.sqrt(input_size))\n",
    "        if side * side == input_size:\n",
    "            img_shape = (side, side)\n",
    "        else:\n",
    "            for h in range(int(np.sqrt(input_size)), 0, -1):\n",
    "                if input_size % h == 0:\n",
    "                    w = input_size // h\n",
    "                    img_shape = (h, w)\n",
    "                    break\n",
    "\n",
    "    # Get reconstructions\n",
    "    with torch.no_grad():\n",
    "        _, reconstructions = model(inputs)\n",
    "\n",
    "    inputs = inputs.cpu().numpy()\n",
    "    reconstructions = reconstructions.cpu().numpy()\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 2, 4))\n",
    "    model_name = getattr(model, 'name', 'SAE')\n",
    "    plt.suptitle(f'{model_name} Reconstructions ({img_shape[0]}×{img_shape[1]})', fontsize=14)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        axes[0, i].imshow(inputs[i].reshape(img_shape), cmap='gray', interpolation='nearest')\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('Original', fontsize=10)\n",
    "\n",
    "        axes[1, i].imshow(reconstructions[i].reshape(img_shape), cmap='gray', interpolation='nearest')\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('Reconstructed', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "c7b1cd1e283aafb6",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Functions to count dead neurons and test loss on the dataset given",
   "id": "6f23cd993e61745e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T18:01:17.367423Z",
     "start_time": "2025-10-20T18:01:17.356069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def count_dead_neurons(model, data_loader, dataset_type='mnist'):\n",
    "    \"\"\"\n",
    "    Count dead neurons (features that never activate)\n",
    "\n",
    "    Args:\n",
    "        model: Trained SAE model\n",
    "        data_loader: DataLoader with data\n",
    "        dataset_type: 'mnist', 'olivetti', or 'imagenet' for proper unpacking\n",
    "\n",
    "    Returns:\n",
    "        num_dead: Number of dead neurons\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dead_neurons = torch.ones(model.hidden_size, dtype=torch.bool).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            # Handle different data loader formats\n",
    "            if dataset_type == 'olivetti':\n",
    "                inputs, = data  # Single-element tuple\n",
    "            else:  # mnist or imagenet\n",
    "                inputs, _ = data  # (inputs, labels) tuple\n",
    "\n",
    "            inputs = inputs.to(device)  # Already preprocessed, no reshape needed\n",
    "            h, _ = model(inputs)\n",
    "\n",
    "            # A neuron is alive if it activates (h > 0) for any sample\n",
    "            dead_neurons &= (h.sum(dim=0) == 0)\n",
    "\n",
    "    num_dead = dead_neurons.sum().item()\n",
    "    model_name = getattr(model, 'name', 'SAE')\n",
    "    print(f'Number of dead neurons in {model_name}: {num_dead} out of {model.hidden_size} '\n",
    "          f'({100*num_dead/model.hidden_size:.2f}%)')\n",
    "    return num_dead\n",
    "\n",
    "\n",
    "def test_loss(model, data_loader, dataset_type='mnist'):\n",
    "    \"\"\"\n",
    "    Compute average test loss\n",
    "\n",
    "    Args:\n",
    "        model: Trained SAE model\n",
    "        data_loader: DataLoader with test data\n",
    "        dataset_type: 'mnist', 'olivetti', or 'imagenet' for proper unpacking\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: Average loss over test set\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            # Handle different data loader formats\n",
    "            if dataset_type == 'olivetti':\n",
    "                inputs, = data  # Single-element tuple\n",
    "            else:  # mnist or imagenet\n",
    "                inputs, _ = data  # (inputs, labels) tuple\n",
    "\n",
    "            inputs = inputs.to(device)  # Already preprocessed, no reshape needed\n",
    "            h, outputs = model(inputs)\n",
    "\n",
    "            # Handle different loss outputs\n",
    "            loss_output = model.compute_loss(inputs, h, outputs)\n",
    "            if isinstance(loss_output, tuple):\n",
    "                loss, *_ = loss_output  # Unpack if tuple (e.g., with aux loss)\n",
    "            else:\n",
    "                loss = loss_output\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    model_name = getattr(model, 'name', 'SAE')\n",
    "    print(f'Test Loss for {model_name}: {avg_loss:.6f}')\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def get_activation_statistics(model, data_loader, dataset_type='mnist'):\n",
    "    \"\"\"\n",
    "    Get comprehensive statistics about feature activations\n",
    "\n",
    "    Args:\n",
    "        model: Trained SAE model\n",
    "        data_loader: DataLoader with data\n",
    "        dataset_type: 'mnist', 'olivetti', or 'imagenet'\n",
    "\n",
    "    Returns:\n",
    "        stats: Dictionary with activation statistics\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    activation_counts = torch.zeros(model.hidden_size).to(device)\n",
    "    activation_sums = torch.zeros(model.hidden_size).to(device)\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            # Handle different data loader formats\n",
    "            if dataset_type == 'olivetti':\n",
    "                inputs, = data\n",
    "            else:\n",
    "                inputs, _ = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            h, _ = model(inputs)\n",
    "\n",
    "            # Count how many times each feature activates (h > 0)\n",
    "            activation_counts += (h > 0).sum(dim=0).float()\n",
    "            activation_sums += h.sum(dim=0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    # Move to CPU for analysis\n",
    "    activation_counts = activation_counts.cpu().numpy()\n",
    "    activation_sums = activation_sums.cpu().numpy()\n",
    "\n",
    "    # Compute statistics\n",
    "    activation_freq = activation_counts / total_samples  # Fraction of samples each feature activates on\n",
    "    mean_activation = activation_sums / total_samples    # Average activation strength\n",
    "\n",
    "    stats = {\n",
    "        'total_features': model.hidden_size,\n",
    "        'dead_features': np.sum(activation_counts == 0),\n",
    "        'active_features': np.sum(activation_counts > 0),\n",
    "        'mean_activation_frequency': np.mean(activation_freq),\n",
    "        'median_activation_frequency': np.median(activation_freq),\n",
    "        'mean_activation_strength': np.mean(mean_activation[activation_counts > 0]),  # Among active features\n",
    "        'activation_frequencies': activation_freq,\n",
    "        'activation_strengths': mean_activation\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    model_name = getattr(model, 'name', 'SAE')\n",
    "    print(f\"\\n=== Activation Statistics for {model_name} ===\")\n",
    "    print(f\"Total features: {stats['total_features']}\")\n",
    "    print(f\"Dead features: {stats['dead_features']} ({100*stats['dead_features']/stats['total_features']:.2f}%)\")\n",
    "    print(f\"Active features: {stats['active_features']} ({100*stats['active_features']/stats['total_features']:.2f}%)\")\n",
    "    print(f\"Mean activation frequency: {stats['mean_activation_frequency']:.4f}\")\n",
    "    print(f\"Median activation frequency: {stats['median_activation_frequency']:.4f}\")\n",
    "    print(f\"Mean activation strength (active features): {stats['mean_activation_strength']:.4f}\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def plot_activation_histogram(model, data_loader, dataset_type='mnist'):\n",
    "    \"\"\"\n",
    "    Plot histogram of feature activation frequencies\n",
    "\n",
    "    Args:\n",
    "        model: Trained SAE model\n",
    "        data_loader: DataLoader with data\n",
    "        dataset_type: 'mnist', 'olivetti', or 'imagenet'\n",
    "    \"\"\"\n",
    "    stats = get_activation_statistics(model, data_loader, dataset_type)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    model_name = getattr(model, 'name', 'SAE')\n",
    "\n",
    "    # Histogram of activation frequencies\n",
    "    axes[0].hist(stats['activation_frequencies'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_xlabel('Activation Frequency (fraction of samples)')\n",
    "    axes[0].set_ylabel('Number of Features')\n",
    "    axes[0].set_title(f'{model_name}: Feature Activation Frequencies')\n",
    "    axes[0].axvline(stats['mean_activation_frequency'], color='r', linestyle='--',\n",
    "                    label=f'Mean: {stats[\"mean_activation_frequency\"]:.4f}')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Histogram of activation strengths (excluding dead features)\n",
    "    active_strengths = stats['activation_strengths'][stats['activation_strengths'] > 0]\n",
    "    axes[1].hist(active_strengths, bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "    axes[1].set_xlabel('Mean Activation Strength')\n",
    "    axes[1].set_ylabel('Number of Features')\n",
    "    axes[1].set_title(f'{model_name}: Feature Activation Strengths (Active Features Only)')\n",
    "    axes[1].axvline(stats['mean_activation_strength'], color='r', linestyle='--',\n",
    "                    label=f'Mean: {stats[\"mean_activation_strength\"]:.4f}')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "1e6c80537d31d35",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Initializing",
   "id": "a25fde095862b2cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T18:01:23.602355Z",
     "start_time": "2025-10-20T18:01:17.416543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_loader, test_loader, mean = load_olivetti_data(batch_size=32)\n",
    "train_loader, test_loader, mean = load_lfw_data(batch_size=128, img_size=64)"
   ],
   "id": "8105d6e7d4fa8ec0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original LFW shape: (3023, 125, 94)\n",
      "Resized LFW shape: (3023, 4096)\n",
      "LFW Dataset loaded: 2418 train, 605 test\n",
      "Image size: 64×64, Input dimension: 4096\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Base usage",
   "id": "6e3bce16a2613e38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T18:05:35.972800Z",
     "start_time": "2025-10-20T18:05:24.948936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_loader, test_loader, mean = load_lfw_data(batch_size=128, img_size=64)\n",
    "#\n",
    "# modelBase = train_sparse_autoencoder(\n",
    "#     train_loader,\n",
    "#     num_epochs=50,\n",
    "#     learning_rate=0.001,\n",
    "#     input_size=4096,\n",
    "#     hidden_size=1024,        # ← LARGER dictionary for more parts\n",
    "#     k_top=128,               # ← HIGHER k (12.5% sparsity) for combining parts\n",
    "#     k_aux=512,               # ← Use auxiliary loss\n",
    "#     k_aux_param=1/32,\n",
    "#     modelType=\"SAE_AuxLoss\", # ← Use AuxLoss to prevent dead neurons!\n",
    "#     dataset_type=\"olivetti\"\n",
    "# )\n"
   ],
   "id": "e3bca5d4a7bb234f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original LFW shape: (3023, 125, 94)\n",
      "Resized LFW shape: (3023, 4096)\n",
      "LFW Dataset loaded: 2418 train, 605 test\n",
      "Image size: 64×64, Input dimension: 4096\n",
      "Epoch [1/50], Loss: 0.0662\n",
      "Epoch [2/50], Loss: 0.0246\n",
      "Epoch [3/50], Loss: 0.0168\n",
      "Epoch [4/50], Loss: 0.0134\n",
      "Epoch [5/50], Loss: 0.0115\n",
      "Epoch [6/50], Loss: 0.0103\n",
      "Epoch [7/50], Loss: 0.0094\n",
      "Epoch [8/50], Loss: 0.0088\n",
      "Epoch [9/50], Loss: 0.0082\n",
      "Epoch [10/50], Loss: 0.0078\n",
      "Epoch [11/50], Loss: 0.0075\n",
      "Epoch [12/50], Loss: 0.0073\n",
      "Epoch [13/50], Loss: 0.0070\n",
      "Epoch [14/50], Loss: 0.0066\n",
      "Epoch [15/50], Loss: 0.0063\n",
      "Epoch [16/50], Loss: 0.0061\n",
      "Epoch [17/50], Loss: 0.0060\n",
      "Epoch [18/50], Loss: 0.0059\n",
      "Epoch [19/50], Loss: 0.0059\n",
      "Epoch [20/50], Loss: 0.0061\n",
      "Epoch [21/50], Loss: 0.0060\n",
      "Epoch [22/50], Loss: 0.0058\n",
      "Epoch [23/50], Loss: 0.0056\n",
      "Epoch [24/50], Loss: 0.0055\n",
      "Epoch [25/50], Loss: 0.0053\n",
      "Epoch [26/50], Loss: 0.0053\n",
      "Epoch [27/50], Loss: 0.0054\n",
      "Epoch [28/50], Loss: 0.0055\n",
      "Epoch [29/50], Loss: 0.0055\n",
      "Epoch [30/50], Loss: 0.0054\n",
      "Epoch [31/50], Loss: 0.0054\n",
      "Epoch [32/50], Loss: 0.0053\n",
      "Epoch [33/50], Loss: 0.0054\n",
      "Epoch [34/50], Loss: 0.0053\n",
      "Epoch [35/50], Loss: 0.0051\n",
      "Epoch [36/50], Loss: 0.0051\n",
      "Epoch [37/50], Loss: 0.0050\n",
      "Epoch [38/50], Loss: 0.0050\n",
      "Epoch [39/50], Loss: 0.0051\n",
      "Epoch [40/50], Loss: 0.0053\n",
      "Epoch [41/50], Loss: 0.0056\n",
      "Epoch [42/50], Loss: 0.0053\n",
      "Epoch [43/50], Loss: 0.0051\n",
      "Epoch [44/50], Loss: 0.0051\n",
      "Epoch [45/50], Loss: 0.0050\n",
      "Epoch [46/50], Loss: 0.0047\n",
      "Epoch [47/50], Loss: 0.0046\n",
      "Epoch [48/50], Loss: 0.0044\n",
      "Epoch [49/50], Loss: 0.0044\n",
      "Epoch [50/50], Loss: 0.0044\n",
      "Finished Training\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# visualize_weights_decoder(modelBase, num_features=64)\n",
    "# count_dead_neurons(modelBase, train_loader, dataset_type='olivetti')\n",
    "# test_loss(modelBase, test_loader, dataset_type='olivetti')\n",
    "# plot_activation_histogram(modelBase, train_loader, dataset_type='olivetti')"
   ],
   "id": "cb2f2d3ae0b69c29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TopK Sparsity Analysis",
   "id": "333bad2f014121c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-20T19:26:42.467040Z",
     "start_time": "2025-10-20T19:15:43.961818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test: K sensitivity sweep\n",
    "dataset_configs = [\n",
    "    {'name': 'mnist', 'input_size': 784, 'batch_size': 256},\n",
    "    {'name': 'olivetti', 'input_size': 4096, 'batch_size': 32},\n",
    "    {'name': 'lfw', 'input_size': 4096, 'batch_size': 128}\n",
    "]\n",
    "\n",
    "k_values = [5, 10, 20, 30, 40, 50, 64, 100, 128]\n",
    "hidden_size = 256  # Fixed overcomplete representation\n",
    "\n",
    "results = []\n",
    "for dataset_config in dataset_configs:\n",
    "    train_loader, test_loader, mean = load_data(dataset_config['name'],\n",
    "                                                 dataset_config['batch_size'])\n",
    "\n",
    "    for k in k_values:\n",
    "        model = train_sparse_autoencoder(\n",
    "            train_loader,\n",
    "            num_epochs=50,\n",
    "            input_size=dataset_config['input_size'],\n",
    "            hidden_size=hidden_size,\n",
    "            k_top=k,\n",
    "            modelType=\"SAE\",\n",
    "            dataset_type=dataset_config['name']\n",
    "        )\n",
    "\n",
    "        # Metrics\n",
    "        test_mse = test_loss(model, test_loader, dataset_config['name'])\n",
    "        dead_neurons = count_dead_neurons(model, train_loader, dataset_config['name'])\n",
    "        stats = get_activation_statistics(model, train_loader, dataset_config['name'])\n",
    "\n",
    "        results.append({\n",
    "            'dataset': dataset_config['name'],\n",
    "            'k': k,\n",
    "            'sparsity_ratio': k/hidden_size,\n",
    "            'test_mse': test_mse,\n",
    "            'dead_neurons': dead_neurons,\n",
    "            'active_features': stats['active_features'],\n",
    "            'mean_activation_freq': stats['mean_activation_frequency']\n",
    "        })\n"
   ],
   "id": "a1fd20411abd7168",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.2255\n",
      "Epoch [2/50], Loss: 0.1463\n",
      "Epoch [3/50], Loss: 0.1416\n",
      "Epoch [4/50], Loss: 0.1397\n",
      "Epoch [5/50], Loss: 0.1386\n",
      "Epoch [6/50], Loss: 0.1377\n",
      "Epoch [7/50], Loss: 0.1371\n",
      "Epoch [8/50], Loss: 0.1368\n",
      "Epoch [9/50], Loss: 0.1361\n",
      "Epoch [10/50], Loss: 0.1357\n",
      "Epoch [11/50], Loss: 0.1355\n",
      "Epoch [12/50], Loss: 0.1351\n",
      "Epoch [13/50], Loss: 0.1349\n",
      "Epoch [14/50], Loss: 0.1346\n",
      "Epoch [15/50], Loss: 0.1338\n",
      "Epoch [16/50], Loss: 0.1328\n",
      "Epoch [17/50], Loss: 0.1323\n",
      "Epoch [18/50], Loss: 0.1318\n",
      "Epoch [19/50], Loss: 0.1315\n",
      "Epoch [20/50], Loss: 0.1309\n",
      "Epoch [21/50], Loss: 0.1305\n",
      "Epoch [22/50], Loss: 0.1303\n",
      "Epoch [23/50], Loss: 0.1302\n",
      "Epoch [24/50], Loss: 0.1299\n",
      "Epoch [25/50], Loss: 0.1297\n",
      "Epoch [26/50], Loss: 0.1295\n",
      "Epoch [27/50], Loss: 0.1293\n",
      "Epoch [28/50], Loss: 0.1289\n",
      "Epoch [29/50], Loss: 0.1285\n",
      "Epoch [30/50], Loss: 0.1283\n",
      "Epoch [31/50], Loss: 0.1282\n",
      "Epoch [32/50], Loss: 0.1281\n",
      "Epoch [33/50], Loss: 0.1279\n",
      "Epoch [34/50], Loss: 0.1275\n",
      "Epoch [35/50], Loss: 0.1272\n",
      "Epoch [36/50], Loss: 0.1270\n",
      "Epoch [37/50], Loss: 0.1268\n",
      "Epoch [38/50], Loss: 0.1264\n",
      "Epoch [39/50], Loss: 0.1264\n",
      "Epoch [40/50], Loss: 0.1264\n",
      "Epoch [41/50], Loss: 0.1261\n",
      "Epoch [42/50], Loss: 0.1258\n",
      "Epoch [43/50], Loss: 0.1255\n",
      "Epoch [44/50], Loss: 0.1252\n",
      "Epoch [45/50], Loss: 0.1254\n",
      "Epoch [46/50], Loss: 0.1252\n",
      "Epoch [47/50], Loss: 0.1249\n",
      "Epoch [48/50], Loss: 0.1247\n",
      "Epoch [49/50], Loss: 0.1256\n",
      "Epoch [50/50], Loss: 0.1254\n",
      "Finished Training\n",
      "Test Loss for Default Sparse Autoencoder: 0.122875\n",
      "Number of dead neurons in Default Sparse Autoencoder: 185 out of 256 (72.27%)\n",
      "\n",
      "=== Activation Statistics for Default Sparse Autoencoder ===\n",
      "Total features: 256\n",
      "Dead features: 185 (72.27%)\n",
      "Active features: 71 (27.73%)\n",
      "Mean activation frequency: 0.0195\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (active features): 0.0201\n",
      "Epoch [1/50], Loss: 0.2129\n",
      "Epoch [2/50], Loss: 0.1170\n",
      "Epoch [3/50], Loss: 0.1106\n",
      "Epoch [4/50], Loss: 0.1081\n",
      "Epoch [5/50], Loss: 0.1066\n",
      "Epoch [6/50], Loss: 0.1057\n",
      "Epoch [7/50], Loss: 0.1052\n",
      "Epoch [8/50], Loss: 0.1048\n",
      "Epoch [9/50], Loss: 0.1044\n",
      "Epoch [10/50], Loss: 0.1040\n",
      "Epoch [11/50], Loss: 0.1037\n",
      "Epoch [12/50], Loss: 0.1033\n",
      "Epoch [13/50], Loss: 0.1030\n",
      "Epoch [14/50], Loss: 0.1029\n",
      "Epoch [15/50], Loss: 0.1027\n",
      "Epoch [16/50], Loss: 0.1026\n",
      "Epoch [17/50], Loss: 0.1025\n",
      "Epoch [18/50], Loss: 0.1024\n",
      "Epoch [19/50], Loss: 0.1022\n",
      "Epoch [20/50], Loss: 0.1019\n",
      "Epoch [21/50], Loss: 0.1017\n",
      "Epoch [22/50], Loss: 0.1015\n",
      "Epoch [23/50], Loss: 0.1014\n",
      "Epoch [24/50], Loss: 0.1012\n",
      "Epoch [25/50], Loss: 0.1010\n",
      "Epoch [26/50], Loss: 0.1008\n",
      "Epoch [27/50], Loss: 0.1006\n",
      "Epoch [28/50], Loss: 0.1004\n",
      "Epoch [29/50], Loss: 0.1003\n",
      "Epoch [30/50], Loss: 0.1000\n",
      "Epoch [31/50], Loss: 0.0999\n",
      "Epoch [32/50], Loss: 0.0998\n",
      "Epoch [33/50], Loss: 0.0996\n",
      "Epoch [34/50], Loss: 0.0992\n",
      "Epoch [35/50], Loss: 0.0990\n",
      "Epoch [36/50], Loss: 0.0987\n",
      "Epoch [37/50], Loss: 0.0984\n",
      "Epoch [38/50], Loss: 0.0980\n",
      "Epoch [39/50], Loss: 0.0976\n",
      "Epoch [40/50], Loss: 0.0974\n",
      "Epoch [41/50], Loss: 0.0972\n",
      "Epoch [42/50], Loss: 0.0968\n",
      "Epoch [43/50], Loss: 0.0968\n",
      "Epoch [44/50], Loss: 0.0966\n",
      "Epoch [45/50], Loss: 0.0963\n",
      "Epoch [46/50], Loss: 0.0962\n",
      "Epoch [47/50], Loss: 0.0960\n",
      "Epoch [48/50], Loss: 0.0959\n",
      "Epoch [49/50], Loss: 0.0960\n",
      "Epoch [50/50], Loss: 0.0959\n",
      "Finished Training\n",
      "Test Loss for Default Sparse Autoencoder: 0.093752\n",
      "Number of dead neurons in Default Sparse Autoencoder: 187 out of 256 (73.05%)\n",
      "\n",
      "=== Activation Statistics for Default Sparse Autoencoder ===\n",
      "Total features: 256\n",
      "Dead features: 187 (73.05%)\n",
      "Active features: 69 (26.95%)\n",
      "Mean activation frequency: 0.0391\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (active features): 0.0337\n",
      "Epoch [1/50], Loss: 0.2103\n",
      "Epoch [2/50], Loss: 0.0891\n",
      "Epoch [3/50], Loss: 0.0765\n",
      "Epoch [4/50], Loss: 0.0709\n",
      "Epoch [5/50], Loss: 0.0679\n",
      "Epoch [6/50], Loss: 0.0662\n",
      "Epoch [7/50], Loss: 0.0651\n",
      "Epoch [8/50], Loss: 0.0644\n",
      "Epoch [9/50], Loss: 0.0638\n",
      "Epoch [10/50], Loss: 0.0633\n",
      "Epoch [11/50], Loss: 0.0629\n",
      "Epoch [12/50], Loss: 0.0625\n",
      "Epoch [13/50], Loss: 0.0622\n",
      "Epoch [14/50], Loss: 0.0619\n",
      "Epoch [15/50], Loss: 0.0617\n",
      "Epoch [16/50], Loss: 0.0615\n",
      "Epoch [17/50], Loss: 0.0613\n",
      "Epoch [18/50], Loss: 0.0611\n",
      "Epoch [19/50], Loss: 0.0610\n",
      "Epoch [20/50], Loss: 0.0609\n",
      "Epoch [21/50], Loss: 0.0608\n",
      "Epoch [22/50], Loss: 0.0606\n",
      "Epoch [23/50], Loss: 0.0605\n",
      "Epoch [24/50], Loss: 0.0604\n",
      "Epoch [25/50], Loss: 0.0603\n",
      "Epoch [26/50], Loss: 0.0602\n",
      "Epoch [27/50], Loss: 0.0601\n",
      "Epoch [28/50], Loss: 0.0600\n",
      "Epoch [29/50], Loss: 0.0599\n",
      "Epoch [30/50], Loss: 0.0598\n",
      "Epoch [31/50], Loss: 0.0596\n",
      "Epoch [32/50], Loss: 0.0594\n",
      "Epoch [33/50], Loss: 0.0593\n",
      "Epoch [34/50], Loss: 0.0591\n",
      "Epoch [35/50], Loss: 0.0589\n",
      "Epoch [36/50], Loss: 0.0588\n",
      "Epoch [37/50], Loss: 0.0586\n",
      "Epoch [38/50], Loss: 0.0583\n",
      "Epoch [39/50], Loss: 0.0581\n",
      "Epoch [40/50], Loss: 0.0580\n",
      "Epoch [41/50], Loss: 0.0579\n",
      "Epoch [42/50], Loss: 0.0577\n",
      "Epoch [43/50], Loss: 0.0577\n",
      "Epoch [44/50], Loss: 0.0576\n",
      "Epoch [45/50], Loss: 0.0576\n",
      "Epoch [46/50], Loss: 0.0575\n",
      "Epoch [47/50], Loss: 0.0575\n",
      "Epoch [48/50], Loss: 0.0574\n",
      "Epoch [49/50], Loss: 0.0574\n",
      "Epoch [50/50], Loss: 0.0573\n",
      "Finished Training\n",
      "Test Loss for Default Sparse Autoencoder: 0.056303\n",
      "Number of dead neurons in Default Sparse Autoencoder: 138 out of 256 (53.91%)\n",
      "\n",
      "=== Activation Statistics for Default Sparse Autoencoder ===\n",
      "Total features: 256\n",
      "Dead features: 138 (53.91%)\n",
      "Active features: 118 (46.09%)\n",
      "Mean activation frequency: 0.0781\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (active features): 0.0285\n",
      "Epoch [1/50], Loss: 0.2111\n",
      "Epoch [2/50], Loss: 0.0751\n",
      "Epoch [3/50], Loss: 0.0609\n",
      "Epoch [4/50], Loss: 0.0548\n",
      "Epoch [5/50], Loss: 0.0514\n",
      "Epoch [6/50], Loss: 0.0491\n",
      "Epoch [7/50], Loss: 0.0476\n",
      "Epoch [8/50], Loss: 0.0465\n",
      "Epoch [9/50], Loss: 0.0457\n",
      "Epoch [10/50], Loss: 0.0451\n",
      "Epoch [11/50], Loss: 0.0446\n",
      "Epoch [12/50], Loss: 0.0442\n",
      "Epoch [13/50], Loss: 0.0439\n",
      "Epoch [14/50], Loss: 0.0436\n",
      "Epoch [15/50], Loss: 0.0433\n",
      "Epoch [16/50], Loss: 0.0432\n",
      "Epoch [17/50], Loss: 0.0430\n",
      "Epoch [18/50], Loss: 0.0430\n",
      "Epoch [19/50], Loss: 0.0428\n",
      "Epoch [20/50], Loss: 0.0428\n",
      "Epoch [21/50], Loss: 0.0427\n",
      "Epoch [22/50], Loss: 0.0426\n",
      "Epoch [23/50], Loss: 0.0425\n",
      "Epoch [24/50], Loss: 0.0425\n",
      "Epoch [25/50], Loss: 0.0423\n",
      "Epoch [26/50], Loss: 0.0422\n",
      "Epoch [27/50], Loss: 0.0422\n",
      "Epoch [28/50], Loss: 0.0421\n",
      "Epoch [29/50], Loss: 0.0421\n",
      "Epoch [30/50], Loss: 0.0420\n",
      "Epoch [31/50], Loss: 0.0420\n",
      "Epoch [32/50], Loss: 0.0420\n",
      "Epoch [33/50], Loss: 0.0420\n",
      "Epoch [34/50], Loss: 0.0419\n",
      "Epoch [35/50], Loss: 0.0419\n",
      "Epoch [36/50], Loss: 0.0419\n",
      "Epoch [37/50], Loss: 0.0419\n",
      "Epoch [38/50], Loss: 0.0418\n",
      "Epoch [39/50], Loss: 0.0419\n",
      "Epoch [40/50], Loss: 0.0418\n",
      "Epoch [41/50], Loss: 0.0418\n",
      "Epoch [42/50], Loss: 0.0417\n",
      "Epoch [43/50], Loss: 0.0417\n",
      "Epoch [44/50], Loss: 0.0416\n",
      "Epoch [45/50], Loss: 0.0416\n",
      "Epoch [46/50], Loss: 0.0416\n",
      "Epoch [47/50], Loss: 0.0416\n",
      "Epoch [48/50], Loss: 0.0416\n",
      "Epoch [49/50], Loss: 0.0415\n",
      "Epoch [50/50], Loss: 0.0415\n",
      "Finished Training\n",
      "Test Loss for Default Sparse Autoencoder: 0.040987\n",
      "Number of dead neurons in Default Sparse Autoencoder: 116 out of 256 (45.31%)\n",
      "\n",
      "=== Activation Statistics for Default Sparse Autoencoder ===\n",
      "Total features: 256\n",
      "Dead features: 116 (45.31%)\n",
      "Active features: 140 (54.69%)\n",
      "Mean activation frequency: 0.1172\n",
      "Median activation frequency: 0.0513\n",
      "Mean activation strength (active features): 0.0293\n",
      "Epoch [1/50], Loss: 0.2164\n",
      "Epoch [2/50], Loss: 0.0708\n",
      "Epoch [3/50], Loss: 0.0540\n",
      "Epoch [4/50], Loss: 0.0467\n",
      "Epoch [5/50], Loss: 0.0427\n",
      "Epoch [6/50], Loss: 0.0403\n",
      "Epoch [7/50], Loss: 0.0386\n",
      "Epoch [8/50], Loss: 0.0375\n",
      "Epoch [9/50], Loss: 0.0367\n",
      "Epoch [10/50], Loss: 0.0361\n",
      "Epoch [11/50], Loss: 0.0356\n",
      "Epoch [12/50], Loss: 0.0352\n",
      "Epoch [13/50], Loss: 0.0349\n",
      "Epoch [14/50], Loss: 0.0346\n",
      "Epoch [15/50], Loss: 0.0342\n",
      "Epoch [16/50], Loss: 0.0339\n",
      "Epoch [17/50], Loss: 0.0337\n",
      "Epoch [18/50], Loss: 0.0337\n",
      "Epoch [19/50], Loss: 0.0335\n",
      "Epoch [20/50], Loss: 0.0334\n",
      "Epoch [21/50], Loss: 0.0334\n",
      "Epoch [22/50], Loss: 0.0333\n",
      "Epoch [23/50], Loss: 0.0332\n",
      "Epoch [24/50], Loss: 0.0332\n",
      "Epoch [25/50], Loss: 0.0331\n",
      "Epoch [26/50], Loss: 0.0331\n",
      "Epoch [27/50], Loss: 0.0330\n",
      "Epoch [28/50], Loss: 0.0330\n",
      "Epoch [29/50], Loss: 0.0330\n",
      "Epoch [30/50], Loss: 0.0330\n",
      "Epoch [31/50], Loss: 0.0329\n",
      "Epoch [32/50], Loss: 0.0328\n",
      "Epoch [33/50], Loss: 0.0329\n",
      "Epoch [34/50], Loss: 0.0328\n",
      "Epoch [35/50], Loss: 0.0328\n",
      "Epoch [36/50], Loss: 0.0327\n",
      "Epoch [37/50], Loss: 0.0326\n",
      "Epoch [38/50], Loss: 0.0326\n",
      "Epoch [39/50], Loss: 0.0326\n",
      "Epoch [40/50], Loss: 0.0325\n",
      "Epoch [41/50], Loss: 0.0325\n",
      "Epoch [42/50], Loss: 0.0325\n",
      "Epoch [43/50], Loss: 0.0325\n",
      "Epoch [44/50], Loss: 0.0324\n",
      "Epoch [45/50], Loss: 0.0324\n",
      "Epoch [46/50], Loss: 0.0324\n",
      "Epoch [47/50], Loss: 0.0324\n",
      "Epoch [48/50], Loss: 0.0323\n",
      "Epoch [49/50], Loss: 0.0323\n",
      "Epoch [50/50], Loss: 0.0323\n",
      "Finished Training\n",
      "Test Loss for Default Sparse Autoencoder: 0.032075\n",
      "Number of dead neurons in Default Sparse Autoencoder: 92 out of 256 (35.94%)\n",
      "\n",
      "=== Activation Statistics for Default Sparse Autoencoder ===\n",
      "Total features: 256\n",
      "Dead features: 92 (35.94%)\n",
      "Active features: 164 (64.06%)\n",
      "Mean activation frequency: 0.1562\n",
      "Median activation frequency: 0.1100\n",
      "Mean activation strength (active features): 0.0287\n",
      "Epoch [1/50], Loss: 0.2258\n",
      "Epoch [2/50], Loss: 0.0688\n",
      "Epoch [3/50], Loss: 0.0497\n",
      "Epoch [4/50], Loss: 0.0421\n",
      "Epoch [5/50], Loss: 0.0376\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[160]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m     13\u001B[39m train_loader, test_loader, mean = load_data(dataset_config[\u001B[33m'\u001B[39m\u001B[33mname\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m     14\u001B[39m                                              dataset_config[\u001B[33m'\u001B[39m\u001B[33mbatch_size\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m k_values:\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     model = \u001B[43mtrain_sparse_autoencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdataset_config\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43minput_size\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhidden_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m        \u001B[49m\u001B[43mk_top\u001B[49m\u001B[43m=\u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmodelType\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mSAE\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdataset_type\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdataset_config\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mname\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     27\u001B[39m     \u001B[38;5;66;03m# Metrics\u001B[39;00m\n\u001B[32m     28\u001B[39m     test_mse = test_loss(model, test_loader, dataset_config[\u001B[33m'\u001B[39m\u001B[33mname\u001B[39m\u001B[33m'\u001B[39m])\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[142]\u001B[39m\u001B[32m, line 39\u001B[39m, in \u001B[36mtrain_sparse_autoencoder\u001B[39m\u001B[34m(train_loader, num_epochs, learning_rate, input_size, hidden_size, k_top, JumpReLU, k_aux, k_aux_param, dead_feature_threshold, modelType, dataset_type)\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[32m     38\u001B[39m     running_loss = \u001B[32m0.0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m39\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     40\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Handle different data loader formats\u001B[39;49;00m\n\u001B[32m     41\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdataset_type\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43molivetti\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m:\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# Olivetti returns single-element tuple: (inputs,)\u001B[39;49;00m\n\u001B[32m     43\u001B[39m \u001B[43m            \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Note the comma - unpacks single element\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ML-Research-Training/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:734\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    731\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    732\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    733\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m734\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    735\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    736\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    737\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    739\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    740\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ML-Research-Training/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:790\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    788\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    789\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m790\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    791\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    792\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ML-Research-Training/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ML-Research-Training/.venv/lib/python3.13/site-packages/torchvision/datasets/mnist.py:146\u001B[39m, in \u001B[36mMNIST.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m    143\u001B[39m img = _Image_fromarray(img.numpy(), mode=\u001B[33m\"\u001B[39m\u001B[33mL\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    145\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m146\u001B[39m     img = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.target_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    149\u001B[39m     target = \u001B[38;5;28mself\u001B[39m.target_transform(target)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ML-Research-Training/.venv/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001B[39m, in \u001B[36mCompose.__call__\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transforms:\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m         img = \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/ML-Research-Training/.venv/lib/python3.13/site-packages/torchvision/transforms/transforms.py:479\u001B[39m, in \u001B[36mLambda.__call__\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m    478\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m--> \u001B[39m\u001B[32m479\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlambd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[138]\u001B[39m\u001B[32m, line 26\u001B[39m, in \u001B[36mload_mnist_data.<locals>.preprocess\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpreprocess\u001B[39m(x):\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m     x_flat = \u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Flatten from (1, 28, 28) to (784,)\u001B[39;00m\n\u001B[32m     27\u001B[39m     x_centered = x_flat - dataset_mean  \u001B[38;5;66;03m# Subtract mean\u001B[39;00m\n\u001B[32m     28\u001B[39m     x_norm = x_centered / (torch.norm(x_centered) + \u001B[32m1e-8\u001B[39m)  \u001B[38;5;66;03m# Normalize to unit norm\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a9476820a0d8a6d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
