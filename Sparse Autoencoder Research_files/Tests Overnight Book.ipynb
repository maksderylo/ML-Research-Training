{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-23T06:22:14.171916Z",
     "start_time": "2025-10-23T06:22:14.127774Z"
    }
   },
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Sparse Autoencoder Interpretability Analysis\n",
    "Visualizes decoder features, computes monosemanticity scores, and compares model configurations\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.datasets import fetch_olivetti_faces, fetch_lfw_people\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=64, k_top=20):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.training = True\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.k_top = k_top\n",
    "        self.name = \"Default Sparse Autoencoder\"\n",
    "\n",
    "        # Encoder maps input to hidden representation\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Decoder maps hidden representation back to input space\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def _topk_mask(self, activations: torch.Tensor) -> torch.Tensor:\n",
    "        # activations: (batch, hidden)\n",
    "        k = max(0, min(self.k_top, activations.size(1)))\n",
    "        _, idx = torch.topk(activations, k, dim=1)\n",
    "        mask = torch.zeros_like(activations)\n",
    "        mask.scatter_(1, idx, 1.0)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        pre_activations = self.encoder(x)\n",
    "        pre_activations = F.relu(pre_activations)\n",
    "        mask = self._topk_mask(pre_activations)\n",
    "        h = pre_activations * mask\n",
    "        x_hat = self.decoder(h)\n",
    "        return h, x_hat\n",
    "\n",
    "\n",
    "    def compute_loss(self, x, h, x_hat):\n",
    "        # We compute sum of squares and normalize by batch size\n",
    "        recon_loss = torch.sum((x - x_hat) ** 2) / (x.size(0))\n",
    "\n",
    "        return recon_loss\n",
    "\n",
    "class SparseAutoencoderInit(SparseAutoencoder):\n",
    "    def __init__(self, input_size=784, hidden_size=64, k_top=20):\n",
    "        super(SparseAutoencoderInit, self).__init__(input_size, hidden_size, k_top)\n",
    "\n",
    "        self.name = \"Sparse Autoencoder with just weight initialization\"\n",
    "        # Initialize encoder weights first with random directions\n",
    "        nn.init.kaiming_uniform_(self.encoder.weight, a=math.sqrt(5))\n",
    "        # Initialize the decoder to be the transpose of the encoder weights\n",
    "        with torch.no_grad():\n",
    "            self.decoder.weight.copy_(self.encoder.weight.t())\n",
    "\n",
    "class SparseAutoencoderAuxLoss(SparseAutoencoder):\n",
    "    def __init__(self, input_size, hidden_size, k_top, k_aux, k_aux_param, dead_feature_threshold):\n",
    "        super(SparseAutoencoderAuxLoss, self).__init__(input_size, hidden_size, k_top)\n",
    "        self.name = \"Sparse Autoencoder with Auxiliary Loss\"\n",
    "        # k_aux is typically 2*k or more to revive dead features\n",
    "        self.k_aux = k_aux if k_aux is not None else 2 * k_top\n",
    "        self.k_aux_param = k_aux_param\n",
    "        # Track dead features: count steps since each feature was last active\n",
    "        self.register_buffer('steps_since_active', torch.zeros(hidden_size))\n",
    "        self.dead_feature_threshold = dead_feature_threshold\n",
    "\n",
    "    # Function to track which features are dead\n",
    "    def _update_dead_features(self, h: torch.Tensor):\n",
    "        # Feature is active if ANY sample in batch activates it\n",
    "        active_mask = (h.abs() > 1e-8).any(dim=0)\n",
    "\n",
    "        # Increment counter for inactive features, reset for active ones\n",
    "        self.steps_since_active += 1\n",
    "        self.steps_since_active[active_mask] = 0\n",
    "\n",
    "    def _get_dead_feature_mask(self) -> torch.Tensor:\n",
    "        \"\"\"Return boolean mask of dead features\"\"\"\n",
    "        return self.steps_since_active > self.dead_feature_threshold\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h_raw = self.encoder(x)\n",
    "        mask = self._topk_mask(h_raw)\n",
    "        h = h_raw * mask\n",
    "        x_hat = self.decoder(h)\n",
    "\n",
    "        # Track dead features during training\n",
    "        if self.training:\n",
    "            self._update_dead_features(h)\n",
    "\n",
    "        return h, x_hat\n",
    "\n",
    "    def compute_loss(self, x, h, x_hat):\n",
    "\n",
    "        recon_loss = torch.sum((x - x_hat) ** 2) / x.size(0)\n",
    "\n",
    "        aux_loss = torch.tensor(0.0, device=x.device)\n",
    "\n",
    "        if self.training:\n",
    "            dead_mask = self._get_dead_feature_mask()\n",
    "            n_dead = dead_mask.sum().item()\n",
    "\n",
    "            if n_dead > 0:\n",
    "                recon_error_vec = x - x_hat\n",
    "                with torch.no_grad():\n",
    "                    h_raw = F.relu(self.encoder(x))\n",
    "\n",
    "                h_dead = h_raw * dead_mask.float().unsqueeze(0)\n",
    "                k_aux_features = min(self.k_aux, n_dead)\n",
    "                _, idx_aux = torch.topk(h_dead, k_aux_features, dim=1)\n",
    "                mask_aux = torch.zeros_like(h_dead)\n",
    "                mask_aux.scatter_(1, idx_aux, 1.0)\n",
    "\n",
    "                z_aux = h_raw * mask_aux\n",
    "                e_hat = self.decoder(z_aux)\n",
    "\n",
    "                aux_loss = torch.sum((recon_error_vec - e_hat) ** 2) / x.size(0)\n",
    "                aux_loss = aux_loss * self.k_aux_param\n",
    "\n",
    "        total_loss = recon_loss + aux_loss\n",
    "        return total_loss, recon_loss, aux_loss\n",
    "\n",
    "class SparseAutoencoderComplete(SparseAutoencoder):\n",
    "    def __init__(self, input_size, hidden_size, k_top, k_aux, k_aux_param, dead_feature_threshold, jump_value):\n",
    "        super(SparseAutoencoderComplete, self).__init__(input_size, hidden_size, k_top)\n",
    "        self.name = \"Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss\"\n",
    "        self.jump_value = jump_value\n",
    "\n",
    "        # k_aux is typically 2*k or more to revive dead features\n",
    "        self.k_aux = k_aux if k_aux is not None else 2 * k_top\n",
    "        self.k_aux_param = k_aux_param\n",
    "        # Track dead features: count steps since each feature was last active\n",
    "        self.register_buffer('steps_since_active', torch.zeros(hidden_size))\n",
    "        self.dead_feature_threshold = dead_feature_threshold\n",
    "\n",
    "        # Initialize encoder weights first with random directions\n",
    "        nn.init.kaiming_uniform_(self.encoder.weight, a=math.sqrt(5))\n",
    "        # Initialize the decoder to be the transpose of the encoder weights\n",
    "        with torch.no_grad():\n",
    "            self.decoder.weight.copy_(self.encoder.weight.t())\n",
    "\n",
    "    # Function to track which features are dead\n",
    "    def _update_dead_features(self, h: torch.Tensor):\n",
    "        # Feature is active if ANY sample in batch activates it\n",
    "        active_mask = (h.abs() > 1e-8).any(dim=0)\n",
    "\n",
    "        # Increment counter for inactive features, reset for active ones\n",
    "        self.steps_since_active += 1\n",
    "        self.steps_since_active[active_mask] = 0\n",
    "\n",
    "    def _get_dead_feature_mask(self) -> torch.Tensor:\n",
    "        \"\"\"Return boolean mask of dead features\"\"\"\n",
    "        return self.steps_since_active > self.dead_feature_threshold\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        pre_activations = self.encoder(x)\n",
    "        pre_activations = F.relu(pre_activations)\n",
    "        mask = self._topk_mask(pre_activations)\n",
    "        h = pre_activations * mask\n",
    "        x_hat = self.decoder(h)\n",
    "\n",
    "        # Track dead features during training\n",
    "        if self.training:\n",
    "            self._update_dead_features(h)\n",
    "\n",
    "        return h, x_hat\n",
    "\n",
    "    def compute_loss(self, x, h, x_hat):\n",
    "        # Main reconstruction loss\n",
    "        recon_error = torch.sum((x - x_hat) ** 2)\n",
    "        recon_loss = recon_error / x.size(0)\n",
    "\n",
    "        # Auxiliary loss using dead features only\n",
    "        aux_loss = torch.tensor(0.0, device=x.device)\n",
    "\n",
    "        if self.training:\n",
    "            dead_mask = self._get_dead_feature_mask()  # (hidden_size,)\n",
    "            n_dead = dead_mask.sum().item()\n",
    "\n",
    "            if n_dead > 0:\n",
    "                # Compute reconstruction error: e = x - x_hat\n",
    "                recon_error_vec = x - x_hat  # (batch, input_size)\n",
    "\n",
    "                # Get raw activations again (before TopK masking)\n",
    "                with torch.no_grad():\n",
    "                    h_raw = self.encoder(x)\n",
    "\n",
    "                # Select only dead features\n",
    "                h_dead = h_raw * dead_mask.float().unsqueeze(0)  # (batch, hidden_size)\n",
    "\n",
    "                # Select top-k_aux dead features\n",
    "                k_aux_features = min(self.k_aux, n_dead)\n",
    "                _, idx_aux = torch.topk(h_dead, k_aux_features, dim=1)\n",
    "                mask_aux = torch.zeros_like(h_dead)\n",
    "                mask_aux.scatter_(1, idx_aux, 1.0)\n",
    "\n",
    "                # Sparse activations using only dead features\n",
    "                z_aux = h_raw * mask_aux  # (batch, hidden_size)\n",
    "\n",
    "                # Reconstruct error using dead features\n",
    "                e_hat = self.decoder(z_aux)  # (batch, input_size)\n",
    "\n",
    "                # Auxiliary loss: ||e - e_hat||^2\n",
    "                aux_loss = torch.sum((recon_error_vec - e_hat) ** 2) / self.input_size\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = recon_loss + self.k_aux_param * aux_loss\n",
    "\n",
    "        return total_loss, recon_loss, aux_loss\n",
    "\n",
    "def load_mnist_data(batch_size=256):\n",
    "    # First load raw data to compute mean\n",
    "    raw_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Converts to [0,1] and creates tensor\n",
    "    ])\n",
    "\n",
    "    # Load training set to compute mean\n",
    "    trainset_raw = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                              download=True, transform=raw_transform)\n",
    "\n",
    "    # Compute mean over entire training set\n",
    "    train_loader_temp = DataLoader(trainset_raw, batch_size=len(trainset_raw), shuffle=False)\n",
    "    all_data = next(iter(train_loader_temp))[0]\n",
    "    all_data = all_data.view(all_data.size(0), -1)  # Flatten to (N, 784)\n",
    "    dataset_mean = all_data.mean(dim=0)  # Mean across samples, shape (784,)\n",
    "\n",
    "    # Define preprocessing transform with mean subtraction and normalization\n",
    "    def preprocess(x):\n",
    "        x_flat = x.view(-1)  # Flatten from (1, 28, 28) to (784,)\n",
    "        x_centered = x_flat - dataset_mean  # Subtract mean\n",
    "        x_norm = x_centered / (torch.norm(x_centered) + 1e-8)  # Normalize to unit norm\n",
    "        return x_norm\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(preprocess)\n",
    "    ])\n",
    "\n",
    "    # Load datasets with proper preprocessing\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                          download=True, transform=transform)\n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, dataset_mean\n",
    "\n",
    "def load_olivetti_data(batch_size=32, train_split=0.8):\n",
    "    \"\"\"\n",
    "    Load Olivetti Faces dataset (400 images, 64x64 grayscale)\n",
    "    Returns data with shape (N, 4096) after flattening\n",
    "    \"\"\"\n",
    "    # Download Olivetti Faces using sklearn\n",
    "    faces = fetch_olivetti_faces(shuffle=True, random_state=42)\n",
    "    data = faces.data  # Already normalized to [0, 1], shape (400, 4096)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    data_tensor = torch.FloatTensor(data)  # Shape: (400, 4096)\n",
    "\n",
    "    # Compute mean over entire dataset\n",
    "    dataset_mean = data_tensor.mean(dim=0)  # Shape: (4096,)\n",
    "\n",
    "    # Define preprocessing function\n",
    "    def preprocess(x):\n",
    "        x_centered = x - dataset_mean  # Subtract mean\n",
    "        x_norm = x_centered / (torch.norm(x_centered) + 1e-8)  # Unit norm\n",
    "        return x_norm\n",
    "\n",
    "    # Apply preprocessing to all data\n",
    "    preprocessed_data = torch.stack([preprocess(x) for x in data_tensor])\n",
    "\n",
    "    # Create dataset (no labels needed for autoencoder)\n",
    "    dataset = TensorDataset(preprocessed_data)\n",
    "\n",
    "    # Split into train/test\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size],\n",
    "                                     generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, dataset_mean\n",
    "\n",
    "def load_lfw_data(batch_size=128, img_size=64, min_faces_per_person=20):\n",
    "    \"\"\"\n",
    "    Load Labeled Faces in the Wild dataset with proper resizing\n",
    "\n",
    "    Args:\n",
    "        batch_size: Batch size\n",
    "        img_size: Resize to (img_size, img_size) - actual pixels\n",
    "        min_faces_per_person: Filter people with fewer images\n",
    "    \"\"\"\n",
    "    # Download LFW with original size\n",
    "    lfw_people = fetch_lfw_people(\n",
    "        min_faces_per_person=min_faces_per_person,\n",
    "        resize=1.0,  # Keep original size, we'll resize manually\n",
    "        color=False\n",
    "    )\n",
    "\n",
    "    print(f\"Original LFW shape: {lfw_people.images.shape}\")\n",
    "\n",
    "    # Manually resize to exact dimensions\n",
    "    resized_images = []\n",
    "    for img in lfw_people.images:\n",
    "        # Convert to PIL Image for proper resizing\n",
    "        pil_img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "        # Resize to exact target size\n",
    "        pil_img = pil_img.resize((img_size, img_size), Image.LANCZOS)\n",
    "        # Back to normalized array\n",
    "        resized = np.array(pil_img).astype(np.float32) / 255.0\n",
    "        resized_images.append(resized.flatten())\n",
    "\n",
    "    data_flat = np.array(resized_images)\n",
    "    print(f\"Resized LFW shape: {data_flat.shape}\")  # Should be (n_samples, img_size²)\n",
    "\n",
    "    # Convert to torch\n",
    "    data_tensor = torch.FloatTensor(data_flat)\n",
    "\n",
    "    # Compute mean\n",
    "    dataset_mean = data_tensor.mean(dim=0)\n",
    "\n",
    "    # Preprocess\n",
    "    def preprocess(x):\n",
    "        x_centered = x - dataset_mean\n",
    "        x_norm = x_centered / (torch.norm(x_centered) + 1e-8)\n",
    "        return x_norm\n",
    "\n",
    "    preprocessed_data = torch.stack([preprocess(x) for x in data_tensor])\n",
    "\n",
    "    # Create dataset\n",
    "    class LFWDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return (self.data[idx],)\n",
    "\n",
    "    dataset = LFWDataset(preprocessed_data)\n",
    "\n",
    "    # Split 80/20\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size],\n",
    "                                     generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    print(f\"LFW Dataset loaded: {len(trainset)} train, {len(testset)} test\")\n",
    "    print(f\"Image size: {img_size}×{img_size}, Input dimension: {img_size**2}\")\n",
    "\n",
    "    return train_loader, test_loader, dataset_mean\n",
    "\n",
    "def load_data(dataset_name, batch_size=128, img_size=64, **kwargs):\n",
    "    \"\"\"\n",
    "    Unified data loading function for multiple datasets.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: One of ['mnist', 'olivetti', 'lfw', 'imagenet']\n",
    "        batch_size: Batch size for DataLoader\n",
    "        img_size: Image size for face datasets (default 64)\n",
    "        **kwargs: Additional dataset-specific arguments\n",
    "\n",
    "    Returns:\n",
    "        train_loader: DataLoader for training\n",
    "        test_loader: DataLoader for testing\n",
    "        dataset_mean: Mean vector used for preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset_name.lower() == 'mnist':\n",
    "        return load_mnist_data(batch_size)\n",
    "\n",
    "    elif dataset_name.lower() == 'olivetti':\n",
    "        train_split = kwargs.get('train_split', 0.8)\n",
    "        return load_olivetti_data(batch_size, train_split)\n",
    "\n",
    "    elif dataset_name.lower() == 'lfw':\n",
    "        min_faces_per_person = kwargs.get('min_faces_per_person', 20)\n",
    "        return load_lfw_data(batch_size, img_size, min_faces_per_person)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}. Choose from ['mnist', 'olivetti', 'lfw']\")\n",
    "\n",
    "def train_sparse_autoencoder(train_loader, num_epochs=50, learning_rate=0.001,\n",
    "                            input_size=784, hidden_size=64, k_top=20,\n",
    "                            JumpReLU=0.1, k_aux=None, k_aux_param=1/32,\n",
    "                            dead_feature_threshold=1000, modelType=\"SAE\",\n",
    "                            dataset_type=\"mnist\"):\n",
    "    \"\"\"\n",
    "    Train sparse autoencoder with support for different datasets\n",
    "\n",
    "    Args:\n",
    "        train_loader: DataLoader for training data\n",
    "        dataset_type: 'mnist', 'olivetti', or 'imagenet' to handle different unpacking\n",
    "        ... (other args as before)\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    if modelType == \"SAE\":\n",
    "        model = SparseAutoencoder(input_size=input_size, hidden_size=hidden_size, k_top=k_top).to(device)\n",
    "    elif modelType == \"SAE_Init\":\n",
    "        model = SparseAutoencoderInit(input_size=input_size, hidden_size=hidden_size, k_top=k_top).to(device)\n",
    "    elif modelType == \"SAE_AuxLoss\":\n",
    "        model = SparseAutoencoderAuxLoss(input_size=input_size, hidden_size=hidden_size, k_top=k_top, k_aux=k_aux,\n",
    "                                         k_aux_param=k_aux_param, dead_feature_threshold=dead_feature_threshold).to(device)\n",
    "    elif modelType == \"Complete\":\n",
    "        model = SparseAutoencoderComplete(input_size=input_size, hidden_size=hidden_size, k_top=k_top, k_aux=k_aux,\n",
    "                                         k_aux_param=k_aux_param, dead_feature_threshold=dead_feature_threshold, jump_value=JumpReLU).to(device)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid modelType specified.\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_aux_loss = 0.0\n",
    "        running_mse = 0.0\n",
    "\n",
    "        for data in train_loader:\n",
    "            # Handle different data loader formats\n",
    "            if dataset_type in ['olivetti', 'lfw']:\n",
    "                # Olivetti returns single-element tuple: (inputs,)\n",
    "                inputs, = data  # Note the comma - unpacks single element\n",
    "                inputs = inputs.to(device)\n",
    "            elif dataset_type in ['mnist', 'imagenet']:\n",
    "                # MNIST and ImageNet return (inputs, labels)\n",
    "                inputs, _ = data\n",
    "                # No need to reshape - already preprocessed to correct shape\n",
    "                inputs = inputs.to(device)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown dataset_type: {dataset_type}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            h, outputs = model(inputs)\n",
    "\n",
    "\n",
    "\n",
    "            if modelType == 'SAE_AuxLoss' or modelType == 'Complete':\n",
    "                total_loss, mse_loss, aux_loss = model.compute_loss(inputs, h, outputs)\n",
    "                loss = total_loss\n",
    "                running_loss += total_loss.item()\n",
    "                running_aux_loss += aux_loss.item()\n",
    "                running_mse += mse_loss.item()\n",
    "            else:\n",
    "                loss = model.compute_loss(inputs, h, outputs)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clamp weights to enforce non-negativity\n",
    "            with torch.no_grad():\n",
    "                model.encoder.weight.clamp_(0.0)\n",
    "                model.decoder.weight.clamp_(0.0)\n",
    "        if modelType == 'SAE_AuxLoss' or modelType == 'Complete':\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], MSE Loss: {running_loss/len(train_loader):.4f}, Aux Loss: {running_aux_loss/len(train_loader):.4f}, Total Loss: {(running_loss + running_aux_loss)/len(train_loader):.4f}')\n",
    "        else:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    print('Finished Training')\n",
    "    return model\n",
    "\n",
    "def count_dead_neurons(model, data_loader, dataset_type='mnist'):\n",
    "    \"\"\"\n",
    "    Count dead neurons (features that never activate)\n",
    "\n",
    "    Args:\n",
    "        model: Trained SAE model\n",
    "        data_loader: DataLoader with data\n",
    "        dataset_type: 'mnist', 'olivetti', or 'imagenet' for proper unpacking\n",
    "\n",
    "    Returns:\n",
    "        num_dead: Number of dead neurons\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dead_neurons = torch.ones(model.hidden_size, dtype=torch.bool).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            # Handle different data loader formats\n",
    "            if dataset_type in ['olivetti', 'lfw']:\n",
    "                inputs, = data  # Single-element tuple\n",
    "            else:  # mnist or imagenet\n",
    "                inputs, _ = data  # (inputs, labels) tuple\n",
    "\n",
    "            inputs = inputs.to(device)  # Already preprocessed, no reshape needed\n",
    "            h, _ = model(inputs)\n",
    "\n",
    "            # A neuron is alive if it activates (h > 0) for any sample\n",
    "            dead_neurons &= (h.sum(dim=0) == 0)\n",
    "\n",
    "    num_dead = dead_neurons.sum().item()\n",
    "    model_name = getattr(model, 'name', 'SAE')\n",
    "    print(f'Number of dead neurons in {model_name}: {num_dead} out of {model.hidden_size} '\n",
    "          f'({100*num_dead/model.hidden_size:.2f}%)')\n",
    "    return num_dead\n",
    "\n",
    "def test_loss(model, data_loader, dataset_type='mnist'):\n",
    "    \"\"\"\n",
    "    Compute average test loss\n",
    "\n",
    "    Args:\n",
    "        model: Trained SAE model\n",
    "        data_loader: DataLoader with test data\n",
    "        dataset_type: 'mnist', 'olivetti', or 'imagenet' for proper unpacking\n",
    "\n",
    "    Returns:\n",
    "        avg_loss: Average loss over test set\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            # Handle different data loader formats\n",
    "            if dataset_type in ['olivetti', 'lfw']:\n",
    "                inputs, = data  # Single-element tuple\n",
    "            else:  # mnist or imagenet\n",
    "                inputs, _ = data  # (inputs, labels) tuple\n",
    "\n",
    "            inputs = inputs.to(device)  # Already preprocessed, no reshape needed\n",
    "            h, outputs = model(inputs)\n",
    "\n",
    "            # Handle different loss outputs\n",
    "            loss_output = model.compute_loss(inputs, h, outputs)\n",
    "            if isinstance(loss_output, tuple):\n",
    "                loss = loss_output[1]  # Extract MSE component\n",
    "                total_loss += loss.item()\n",
    "            else:\n",
    "                loss = loss_output\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    model_name = getattr(model, 'name', 'SAE')\n",
    "    print(f'Test Loss for {model_name}: {avg_loss:.6f}')\n",
    "    return avg_loss\n",
    "\n",
    "def get_activation_statistics(model, data_loader, dataset_type='mnist'):\n",
    "    \"\"\"\n",
    "    Get comprehensive statistics about feature activations\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    activation_counts = torch.zeros(model.hidden_size).to(device)\n",
    "    activation_sums = torch.zeros(model.hidden_size).to(device)\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            # Handle different data loader formats\n",
    "            if dataset_type in ['olivetti', 'lfw']:\n",
    "                inputs, = data\n",
    "            else:\n",
    "                inputs, _ = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            h, _ = model(inputs)\n",
    "\n",
    "            # Count how many times each feature activates (h > 0)\n",
    "            activation_counts += (h > 0).sum(dim=0).float()\n",
    "            activation_sums += h.sum(dim=0)\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    # Move to CPU for analysis\n",
    "    activation_counts = activation_counts.cpu().numpy()\n",
    "    activation_sums = activation_sums.cpu().numpy()\n",
    "\n",
    "    # Compute statistics\n",
    "    activation_freq = activation_counts / total_samples  # Fraction of samples each feature activates on\n",
    "\n",
    "    # FIXED: Only compute mean over ACTIVE features (where count > 0)\n",
    "    active_mask = activation_counts > 0\n",
    "    mean_activation = np.zeros(model.hidden_size)\n",
    "    mean_activation[active_mask] = activation_sums[active_mask] / activation_counts[active_mask]\n",
    "\n",
    "    # Mean strength across ALL active features (not averaged over all samples)\n",
    "    if np.sum(active_mask) > 0:\n",
    "        mean_act_strength_active = np.mean(mean_activation[active_mask])\n",
    "    else:\n",
    "        mean_act_strength_active = 0.0\n",
    "\n",
    "    stats = {\n",
    "        'total_features': model.hidden_size,\n",
    "        'dead_features': np.sum(activation_counts == 0),\n",
    "        'active_features': np.sum(activation_counts > 0),\n",
    "        'mean_activation_frequency': np.mean(activation_freq),\n",
    "        'median_activation_frequency': np.median(activation_freq),\n",
    "        'mean_activation_strength': mean_act_strength_active,  # Corrected calculation\n",
    "        'activation_frequencies': activation_freq,\n",
    "        'activation_strengths': mean_activation,\n",
    "        'activation_counts': activation_counts\n",
    "    }\n",
    "\n",
    "    # Print summary\n",
    "    model_name = getattr(model, 'name', 'SAE')\n",
    "    print(f\"\\n=== Activation Statistics for {model_name} ===\")\n",
    "    print(f\"Total features: {stats['total_features']}\")\n",
    "    print(f\"Dead features: {stats['dead_features']} ({100*stats['dead_features']/stats['total_features']:.2f}%)\")\n",
    "    print(f\"Active features: {stats['active_features']} ({100*stats['active_features']/stats['total_features']:.2f}%)\")\n",
    "    print(f\"Mean activation frequency: {stats['mean_activation_frequency']:.4f}\")\n",
    "    print(f\"Median activation frequency: {stats['median_activation_frequency']:.4f}\")\n",
    "    print(f\"Mean activation strength (when active): {stats['mean_activation_strength']:.6f}\")\n",
    "\n",
    "    return stats\n",
    "\n",
    "def plot_activation_histogram(model, data_loader, dataset_type='mnist'):\n",
    "    \"\"\"\n",
    "    Plot histogram of feature activation frequencies\n",
    "\n",
    "    Args:\n",
    "        model: Trained SAE model\n",
    "        data_loader: DataLoader with data\n",
    "        dataset_type: 'mnist', 'olivetti', or 'imagenet'\n",
    "    \"\"\"\n",
    "    stats = get_activation_statistics(model, data_loader, dataset_type)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    model_name = getattr(model, 'name', 'SAE')\n",
    "\n",
    "    # Histogram of activation frequencies\n",
    "    axes[0].hist(stats['activation_frequencies'], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0].set_xlabel('Activation Frequency (fraction of samples)')\n",
    "    axes[0].set_ylabel('Number of Features')\n",
    "    axes[0].set_title(f'{model_name}: Feature Activation Frequencies')\n",
    "    axes[0].axvline(stats['mean_activation_frequency'], color='r', linestyle='--',\n",
    "                    label=f'Mean: {stats[\"mean_activation_frequency\"]:.4f}')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Histogram of activation strengths (excluding dead features)\n",
    "    active_strengths = stats['activation_strengths'][stats['activation_strengths'] > 0]\n",
    "    axes[1].hist(active_strengths, bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "    axes[1].set_xlabel('Mean Activation Strength')\n",
    "    axes[1].set_ylabel('Number of Features')\n",
    "    axes[1].set_title(f'{model_name}: Feature Activation Strengths (Active Features Only)')\n",
    "    axes[1].axvline(stats['mean_activation_strength'], color='r', linestyle='--',\n",
    "                    label=f'Mean: {stats[\"mean_activation_strength\"]:.4f}')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seeds(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "# Configuration for comprehensive experiments\n",
    "experiment_configs = {\n",
    "    'mnist': {\n",
    "        'input_size': 784,\n",
    "        'hidden_sizes': [128],\n",
    "        'k_tops': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120],\n",
    "        'batch_size': 256,\n",
    "        'num_epochs': 50,\n",
    "        'learning_rate': 0.001\n",
    "    }\n",
    "    ,\n",
    "    'olivetti': {\n",
    "        'input_size': 4096,\n",
    "        'hidden_sizes': [256, 512, 1024],\n",
    "        'k_tops': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120],\n",
    "        'batch_size': 32,\n",
    "        'num_epochs': 50,\n",
    "        'learning_rate': 0.001\n",
    "    },\n",
    "    'lfw': {\n",
    "        'input_size': 4096,  # 64x64 images\n",
    "        'hidden_sizes': [512, 1024, 2048],\n",
    "        'k_tops': [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120],\n",
    "        'batch_size': 64,\n",
    "        'num_epochs': 50,\n",
    "        'learning_rate': 0.001\n",
    "    }\n",
    "}\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 1: Weight Initialization Comparison\n",
    "# Tests: Random Xavier, Tied Weights, and Baseline\n",
    "# Duration: ~4-6 hours for all datasets\n",
    "# ============================================================================\n",
    "\n",
    "# Helper function to safely get activation strength\n",
    "def get_activation_strength_safe(activation_stats):\n",
    "    \"\"\"Safely extract mean activation strength, handling edge cases\"\"\"\n",
    "    if 'mean_activation_strength' in activation_stats:\n",
    "        strength = activation_stats['mean_activation_strength']\n",
    "        # Handle NaN or infinite values\n",
    "        if np.isnan(strength) or np.isinf(strength):\n",
    "            return 0.0\n",
    "        return float(strength)\n",
    "    else:\n",
    "        # If key doesn't exist, compute it manually\n",
    "        activation_strengths = activation_stats.get('activation_strengths', np.array([]))\n",
    "        if len(activation_strengths) > 0 and np.sum(activation_strengths > 0) > 0:\n",
    "            return float(np.mean(activation_strengths[activation_strengths > 0]))\n",
    "        return 0.0\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 1: Weight Initialization Comparison\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPERIMENT 1: WEIGHT INITIALIZATION COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "initialization_results = defaultdict(list)\n",
    "\n",
    "for dataset_name in ['mnist', 'olivetti', 'lfw']:\n",
    "    print(f\"\\n### Processing {dataset_name.upper()} Dataset ###\")\n",
    "\n",
    "    config = experiment_configs[dataset_name]\n",
    "\n",
    "    # Load data\n",
    "    train_loader, test_loader, dataset_mean = load_data(\n",
    "        dataset_name,\n",
    "        batch_size=config['batch_size'],\n",
    "        img_size=64\n",
    "    )\n",
    "\n",
    "    for hidden_size in config['hidden_sizes']:\n",
    "        for k_top in config['k_tops']:\n",
    "            print(f\"\\nHidden Size: {hidden_size}, k_top: {k_top}\")\n",
    "\n",
    "            # Test 3 initialization strategies\n",
    "            init_strategies = ['SAE', 'SAE_Init', 'SAE_AuxLoss', 'Complete']\n",
    "\n",
    "            for strategy in init_strategies:\n",
    "                print(f\"  Training {strategy}...\")\n",
    "                set_seeds(42)  # Ensure reproducibility\n",
    "\n",
    "                try:\n",
    "                    # Train model\n",
    "                    model = train_sparse_autoencoder(\n",
    "                        train_loader=train_loader,\n",
    "                        num_epochs=config['num_epochs'],\n",
    "                        learning_rate=config['learning_rate'],\n",
    "                        input_size=config['input_size'],\n",
    "                        hidden_size=hidden_size,\n",
    "                        k_top=k_top,\n",
    "                        modelType=strategy,\n",
    "                        dataset_type=dataset_name,\n",
    "                        k_aux= 2 * k_top if strategy in ['SAE_AuxLoss', 'Complete'] else None,\n",
    "                        k_aux_param= 0.1,\n",
    "                        dead_feature_threshold=500\n",
    "                    )\n",
    "\n",
    "                    # Evaluate\n",
    "                    test_loss_val = test_loss(model, test_loader, dataset_name)\n",
    "                    dead_neurons_count = count_dead_neurons(model, train_loader, dataset_name)\n",
    "                    activation_stats = get_activation_statistics(model, train_loader, dataset_name)\n",
    "\n",
    "                    # Safely extract activation strength\n",
    "                    mean_act_strength = get_activation_strength_safe(activation_stats)\n",
    "\n",
    "                    # Store results\n",
    "                    initialization_results['dataset'].append(dataset_name)\n",
    "                    initialization_results['hidden_size'].append(hidden_size)\n",
    "                    initialization_results['k_top'].append(k_top)\n",
    "                    initialization_results['initialization'].append(strategy)\n",
    "                    initialization_results['test_loss'].append(test_loss_val)\n",
    "                    initialization_results['dead_neurons'].append(dead_neurons_count)\n",
    "                    initialization_results['dead_neuron_pct'].append(\n",
    "                        100 * dead_neurons_count / hidden_size\n",
    "                    )\n",
    "                    initialization_results['active_features'].append(\n",
    "                        activation_stats['active_features']\n",
    "                    )\n",
    "                    initialization_results['mean_activation_freq'].append(\n",
    "                        activation_stats['mean_activation_frequency']\n",
    "                    )\n",
    "                    initialization_results['mean_activation_strength'].append(mean_act_strength)\n",
    "\n",
    "                    print(f\"    Dead neurons: {dead_neurons_count}/{hidden_size} \"\n",
    "                          f\"({100 * dead_neurons_count / hidden_size:.2f}%)\")\n",
    "                    print(f\"    Test loss: {test_loss_val:.6f}\")\n",
    "                    print(f\"    Mean activation strength: {mean_act_strength:.6f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"    ERROR training {strategy}: {str(e)}\")\n",
    "                    # Log error but continue with other experiments\n",
    "                    continue\n",
    "\n",
    "# Save results\n",
    "df_init = pd.DataFrame(initialization_results)\n",
    "df_init.to_csv('experiment1_initialization_results.csv', index=False)\n",
    "print(\"\\n✓ Experiment 1 results saved to 'experiment1_initialization_results.csv'\")\n",
    "print(f\"  Total experiments completed: {len(df_init)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 2: Auxiliary Loss Effects\n",
    "# Tests: SAE with/without auxiliary loss\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPERIMENT 2: AUXILIARY LOSS EFFECTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "auxloss_results = defaultdict(list)\n",
    "\n",
    "for dataset_name in ['mnist', 'olivetti', 'lfw']:\n",
    "    print(f\"\\n### Processing {dataset_name.upper()} Dataset ###\")\n",
    "\n",
    "    config = experiment_configs[dataset_name]\n",
    "\n",
    "    # Load data\n",
    "    train_loader, test_loader, dataset_mean = load_data(\n",
    "        dataset_name,\n",
    "        batch_size=config['batch_size'],\n",
    "        img_size=64\n",
    "    )\n",
    "\n",
    "    for hidden_size in config['hidden_sizes']:\n",
    "        k_top = config['k_tops'][1]  # Use middle k_top value\n",
    "\n",
    "        print(f\"\\nHidden Size: {hidden_size}, k_top: {k_top}\")\n",
    "\n",
    "        # Test with and without auxiliary loss\n",
    "        for use_aux_loss in [False, True]:\n",
    "            model_type = 'SAE_AuxLoss' if use_aux_loss else 'SAE'\n",
    "            print(f\"  Training {'WITH' if use_aux_loss else 'WITHOUT'} auxiliary loss...\")\n",
    "\n",
    "            set_seeds(42)\n",
    "\n",
    "            # Train model\n",
    "            model = train_sparse_autoencoder(\n",
    "                train_loader=train_loader,\n",
    "                num_epochs=config['num_epochs'],\n",
    "                learning_rate=config['learning_rate'],\n",
    "                input_size=config['input_size'],\n",
    "                hidden_size=hidden_size,\n",
    "                k_top=k_top,\n",
    "                modelType=model_type,\n",
    "                dataset_type=dataset_name,\n",
    "                k_aux=2 * k_top if use_aux_loss else None,\n",
    "                k_aux_param= 0.1,\n",
    "                dead_feature_threshold=500\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            test_loss_val = test_loss(model, test_loader, dataset_name)\n",
    "            dead_neurons_count = count_dead_neurons(model, train_loader, dataset_name)\n",
    "            activation_stats = get_activation_statistics(model, train_loader, dataset_name)\n",
    "\n",
    "            # Store results\n",
    "            auxloss_results['dataset'].append(dataset_name)\n",
    "            auxloss_results['hidden_size'].append(hidden_size)\n",
    "            auxloss_results['k_top'].append(k_top)\n",
    "            auxloss_results['aux_loss'].append('Yes' if use_aux_loss else 'No')\n",
    "            auxloss_results['test_loss'].append(test_loss_val)\n",
    "            auxloss_results['dead_neurons'].append(dead_neurons_count)\n",
    "            auxloss_results['dead_neuron_pct'].append(100 * dead_neurons_count / hidden_size)\n",
    "            auxloss_results['active_features'].append(activation_stats['active_features'])\n",
    "            auxloss_results['mean_activation_freq'].append(\n",
    "                activation_stats['mean_activation_frequency']\n",
    "            )\n",
    "\n",
    "            print(f\"    Dead neurons: {dead_neurons_count}/{hidden_size} \"\n",
    "                  f\"({100 * dead_neurons_count / hidden_size:.2f}%)\")\n",
    "\n",
    "# Save results\n",
    "df_aux = pd.DataFrame(auxloss_results)\n",
    "df_aux.to_csv('experiment2_auxiliary_loss_results.csv', index=False)\n",
    "print(\"\\nExperiment 2 results saved to 'experiment2_auxiliary_loss_results.csv'\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 3: Combined Effects (Init + AuxLoss)\n",
    "# Tests all combinations\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPERIMENT 3: COMBINED EFFECTS (Initialization + Auxiliary Loss)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "combined_results = defaultdict(list)\n",
    "\n",
    "for dataset_name in ['mnist', 'olivetti', 'lfw']:\n",
    "    print(f\"\\n### Processing {dataset_name.upper()} Dataset ###\")\n",
    "\n",
    "    config = experiment_configs[dataset_name]\n",
    "\n",
    "    # Load data\n",
    "    train_loader, test_loader, dataset_mean = load_data(\n",
    "        dataset_name,\n",
    "        batch_size=config['batch_size'],\n",
    "        img_size=64\n",
    "    )\n",
    "\n",
    "    hidden_size = config['hidden_sizes'][0]  # Use middle size\n",
    "    k_top = config['k_tops'][1]\n",
    "\n",
    "    print(f\"\\nHidden Size: {hidden_size}, k_top: {k_top}\")\n",
    "\n",
    "    # Test all combinations\n",
    "    model_configs = [\n",
    "        ('SAE', 'None', 'None'),\n",
    "        ('SAE_Init', 'Tied', 'None'),\n",
    "        ('SAE_AuxLoss', 'None', 'AuxLoss'),\n",
    "        ('Complete', 'Tied', 'AuxLoss')\n",
    "    ]\n",
    "\n",
    "    for model_type, init_type, aux_type in model_configs:\n",
    "        print(f\"  Training {model_type}...\")\n",
    "\n",
    "        set_seeds(42)\n",
    "\n",
    "        # Train model\n",
    "        model = train_sparse_autoencoder(\n",
    "            train_loader=train_loader,\n",
    "            num_epochs=config['num_epochs'],\n",
    "            learning_rate=config['learning_rate'],\n",
    "            input_size=config['input_size'],\n",
    "            hidden_size=hidden_size,\n",
    "            k_top=k_top,\n",
    "            modelType=model_type,\n",
    "            dataset_type=dataset_name,\n",
    "            k_aux=2 * k_top if 'AuxLoss' in aux_type else None,\n",
    "            k_aux_param=1 / 32,\n",
    "            dead_feature_threshold=200,\n",
    "            JumpReLU=0.1\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # Evaluate\n",
    "        test_loss_val = test_loss(model, test_loader, dataset_name)\n",
    "        dead_neurons_count = count_dead_neurons(model, train_loader, dataset_name)\n",
    "        activation_stats = get_activation_statistics(model, train_loader, dataset_name)\n",
    "\n",
    "        # Store results\n",
    "        combined_results['dataset'].append(dataset_name)\n",
    "        combined_results['model_type'].append(model_type)\n",
    "        combined_results['initialization'].append(init_type)\n",
    "        combined_results['auxiliary_loss'].append(aux_type)\n",
    "        combined_results['test_loss'].append(test_loss_val)\n",
    "        combined_results['dead_neurons'].append(dead_neurons_count)\n",
    "        combined_results['dead_neuron_pct'].append(100 * dead_neurons_count / hidden_size)\n",
    "        combined_results['active_features'].append(activation_stats['active_features'])\n",
    "\n",
    "        print(f\"    Dead neurons: {dead_neurons_count}/{hidden_size} \"\n",
    "              f\"({100 * dead_neurons_count / hidden_size:.2f}%)\")\n",
    "\n",
    "# Save results\n",
    "df_combined = pd.DataFrame(combined_results)\n",
    "df_combined.to_csv('experiment3_combined_effects_results.csv', index=False)\n",
    "print(\"\\nExperiment 3 results saved to 'experiment3_combined_effects_results.csv'\")\n",
    "\n",
    "# ============================================================================\n",
    "# FIGURE GENERATION: Publication-Quality Plots\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING PUBLICATION-QUALITY FIGURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set style for publication\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Figure 1: Dead Neuron Percentage by Initialization Strategy\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "    df_subset = df_init[df_init['dataset'] == dataset]\n",
    "\n",
    "    # Group by initialization and hidden size\n",
    "    pivot_data = df_subset.pivot_table(\n",
    "        values='dead_neuron_pct',\n",
    "        index='hidden_size',\n",
    "        columns='initialization',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "\n",
    "    pivot_data.plot(kind='bar', ax=axes[idx], width=0.8)\n",
    "    axes[idx].set_title(f'{dataset.upper()} Dataset', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Hidden Size', fontsize=12)\n",
    "    axes[idx].set_ylabel('Dead Neurons (%)', fontsize=12)\n",
    "    axes[idx].legend(title='Initialization', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    axes[idx].set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure1_dead_neurons_by_initialization.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure1_dead_neurons_by_initialization.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure1_dead_neurons_by_initialization.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 2: Auxiliary Loss Impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel A: Dead neuron percentage\n",
    "ax = axes[0]\n",
    "df_aux_grouped = df_aux.groupby(['dataset', 'aux_loss'])['dead_neuron_pct'].mean().reset_index()\n",
    "pivot_aux = df_aux_grouped.pivot(index='dataset', columns='aux_loss', values='dead_neuron_pct')\n",
    "pivot_aux.plot(kind='bar', ax=ax, width=0.7, color=['#d62728', '#2ca02c'])\n",
    "ax.set_title('Dead Neuron Percentage\\nWith/Without Auxiliary Loss', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Dead Neurons (%)', fontsize=12)\n",
    "ax.legend(title='Auxiliary Loss', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel B: Test loss comparison\n",
    "ax = axes[1]\n",
    "df_loss_grouped = df_aux.groupby(['dataset', 'aux_loss'])['test_loss'].mean().reset_index()\n",
    "pivot_loss = df_loss_grouped.pivot(index='dataset', columns='aux_loss', values='test_loss')\n",
    "pivot_loss.plot(kind='bar', ax=ax, width=0.7, color=['#d62728', '#2ca02c'])\n",
    "ax.set_title('Reconstruction Loss\\nWith/Without Auxiliary Loss', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Test Loss (MSE)', fontsize=12)\n",
    "ax.legend(title='Auxiliary Loss', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure2_auxiliary_loss_effects.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure2_auxiliary_loss_effects.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure2_auxiliary_loss_effects.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 3: Combined Effects Summary\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Panel A: Dead neurons by model type\n",
    "ax = axes[0, 0]\n",
    "df_combined_grouped = df_combined.groupby(['dataset', 'model_type'])['dead_neuron_pct'].mean().reset_index()\n",
    "pivot_combined = df_combined_grouped.pivot(index='dataset', columns='model_type', values='dead_neuron_pct')\n",
    "pivot_combined.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Dead Neuron Percentage by Model Configuration', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Dead Neurons (%)', fontsize=12)\n",
    "ax.legend(title='Model Type', fontsize=9, loc='upper right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel B: Test loss by model type\n",
    "ax = axes[0, 1]\n",
    "df_loss_combined = df_combined.groupby(['dataset', 'model_type'])['test_loss'].mean().reset_index()\n",
    "pivot_loss_combined = df_loss_combined.pivot(index='dataset', columns='model_type', values='test_loss')\n",
    "pivot_loss_combined.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Reconstruction Loss by Model Configuration', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Test Loss (MSE)', fontsize=12)\n",
    "ax.legend(title='Model Type', fontsize=9, loc='upper right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel C: Scatter plot - Dead neurons vs Test loss\n",
    "ax = axes[1, 0]\n",
    "for model_type in df_combined['model_type'].unique():\n",
    "    df_model = df_combined[df_combined['model_type'] == model_type]\n",
    "    ax.scatter(df_model['dead_neuron_pct'], df_model['test_loss'],\n",
    "               label=model_type, s=100, alpha=0.7)\n",
    "ax.set_xlabel('Dead Neurons (%)', fontsize=12)\n",
    "ax.set_ylabel('Test Loss (MSE)', fontsize=12)\n",
    "ax.set_title('Dead Neurons vs Reconstruction Performance', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Model Type', fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Panel D: Active features comparison\n",
    "ax = axes[1, 1]\n",
    "df_active = df_combined.groupby(['dataset', 'model_type'])['active_features'].mean().reset_index()\n",
    "pivot_active = df_active.pivot(index='dataset', columns='model_type', values='active_features')\n",
    "pivot_active.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Active Feature Count by Model Configuration', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Number of Active Features', fontsize=12)\n",
    "ax.legend(title='Model Type', fontsize=9)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure3_combined_effects_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure3_combined_effects_summary.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure3_combined_effects_summary.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 4: Activation Statistics Histograms\n",
    "# Compare best vs worst configurations\n",
    "print(\"\\nGenerating activation histograms for best/worst models...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for idx, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "    print(f\"  Processing {dataset}...\")\n",
    "\n",
    "    config = experiment_configs[dataset]\n",
    "    train_loader, test_loader, dataset_mean = load_data(\n",
    "        dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        img_size=64\n",
    "    )\n",
    "\n",
    "    hidden_size = config['hidden_sizes'][1]\n",
    "    k_top = config['k_tops'][1]\n",
    "\n",
    "    # Train baseline (worst) and complete (best) models\n",
    "    set_seeds(42)\n",
    "    model_baseline = train_sparse_autoencoder(\n",
    "        train_loader, num_epochs=config['num_epochs'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        input_size=config['input_size'], hidden_size=hidden_size,\n",
    "        k_top=k_top, modelType='SAE', dataset_type=dataset\n",
    "    )\n",
    "\n",
    "    set_seeds(42)\n",
    "    model_complete = train_sparse_autoencoder(\n",
    "        train_loader, num_epochs=config['num_epochs'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        input_size=config['input_size'], hidden_size=hidden_size,\n",
    "        k_top=k_top, modelType='Complete', dataset_type=dataset,\n",
    "        k_aux=2 * k_top, k_aux_param=1 / 32, dead_feature_threshold=1000,\n",
    "        JumpReLU=0.1\n",
    "    )\n",
    "\n",
    "    # Get activation statistics\n",
    "    stats_baseline = get_activation_statistics(model_baseline, train_loader, dataset)\n",
    "    stats_complete = get_activation_statistics(model_complete, train_loader, dataset)\n",
    "\n",
    "    # Plot histograms\n",
    "    ax = axes[0, idx]\n",
    "    ax.hist(stats_baseline['activation_frequencies'], bins=50, alpha=0.6,\n",
    "            label='Baseline SAE', color='red', edgecolor='black')\n",
    "    ax.hist(stats_complete['activation_frequencies'], bins=50, alpha=0.6,\n",
    "            label='Complete SAE', color='green', edgecolor='black')\n",
    "    ax.set_xlabel('Activation Frequency', fontsize=11)\n",
    "    ax.set_ylabel('Number of Features', fontsize=11)\n",
    "    ax.set_title(f'{dataset.upper()}: Activation Frequencies', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    ax = axes[1, idx]\n",
    "    active_base = stats_baseline['activation_strengths'][stats_baseline['activation_strengths'] > 0]\n",
    "    active_complete = stats_complete['activation_strengths'][stats_complete['activation_strengths'] > 0]\n",
    "    ax.hist(active_base, bins=50, alpha=0.6, label='Baseline SAE',\n",
    "            color='red', edgecolor='black')\n",
    "    ax.hist(active_complete, bins=50, alpha=0.6, label='Complete SAE',\n",
    "            color='green', edgecolor='black')\n",
    "    ax.set_xlabel('Mean Activation Strength', fontsize=11)\n",
    "    ax.set_ylabel('Number of Features', fontsize=11)\n",
    "    ax.set_title(f'{dataset.upper()}: Activation Strengths', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure4_activation_histograms.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure4_activation_histograms.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure4_activation_histograms.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Generate Summary Statistics Table (LaTeX format)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING SUMMARY STATISTICS TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comprehensive summary table\n",
    "summary_stats = []\n",
    "\n",
    "for dataset in ['mnist', 'olivetti', 'lfw']:\n",
    "    for model_type in ['SAE', 'SAE_Init', 'SAE_AuxLoss', 'Complete']:\n",
    "        df_subset = df_combined[\n",
    "            (df_combined['dataset'] == dataset) &\n",
    "            (df_combined['model_type'] == model_type)\n",
    "            ]\n",
    "\n",
    "        if len(df_subset) > 0:\n",
    "            summary_stats.append({\n",
    "                'Dataset': dataset.upper(),\n",
    "                'Model': model_type,\n",
    "                'Dead (%)': f\"{df_subset['dead_neuron_pct'].mean():.2f} ± {df_subset['dead_neuron_pct'].std():.2f}\",\n",
    "                'Test Loss': f\"{df_subset['test_loss'].mean():.6f} ± {df_subset['test_loss'].std():.6f}\",\n",
    "                'Active Features': f\"{df_subset['active_features'].mean():.0f}\"\n",
    "            })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_stats)\n",
    "\n",
    "# Save as CSV\n",
    "df_summary.to_csv('summary_statistics.csv', index=False)\n",
    "print(\"Saved: summary_statistics.csv\")\n",
    "\n",
    "# Save as LaTeX table\n",
    "latex_table = df_summary.to_latex(index=False, escape=False, column_format='lllll')\n",
    "with open('results/dead neurons/summary_statistics.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "print(\"Saved: summary_statistics.tex\")\n",
    "\n",
    "# Print to console\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL EXPERIMENTS COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(\"  - experiment1_initialization_results.csv\")\n",
    "print(\"  - experiment2_auxiliary_loss_results.csv\")\n",
    "print(\"  - experiment3_combined_effects_results.csv\")\n",
    "print(\"  - figure1_dead_neurons_by_initialization.png/pdf\")\n",
    "print(\"  - figure2_auxiliary_loss_effects.png/pdf\")\n",
    "print(\"  - figure3_combined_effects_summary.png/pdf\")\n",
    "print(\"  - figure4_activation_histograms.png/pdf\")\n",
    "print(\"  - summary_statistics.csv\")\n",
    "print(\"  - summary_statistics.tex\")"
   ],
   "id": "147e6a073d704544"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# ============================================================================\n",
    "# FIGURE GENERATION: Publication-Quality Plots\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING PUBLICATION-QUALITY FIGURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set style for publication\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Figure 1: Dead Neuron Percentage by Initialization Strategy\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "    df_subset = df_init[df_init['dataset'] == dataset]\n",
    "\n",
    "    # Group by initialization and hidden size\n",
    "    pivot_data = df_subset.pivot_table(\n",
    "        values='dead_neuron_pct',\n",
    "        index='hidden_size',\n",
    "        columns='initialization',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "\n",
    "    pivot_data.plot(kind='bar', ax=axes[idx], width=0.8)\n",
    "    axes[idx].set_title(f'{dataset.upper()} Dataset', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Hidden Size', fontsize=12)\n",
    "    axes[idx].set_ylabel('Dead Neurons (%)', fontsize=12)\n",
    "    axes[idx].legend(title='Initialization', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    axes[idx].set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure1_dead_neurons_by_initialization.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure1_dead_neurons_by_initialization.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure1_dead_neurons_by_initialization.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 2: Auxiliary Loss Impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel A: Dead neuron percentage\n",
    "ax = axes[0]\n",
    "df_aux_grouped = df_aux.groupby(['dataset', 'aux_loss'])['dead_neuron_pct'].mean().reset_index()\n",
    "pivot_aux = df_aux_grouped.pivot(index='dataset', columns='aux_loss', values='dead_neuron_pct')\n",
    "pivot_aux.plot(kind='bar', ax=ax, width=0.7, color=['#d62728', '#2ca02c'])\n",
    "ax.set_title('Dead Neuron Percentage\\nWith/Without Auxiliary Loss', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Dead Neurons (%)', fontsize=12)\n",
    "ax.legend(title='Auxiliary Loss', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel B: Test loss comparison\n",
    "ax = axes[1]\n",
    "df_loss_grouped = df_aux.groupby(['dataset', 'aux_loss'])['test_loss'].mean().reset_index()\n",
    "pivot_loss = df_loss_grouped.pivot(index='dataset', columns='aux_loss', values='test_loss')\n",
    "pivot_loss.plot(kind='bar', ax=ax, width=0.7, color=['#d62728', '#2ca02c'])\n",
    "ax.set_title('Reconstruction Loss\\nWith/Without Auxiliary Loss', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Test Loss (MSE)', fontsize=12)\n",
    "ax.legend(title='Auxiliary Loss', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure2_auxiliary_loss_effects.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure2_auxiliary_loss_effects.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure2_auxiliary_loss_effects.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 3: Combined Effects Summary\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Panel A: Dead neurons by model type\n",
    "ax = axes[0, 0]\n",
    "df_combined_grouped = df_combined.groupby(['dataset', 'model_type'])['dead_neuron_pct'].mean().reset_index()\n",
    "pivot_combined = df_combined_grouped.pivot(index='dataset', columns='model_type', values='dead_neuron_pct')\n",
    "pivot_combined.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Dead Neuron Percentage by Model Configuration', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Dead Neurons (%)', fontsize=12)\n",
    "ax.legend(title='Model Type', fontsize=9, loc='upper right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel B: Test loss by model type\n",
    "ax = axes[0, 1]\n",
    "df_loss_combined = df_combined.groupby(['dataset', 'model_type'])['test_loss'].mean().reset_index()\n",
    "pivot_loss_combined = df_loss_combined.pivot(index='dataset', columns='model_type', values='test_loss')\n",
    "pivot_loss_combined.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Reconstruction Loss by Model Configuration', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Test Loss (MSE)', fontsize=12)\n",
    "ax.legend(title='Model Type', fontsize=9, loc='upper right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel C: Scatter plot - Dead neurons vs Test loss\n",
    "ax = axes[1, 0]\n",
    "for model_type in df_combined['model_type'].unique():\n",
    "    df_model = df_combined[df_combined['model_type'] == model_type]\n",
    "    ax.scatter(df_model['dead_neuron_pct'], df_model['test_loss'],\n",
    "               label=model_type, s=100, alpha=0.7)\n",
    "ax.set_xlabel('Dead Neurons (%)', fontsize=12)\n",
    "ax.set_ylabel('Test Loss (MSE)', fontsize=12)\n",
    "ax.set_title('Dead Neurons vs Reconstruction Performance', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Model Type', fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Panel D: Active features comparison\n",
    "ax = axes[1, 1]\n",
    "df_active = df_combined.groupby(['dataset', 'model_type'])['active_features'].mean().reset_index()\n",
    "pivot_active = df_active.pivot(index='dataset', columns='model_type', values='active_features')\n",
    "pivot_active.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Active Feature Count by Model Configuration', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Number of Active Features', fontsize=12)\n",
    "ax.legend(title='Model Type', fontsize=9)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure3_combined_effects_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure3_combined_effects_summary.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure3_combined_effects_summary.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 4: Activation Statistics Histograms\n",
    "# Compare best vs worst configurations\n",
    "print(\"\\nGenerating activation histograms for best/worst models...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for idx, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "    print(f\"  Processing {dataset}...\")\n",
    "\n",
    "    config = experiment_configs[dataset]\n",
    "    train_loader, test_loader, dataset_mean = load_data(\n",
    "        dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        img_size=64\n",
    "    )\n",
    "\n",
    "    hidden_size = config['hidden_sizes'][1]\n",
    "    k_top = config['k_tops'][1]\n",
    "\n",
    "    # Train baseline (worst) and complete (best) models\n",
    "    set_seeds(42)\n",
    "    model_baseline = train_sparse_autoencoder(\n",
    "        train_loader, num_epochs=config['num_epochs'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        input_size=config['input_size'], hidden_size=hidden_size,\n",
    "        k_top=k_top, modelType='SAE', dataset_type=dataset\n",
    "    )\n",
    "\n",
    "    set_seeds(42)\n",
    "    model_complete = train_sparse_autoencoder(\n",
    "        train_loader, num_epochs=config['num_epochs'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        input_size=config['input_size'], hidden_size=hidden_size,\n",
    "        k_top=k_top, modelType='Complete', dataset_type=dataset,\n",
    "        k_aux=2 * k_top, k_aux_param=1 / 32, dead_feature_threshold=1000,\n",
    "        JumpReLU=0.1\n",
    "    )\n",
    "\n",
    "    # Get activation statistics\n",
    "    stats_baseline = get_activation_statistics(model_baseline, train_loader, dataset)\n",
    "    stats_complete = get_activation_statistics(model_complete, train_loader, dataset)\n",
    "\n",
    "    # Plot histograms\n",
    "    ax = axes[0, idx]\n",
    "    ax.hist(stats_baseline['activation_frequencies'], bins=50, alpha=0.6,\n",
    "            label='Baseline SAE', color='red', edgecolor='black')\n",
    "    ax.hist(stats_complete['activation_frequencies'], bins=50, alpha=0.6,\n",
    "            label='Complete SAE', color='green', edgecolor='black')\n",
    "    ax.set_xlabel('Activation Frequency', fontsize=11)\n",
    "    ax.set_ylabel('Number of Features', fontsize=11)\n",
    "    ax.set_title(f'{dataset.upper()}: Activation Frequencies', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    ax = axes[1, idx]\n",
    "    active_base = stats_baseline['activation_strengths'][stats_baseline['activation_strengths'] > 0]\n",
    "    active_complete = stats_complete['activation_strengths'][stats_complete['activation_strengths'] > 0]\n",
    "    ax.hist(active_base, bins=50, alpha=0.6, label='Baseline SAE',\n",
    "            color='red', edgecolor='black')\n",
    "    ax.hist(active_complete, bins=50, alpha=0.6, label='Complete SAE',\n",
    "            color='green', edgecolor='black')\n",
    "    ax.set_xlabel('Mean Activation Strength', fontsize=11)\n",
    "    ax.set_ylabel('Number of Features', fontsize=11)\n",
    "    ax.set_title(f'{dataset.upper()}: Activation Strengths', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure4_activation_histograms.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure4_activation_histograms.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure4_activation_histograms.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Generate Summary Statistics Table (LaTeX format)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING SUMMARY STATISTICS TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comprehensive summary table\n",
    "summary_stats = []\n",
    "\n",
    "for dataset in ['mnist', 'olivetti', 'lfw']:\n",
    "    for model_type in ['SAE', 'SAE_Init', 'SAE_AuxLoss', 'Complete']:\n",
    "        df_subset = df_combined[\n",
    "            (df_combined['dataset'] == dataset) &\n",
    "            (df_combined['model_type'] == model_type)\n",
    "            ]\n",
    "\n",
    "        if len(df_subset) > 0:\n",
    "            summary_stats.append({\n",
    "                'Dataset': dataset.upper(),\n",
    "                'Model': model_type,\n",
    "                'Dead (%)': f\"{df_subset['dead_neuron_pct'].mean():.2f} ± {df_subset['dead_neuron_pct'].std():.2f}\",\n",
    "                'Test Loss': f\"{df_subset['test_loss'].mean():.6f} ± {df_subset['test_loss'].std():.6f}\",\n",
    "                'Active Features': f\"{df_subset['active_features'].mean():.0f}\"\n",
    "            })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_stats)\n",
    "\n",
    "# Save as CSV\n",
    "df_summary.to_csv('summary_statistics.csv', index=False)\n",
    "print(\"Saved: summary_statistics.csv\")\n",
    "\n",
    "# Save as LaTeX table\n",
    "latex_table = df_summary.to_latex(index=False, escape=False, column_format='lllll')\n",
    "with open('results/dead neurons/summary_statistics.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "print(\"Saved: summary_statistics.tex\")\n",
    "\n",
    "# Print to console\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL EXPERIMENTS COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(\"  - experiment1_initialization_results.csv\")\n",
    "print(\"  - experiment2_auxiliary_loss_results.csv\")\n",
    "print(\"  - experiment3_combined_effects_results.csv\")\n",
    "print(\"  - figure1_dead_neurons_by_initialization.png/pdf\")\n",
    "print(\"  - figure2_auxiliary_loss_effects.png/pdf\")\n",
    "print(\"  - figure3_combined_effects_summary.png/pdf\")\n",
    "print(\"  - figure4_activation_histograms.png/pdf\")\n",
    "print(\"  - summary_statistics.csv\")\n",
    "print(\"  - summary_statistics.tex\")"
   ],
   "id": "c3e7e29b91d3e506"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T06:32:12.107775Z",
     "start_time": "2025-10-23T06:23:11.771120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 3: Combined Effects (Init + AuxLoss)\n",
    "# Tests all combinations\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPERIMENT 3: COMBINED EFFECTS (Initialization + Auxiliary Loss)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "combined_results = defaultdict(list)\n",
    "\n",
    "for dataset_name in ['mnist', 'olivetti', 'lfw']:\n",
    "    print(f\"\\n### Processing {dataset_name.upper()} Dataset ###\")\n",
    "\n",
    "    config = experiment_configs[dataset_name]\n",
    "\n",
    "    # Load data\n",
    "    train_loader, test_loader, dataset_mean = load_data(\n",
    "        dataset_name,\n",
    "        batch_size=config['batch_size'],\n",
    "        img_size=64\n",
    "    )\n",
    "\n",
    "    hidden_size = config['hidden_sizes'][0]  # Use middle size\n",
    "    k_top = config['k_tops'][1]\n",
    "\n",
    "    print(f\"\\nHidden Size: {hidden_size}, k_top: {k_top}\")\n",
    "\n",
    "    # Test all combinations\n",
    "    model_configs = [\n",
    "        ('SAE', 'None', 'None'),\n",
    "        ('SAE_Init', 'Tied', 'None'),\n",
    "        ('SAE_AuxLoss', 'None', 'AuxLoss'),\n",
    "        ('Complete', 'Tied', 'AuxLoss')\n",
    "    ]\n",
    "\n",
    "    for model_type, init_type, aux_type in model_configs:\n",
    "        print(f\"  Training {model_type}...\")\n",
    "\n",
    "        set_seeds(42)\n",
    "\n",
    "        # Train model\n",
    "        model = train_sparse_autoencoder(\n",
    "            train_loader=train_loader,\n",
    "            num_epochs=config['num_epochs'],\n",
    "            learning_rate=config['learning_rate'],\n",
    "            input_size=config['input_size'],\n",
    "            hidden_size=hidden_size,\n",
    "            k_top=k_top,\n",
    "            modelType=model_type,\n",
    "            dataset_type=dataset_name,\n",
    "            k_aux=2 * k_top if 'AuxLoss' in aux_type else None,\n",
    "            k_aux_param=1 / 32,\n",
    "            dead_feature_threshold=200,\n",
    "            JumpReLU=0.1\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # Evaluate\n",
    "        test_loss_val = test_loss(model, test_loader, dataset_name)\n",
    "        dead_neurons_count = count_dead_neurons(model, train_loader, dataset_name)\n",
    "        activation_stats = get_activation_statistics(model, train_loader, dataset_name)\n",
    "\n",
    "        # Store results\n",
    "        combined_results['dataset'].append(dataset_name)\n",
    "        combined_results['model_type'].append(model_type)\n",
    "        combined_results['initialization'].append(init_type)\n",
    "        combined_results['auxiliary_loss'].append(aux_type)\n",
    "        combined_results['test_loss'].append(test_loss_val)\n",
    "        combined_results['dead_neurons'].append(dead_neurons_count)\n",
    "        combined_results['dead_neuron_pct'].append(100 * dead_neurons_count / hidden_size)\n",
    "        combined_results['active_features'].append(activation_stats['active_features'])\n",
    "\n",
    "        print(f\"    Dead neurons: {dead_neurons_count}/{hidden_size} \"\n",
    "              f\"({100 * dead_neurons_count / hidden_size:.2f}%)\")\n",
    "\n",
    "# Save results\n",
    "df_combined = pd.DataFrame(combined_results)\n",
    "df_combined.to_csv('experiment3_combined_effects_results.csv', index=False)\n",
    "print(\"\\nExperiment 3 results saved to 'experiment3_combined_effects_results.csv'\")\n"
   ],
   "id": "4b0ed4cf16dfa143",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXPERIMENT 3: COMBINED EFFECTS (Initialization + Auxiliary Loss)\n",
      "================================================================================\n",
      "\n",
      "### Processing MNIST Dataset ###\n",
      "\n",
      "Hidden Size: 128, k_top: 10\n",
      "  Training SAE...\n",
      "Epoch [1/50], Loss: 0.9370\n",
      "Epoch [2/50], Loss: 0.5011\n",
      "Epoch [3/50], Loss: 0.4658\n",
      "Epoch [4/50], Loss: 0.4542\n",
      "Epoch [5/50], Loss: 0.4495\n",
      "Epoch [6/50], Loss: 0.4475\n",
      "Epoch [7/50], Loss: 0.4462\n",
      "Epoch [8/50], Loss: 0.4449\n",
      "Epoch [9/50], Loss: 0.4433\n",
      "Epoch [10/50], Loss: 0.4422\n",
      "Epoch [11/50], Loss: 0.4415\n",
      "Epoch [12/50], Loss: 0.4408\n",
      "Epoch [13/50], Loss: 0.4402\n",
      "Epoch [14/50], Loss: 0.4394\n",
      "Epoch [15/50], Loss: 0.4387\n",
      "Epoch [16/50], Loss: 0.4379\n",
      "Epoch [17/50], Loss: 0.4372\n",
      "Epoch [18/50], Loss: 0.4369\n",
      "Epoch [19/50], Loss: 0.4364\n",
      "Epoch [20/50], Loss: 0.4361\n",
      "Epoch [21/50], Loss: 0.4359\n",
      "Epoch [22/50], Loss: 0.4357\n",
      "Epoch [23/50], Loss: 0.4355\n",
      "Epoch [24/50], Loss: 0.4352\n",
      "Epoch [25/50], Loss: 0.4350\n",
      "Epoch [26/50], Loss: 0.4348\n",
      "Epoch [27/50], Loss: 0.4345\n",
      "Epoch [28/50], Loss: 0.4342\n",
      "Epoch [29/50], Loss: 0.4341\n",
      "Epoch [30/50], Loss: 0.4340\n",
      "Epoch [31/50], Loss: 0.4339\n",
      "Epoch [32/50], Loss: 0.4340\n",
      "Epoch [33/50], Loss: 0.4340\n",
      "Epoch [34/50], Loss: 0.4341\n",
      "Epoch [35/50], Loss: 0.4341\n",
      "Epoch [36/50], Loss: 0.4339\n",
      "Epoch [37/50], Loss: 0.4339\n",
      "Epoch [38/50], Loss: 0.4339\n",
      "Epoch [39/50], Loss: 0.4338\n",
      "Epoch [40/50], Loss: 0.4337\n",
      "Epoch [41/50], Loss: 0.4337\n",
      "Epoch [42/50], Loss: 0.4337\n",
      "Epoch [43/50], Loss: 0.4336\n",
      "Epoch [44/50], Loss: 0.4335\n",
      "Epoch [45/50], Loss: 0.4334\n",
      "Epoch [46/50], Loss: 0.4335\n",
      "Epoch [47/50], Loss: 0.4334\n",
      "Epoch [48/50], Loss: 0.4334\n",
      "Epoch [49/50], Loss: 0.4333\n",
      "Epoch [50/50], Loss: 0.4334\n",
      "Finished Training\n",
      "Test Loss for Default Sparse Autoencoder: 0.427059\n",
      "Number of dead neurons in Default Sparse Autoencoder: 103 out of 128 (80.47%)\n",
      "\n",
      "=== Activation Statistics for Default Sparse Autoencoder ===\n",
      "Total features: 128\n",
      "Dead features: 103 (80.47%)\n",
      "Active features: 25 (19.53%)\n",
      "Mean activation frequency: 0.0781\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.153683\n",
      "    Dead neurons: 103/128 (80.47%)\n",
      "  Training SAE_Init...\n",
      "Epoch [1/50], Loss: 0.7793\n",
      "Epoch [2/50], Loss: 0.3920\n",
      "Epoch [3/50], Loss: 0.3638\n",
      "Epoch [4/50], Loss: 0.3531\n",
      "Epoch [5/50], Loss: 0.3480\n",
      "Epoch [6/50], Loss: 0.3449\n",
      "Epoch [7/50], Loss: 0.3427\n",
      "Epoch [8/50], Loss: 0.3409\n",
      "Epoch [9/50], Loss: 0.3397\n",
      "Epoch [10/50], Loss: 0.3390\n",
      "Epoch [11/50], Loss: 0.3382\n",
      "Epoch [12/50], Loss: 0.3377\n",
      "Epoch [13/50], Loss: 0.3372\n",
      "Epoch [14/50], Loss: 0.3367\n",
      "Epoch [15/50], Loss: 0.3365\n",
      "Epoch [16/50], Loss: 0.3361\n",
      "Epoch [17/50], Loss: 0.3359\n",
      "Epoch [18/50], Loss: 0.3360\n",
      "Epoch [19/50], Loss: 0.3361\n",
      "Epoch [20/50], Loss: 0.3363\n",
      "Epoch [21/50], Loss: 0.3363\n",
      "Epoch [22/50], Loss: 0.3364\n",
      "Epoch [23/50], Loss: 0.3364\n",
      "Epoch [24/50], Loss: 0.3364\n",
      "Epoch [25/50], Loss: 0.3363\n",
      "Epoch [26/50], Loss: 0.3357\n",
      "Epoch [27/50], Loss: 0.3357\n",
      "Epoch [28/50], Loss: 0.3356\n",
      "Epoch [29/50], Loss: 0.3355\n",
      "Epoch [30/50], Loss: 0.3355\n",
      "Epoch [31/50], Loss: 0.3355\n",
      "Epoch [32/50], Loss: 0.3356\n",
      "Epoch [33/50], Loss: 0.3355\n",
      "Epoch [34/50], Loss: 0.3356\n",
      "Epoch [35/50], Loss: 0.3356\n",
      "Epoch [36/50], Loss: 0.3356\n",
      "Epoch [37/50], Loss: 0.3357\n",
      "Epoch [38/50], Loss: 0.3358\n",
      "Epoch [39/50], Loss: 0.3358\n",
      "Epoch [40/50], Loss: 0.3358\n",
      "Epoch [41/50], Loss: 0.3357\n",
      "Epoch [42/50], Loss: 0.3358\n",
      "Epoch [43/50], Loss: 0.3371\n",
      "Epoch [44/50], Loss: 0.3369\n",
      "Epoch [45/50], Loss: 0.3370\n",
      "Epoch [46/50], Loss: 0.3370\n",
      "Epoch [47/50], Loss: 0.3373\n",
      "Epoch [48/50], Loss: 0.3370\n",
      "Epoch [49/50], Loss: 0.3373\n",
      "Epoch [50/50], Loss: 0.3370\n",
      "Finished Training\n",
      "Test Loss for Sparse Autoencoder with just weight initialization: 0.333827\n",
      "Number of dead neurons in Sparse Autoencoder with just weight initialization: 53 out of 128 (41.41%)\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with just weight initialization ===\n",
      "Total features: 128\n",
      "Dead features: 53 (41.41%)\n",
      "Active features: 75 (58.59%)\n",
      "Mean activation frequency: 0.0781\n",
      "Median activation frequency: 0.0677\n",
      "Mean activation strength (when active): 0.170654\n",
      "    Dead neurons: 53/128 (41.41%)\n",
      "  Training SAE_AuxLoss...\n",
      "Epoch [1/50], MSE Loss: 0.9459, Aux Loss: 0.0055, Total Loss: 0.9514\n",
      "Epoch [2/50], MSE Loss: 0.5335, Aux Loss: 0.0307, Total Loss: 0.5642\n",
      "Epoch [3/50], MSE Loss: 0.4981, Aux Loss: 0.0290, Total Loss: 0.5271\n",
      "Epoch [4/50], MSE Loss: 0.4878, Aux Loss: 0.0279, Total Loss: 0.5157\n",
      "Epoch [5/50], MSE Loss: 0.4807, Aux Loss: 0.0268, Total Loss: 0.5075\n",
      "Epoch [6/50], MSE Loss: 0.4775, Aux Loss: 0.0263, Total Loss: 0.5038\n",
      "Epoch [7/50], MSE Loss: 0.4756, Aux Loss: 0.0260, Total Loss: 0.5016\n",
      "Epoch [8/50], MSE Loss: 0.4744, Aux Loss: 0.0259, Total Loss: 0.5003\n",
      "Epoch [9/50], MSE Loss: 0.4741, Aux Loss: 0.0259, Total Loss: 0.5000\n",
      "Epoch [10/50], MSE Loss: 0.4738, Aux Loss: 0.0257, Total Loss: 0.4995\n",
      "Epoch [11/50], MSE Loss: 0.4747, Aux Loss: 0.0258, Total Loss: 0.5005\n",
      "Epoch [12/50], MSE Loss: 0.4750, Aux Loss: 0.0257, Total Loss: 0.5008\n",
      "Epoch [13/50], MSE Loss: 0.4757, Aux Loss: 0.0256, Total Loss: 0.5013\n",
      "Epoch [14/50], MSE Loss: 0.4761, Aux Loss: 0.0256, Total Loss: 0.5017\n",
      "Epoch [15/50], MSE Loss: 0.4763, Aux Loss: 0.0255, Total Loss: 0.5018\n",
      "Epoch [16/50], MSE Loss: 0.4765, Aux Loss: 0.0255, Total Loss: 0.5020\n",
      "Epoch [17/50], MSE Loss: 0.4766, Aux Loss: 0.0254, Total Loss: 0.5020\n",
      "Epoch [18/50], MSE Loss: 0.4768, Aux Loss: 0.0258, Total Loss: 0.5027\n",
      "Epoch [19/50], MSE Loss: 0.4767, Aux Loss: 0.0259, Total Loss: 0.5027\n",
      "Epoch [20/50], MSE Loss: 0.4765, Aux Loss: 0.0258, Total Loss: 0.5023\n",
      "Epoch [21/50], MSE Loss: 0.4763, Aux Loss: 0.0255, Total Loss: 0.5018\n",
      "Epoch [22/50], MSE Loss: 0.4764, Aux Loss: 0.0255, Total Loss: 0.5020\n",
      "Epoch [23/50], MSE Loss: 0.4761, Aux Loss: 0.0253, Total Loss: 0.5014\n",
      "Epoch [24/50], MSE Loss: 0.4760, Aux Loss: 0.0253, Total Loss: 0.5013\n",
      "Epoch [25/50], MSE Loss: 0.4761, Aux Loss: 0.0252, Total Loss: 0.5013\n",
      "Epoch [26/50], MSE Loss: 0.4760, Aux Loss: 0.0250, Total Loss: 0.5010\n",
      "Epoch [27/50], MSE Loss: 0.4757, Aux Loss: 0.0248, Total Loss: 0.5004\n",
      "Epoch [28/50], MSE Loss: 0.4758, Aux Loss: 0.0248, Total Loss: 0.5007\n",
      "Epoch [29/50], MSE Loss: 0.4758, Aux Loss: 0.0247, Total Loss: 0.5006\n",
      "Epoch [30/50], MSE Loss: 0.4758, Aux Loss: 0.0247, Total Loss: 0.5005\n",
      "Epoch [31/50], MSE Loss: 0.4758, Aux Loss: 0.0246, Total Loss: 0.5005\n",
      "Epoch [32/50], MSE Loss: 0.4758, Aux Loss: 0.0245, Total Loss: 0.5003\n",
      "Epoch [33/50], MSE Loss: 0.4759, Aux Loss: 0.0246, Total Loss: 0.5005\n",
      "Epoch [34/50], MSE Loss: 0.4758, Aux Loss: 0.0243, Total Loss: 0.5001\n",
      "Epoch [35/50], MSE Loss: 0.4762, Aux Loss: 0.0244, Total Loss: 0.5006\n",
      "Epoch [36/50], MSE Loss: 0.4770, Aux Loss: 0.0246, Total Loss: 0.5016\n",
      "Epoch [37/50], MSE Loss: 0.4791, Aux Loss: 0.0259, Total Loss: 0.5050\n",
      "Epoch [38/50], MSE Loss: 0.4782, Aux Loss: 0.0255, Total Loss: 0.5037\n",
      "Epoch [39/50], MSE Loss: 0.4781, Aux Loss: 0.0252, Total Loss: 0.5033\n",
      "Epoch [40/50], MSE Loss: 0.4782, Aux Loss: 0.0252, Total Loss: 0.5034\n",
      "Epoch [41/50], MSE Loss: 0.4781, Aux Loss: 0.0248, Total Loss: 0.5029\n",
      "Epoch [42/50], MSE Loss: 0.4770, Aux Loss: 0.0244, Total Loss: 0.5014\n",
      "Epoch [43/50], MSE Loss: 0.4756, Aux Loss: 0.0241, Total Loss: 0.4997\n",
      "Epoch [44/50], MSE Loss: 0.4748, Aux Loss: 0.0237, Total Loss: 0.4985\n",
      "Epoch [45/50], MSE Loss: 0.4743, Aux Loss: 0.0248, Total Loss: 0.4992\n",
      "Epoch [46/50], MSE Loss: 0.4726, Aux Loss: 0.0243, Total Loss: 0.4969\n",
      "Epoch [47/50], MSE Loss: 0.4710, Aux Loss: 0.0242, Total Loss: 0.4951\n",
      "Epoch [48/50], MSE Loss: 0.4696, Aux Loss: 0.0237, Total Loss: 0.4933\n",
      "Epoch [49/50], MSE Loss: 0.4692, Aux Loss: 0.0236, Total Loss: 0.4928\n",
      "Epoch [50/50], MSE Loss: 0.4683, Aux Loss: 0.0226, Total Loss: 0.4909\n",
      "Finished Training\n",
      "Test Loss for Sparse Autoencoder with Auxiliary Loss: 0.877357\n",
      "Number of dead neurons in Sparse Autoencoder with Auxiliary Loss: 102 out of 128 (79.69%)\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with Auxiliary Loss ===\n",
      "Total features: 128\n",
      "Dead features: 102 (79.69%)\n",
      "Active features: 26 (20.31%)\n",
      "Mean activation frequency: 0.0780\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.141407\n",
      "    Dead neurons: 102/128 (79.69%)\n",
      "  Training Complete...\n",
      "Epoch [1/50], MSE Loss: 0.7803, Aux Loss: 0.0325, Total Loss: 0.8128\n",
      "Epoch [2/50], MSE Loss: 0.3983, Aux Loss: 0.1999, Total Loss: 0.5982\n",
      "Epoch [3/50], MSE Loss: 0.3698, Aux Loss: 0.1986, Total Loss: 0.5684\n",
      "Epoch [4/50], MSE Loss: 0.3589, Aux Loss: 0.1985, Total Loss: 0.5573\n",
      "Epoch [5/50], MSE Loss: 0.3533, Aux Loss: 0.1972, Total Loss: 0.5506\n",
      "Epoch [6/50], MSE Loss: 0.3499, Aux Loss: 0.1968, Total Loss: 0.5467\n",
      "Epoch [7/50], MSE Loss: 0.3477, Aux Loss: 0.1958, Total Loss: 0.5435\n",
      "Epoch [8/50], MSE Loss: 0.3459, Aux Loss: 0.1947, Total Loss: 0.5406\n",
      "Epoch [9/50], MSE Loss: 0.3447, Aux Loss: 0.1944, Total Loss: 0.5390\n",
      "Epoch [10/50], MSE Loss: 0.3442, Aux Loss: 0.1937, Total Loss: 0.5378\n",
      "Epoch [11/50], MSE Loss: 0.3438, Aux Loss: 0.1938, Total Loss: 0.5376\n",
      "Epoch [12/50], MSE Loss: 0.3438, Aux Loss: 0.1938, Total Loss: 0.5376\n",
      "Epoch [13/50], MSE Loss: 0.3438, Aux Loss: 0.1940, Total Loss: 0.5378\n",
      "Epoch [14/50], MSE Loss: 0.3438, Aux Loss: 0.1945, Total Loss: 0.5383\n",
      "Epoch [15/50], MSE Loss: 0.3440, Aux Loss: 0.1945, Total Loss: 0.5385\n",
      "Epoch [16/50], MSE Loss: 0.3438, Aux Loss: 0.1935, Total Loss: 0.5373\n",
      "Epoch [17/50], MSE Loss: 0.3437, Aux Loss: 0.1916, Total Loss: 0.5354\n",
      "Epoch [18/50], MSE Loss: 0.3438, Aux Loss: 0.1895, Total Loss: 0.5333\n",
      "Epoch [19/50], MSE Loss: 0.3437, Aux Loss: 0.1874, Total Loss: 0.5312\n",
      "Epoch [20/50], MSE Loss: 0.3441, Aux Loss: 0.1865, Total Loss: 0.5306\n",
      "Epoch [21/50], MSE Loss: 0.3445, Aux Loss: 0.1852, Total Loss: 0.5297\n",
      "Epoch [22/50], MSE Loss: 0.3446, Aux Loss: 0.1860, Total Loss: 0.5306\n",
      "Epoch [23/50], MSE Loss: 0.3446, Aux Loss: 0.1813, Total Loss: 0.5258\n",
      "Epoch [24/50], MSE Loss: 0.3444, Aux Loss: 0.1776, Total Loss: 0.5221\n",
      "Epoch [25/50], MSE Loss: 0.3444, Aux Loss: 0.1756, Total Loss: 0.5200\n",
      "Epoch [26/50], MSE Loss: 0.3445, Aux Loss: 0.1737, Total Loss: 0.5182\n",
      "Epoch [27/50], MSE Loss: 0.3444, Aux Loss: 0.1719, Total Loss: 0.5163\n",
      "Epoch [28/50], MSE Loss: 0.3443, Aux Loss: 0.1700, Total Loss: 0.5143\n",
      "Epoch [29/50], MSE Loss: 0.3444, Aux Loss: 0.1683, Total Loss: 0.5126\n",
      "Epoch [30/50], MSE Loss: 0.3444, Aux Loss: 0.1663, Total Loss: 0.5107\n",
      "Epoch [31/50], MSE Loss: 0.3442, Aux Loss: 0.1654, Total Loss: 0.5097\n",
      "Epoch [32/50], MSE Loss: 0.3442, Aux Loss: 0.1647, Total Loss: 0.5089\n",
      "Epoch [33/50], MSE Loss: 0.3444, Aux Loss: 0.1644, Total Loss: 0.5088\n",
      "Epoch [34/50], MSE Loss: 0.3448, Aux Loss: 0.1640, Total Loss: 0.5088\n",
      "Epoch [35/50], MSE Loss: 0.3452, Aux Loss: 0.1637, Total Loss: 0.5089\n",
      "Epoch [36/50], MSE Loss: 0.3457, Aux Loss: 0.1635, Total Loss: 0.5092\n",
      "Epoch [37/50], MSE Loss: 0.3464, Aux Loss: 0.1634, Total Loss: 0.5098\n",
      "Epoch [38/50], MSE Loss: 0.3469, Aux Loss: 0.1636, Total Loss: 0.5105\n",
      "Epoch [39/50], MSE Loss: 0.3477, Aux Loss: 0.1639, Total Loss: 0.5117\n",
      "Epoch [40/50], MSE Loss: 0.3483, Aux Loss: 0.1645, Total Loss: 0.5128\n",
      "Epoch [41/50], MSE Loss: 0.3484, Aux Loss: 0.1651, Total Loss: 0.5135\n",
      "Epoch [42/50], MSE Loss: 0.3486, Aux Loss: 0.1652, Total Loss: 0.5138\n",
      "Epoch [43/50], MSE Loss: 0.3491, Aux Loss: 0.1655, Total Loss: 0.5146\n",
      "Epoch [44/50], MSE Loss: 0.3494, Aux Loss: 0.1655, Total Loss: 0.5149\n",
      "Epoch [45/50], MSE Loss: 0.3499, Aux Loss: 0.1659, Total Loss: 0.5157\n",
      "Epoch [46/50], MSE Loss: 0.3508, Aux Loss: 0.1665, Total Loss: 0.5173\n",
      "Epoch [47/50], MSE Loss: 0.3507, Aux Loss: 0.1674, Total Loss: 0.5182\n",
      "Epoch [48/50], MSE Loss: 0.3511, Aux Loss: 0.1674, Total Loss: 0.5184\n",
      "Epoch [49/50], MSE Loss: 0.3517, Aux Loss: 0.1679, Total Loss: 0.5196\n",
      "Epoch [50/50], MSE Loss: 0.3519, Aux Loss: 0.1683, Total Loss: 0.5203\n",
      "Finished Training\n",
      "Test Loss for Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss: 0.687165\n",
      "Number of dead neurons in Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss: 56 out of 128 (43.75%)\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss ===\n",
      "Total features: 128\n",
      "Dead features: 56 (43.75%)\n",
      "Active features: 72 (56.25%)\n",
      "Mean activation frequency: 0.0781\n",
      "Median activation frequency: 0.0502\n",
      "Mean activation strength (when active): 0.184765\n",
      "    Dead neurons: 56/128 (43.75%)\n",
      "\n",
      "### Processing OLIVETTI Dataset ###\n",
      "\n",
      "Hidden Size: 256, k_top: 10\n",
      "  Training SAE...\n",
      "Epoch [1/50], Loss: 5.4370\n",
      "Epoch [2/50], Loss: 3.2216\n",
      "Epoch [3/50], Loss: 1.9254\n",
      "Epoch [4/50], Loss: 1.2879\n",
      "Epoch [5/50], Loss: 0.9879\n",
      "Epoch [6/50], Loss: 0.8370\n",
      "Epoch [7/50], Loss: 0.7493\n",
      "Epoch [8/50], Loss: 0.6905\n",
      "Epoch [9/50], Loss: 0.6442\n",
      "Epoch [10/50], Loss: 0.6054\n",
      "Epoch [11/50], Loss: 0.5739\n",
      "Epoch [12/50], Loss: 0.5458\n",
      "Epoch [13/50], Loss: 0.5220\n",
      "Epoch [14/50], Loss: 0.5026\n",
      "Epoch [15/50], Loss: 0.4852\n",
      "Epoch [16/50], Loss: 0.4700\n",
      "Epoch [17/50], Loss: 0.4557\n",
      "Epoch [18/50], Loss: 0.4459\n",
      "Epoch [19/50], Loss: 0.4366\n",
      "Epoch [20/50], Loss: 0.4267\n",
      "Epoch [21/50], Loss: 0.4195\n",
      "Epoch [22/50], Loss: 0.4123\n",
      "Epoch [23/50], Loss: 0.4059\n",
      "Epoch [24/50], Loss: 0.4001\n",
      "Epoch [25/50], Loss: 0.3978\n",
      "Epoch [26/50], Loss: 0.3934\n",
      "Epoch [27/50], Loss: 0.3887\n",
      "Epoch [28/50], Loss: 0.3871\n",
      "Epoch [29/50], Loss: 0.3827\n",
      "Epoch [30/50], Loss: 0.3808\n",
      "Epoch [31/50], Loss: 0.3777\n",
      "Epoch [32/50], Loss: 0.3763\n",
      "Epoch [33/50], Loss: 0.3746\n",
      "Epoch [34/50], Loss: 0.3736\n",
      "Epoch [35/50], Loss: 0.3741\n",
      "Epoch [36/50], Loss: 0.3726\n",
      "Epoch [37/50], Loss: 0.3699\n",
      "Epoch [38/50], Loss: 0.3701\n",
      "Epoch [39/50], Loss: 0.3672\n",
      "Epoch [40/50], Loss: 0.3667\n",
      "Epoch [41/50], Loss: 0.3672\n",
      "Epoch [42/50], Loss: 0.3650\n",
      "Epoch [43/50], Loss: 0.3653\n",
      "Epoch [44/50], Loss: 0.3631\n",
      "Epoch [45/50], Loss: 0.3634\n",
      "Epoch [46/50], Loss: 0.3626\n",
      "Epoch [47/50], Loss: 0.3616\n",
      "Epoch [48/50], Loss: 0.3605\n",
      "Epoch [49/50], Loss: 0.3618\n",
      "Epoch [50/50], Loss: 0.3611\n",
      "Finished Training\n",
      "Test Loss for Default Sparse Autoencoder: 0.452630\n",
      "Number of dead neurons in Default Sparse Autoencoder: 219 out of 256 (85.55%)\n",
      "\n",
      "=== Activation Statistics for Default Sparse Autoencoder ===\n",
      "Total features: 256\n",
      "Dead features: 219 (85.55%)\n",
      "Active features: 37 (14.45%)\n",
      "Mean activation frequency: 0.0391\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.132538\n",
      "    Dead neurons: 219/256 (85.55%)\n",
      "  Training SAE_Init...\n",
      "Epoch [1/50], Loss: 4.7098\n",
      "Epoch [2/50], Loss: 2.4451\n",
      "Epoch [3/50], Loss: 1.5524\n",
      "Epoch [4/50], Loss: 1.1020\n",
      "Epoch [5/50], Loss: 0.8400\n",
      "Epoch [6/50], Loss: 0.6929\n",
      "Epoch [7/50], Loss: 0.6010\n",
      "Epoch [8/50], Loss: 0.5437\n",
      "Epoch [9/50], Loss: 0.5034\n",
      "Epoch [10/50], Loss: 0.4700\n",
      "Epoch [11/50], Loss: 0.4465\n",
      "Epoch [12/50], Loss: 0.4273\n",
      "Epoch [13/50], Loss: 0.4117\n",
      "Epoch [14/50], Loss: 0.3993\n",
      "Epoch [15/50], Loss: 0.3895\n",
      "Epoch [16/50], Loss: 0.3802\n",
      "Epoch [17/50], Loss: 0.3716\n",
      "Epoch [18/50], Loss: 0.3652\n",
      "Epoch [19/50], Loss: 0.3574\n",
      "Epoch [20/50], Loss: 0.3551\n",
      "Epoch [21/50], Loss: 0.3472\n",
      "Epoch [22/50], Loss: 0.3439\n",
      "Epoch [23/50], Loss: 0.3387\n",
      "Epoch [24/50], Loss: 0.3361\n",
      "Epoch [25/50], Loss: 0.3309\n",
      "Epoch [26/50], Loss: 0.3270\n",
      "Epoch [27/50], Loss: 0.3263\n",
      "Epoch [28/50], Loss: 0.3225\n",
      "Epoch [29/50], Loss: 0.3169\n",
      "Epoch [30/50], Loss: 0.3150\n",
      "Epoch [31/50], Loss: 0.3131\n",
      "Epoch [32/50], Loss: 0.3103\n",
      "Epoch [33/50], Loss: 0.3080\n",
      "Epoch [34/50], Loss: 0.3058\n",
      "Epoch [35/50], Loss: 0.3058\n",
      "Epoch [36/50], Loss: 0.3028\n",
      "Epoch [37/50], Loss: 0.3024\n",
      "Epoch [38/50], Loss: 0.3001\n",
      "Epoch [39/50], Loss: 0.3004\n",
      "Epoch [40/50], Loss: 0.2976\n",
      "Epoch [41/50], Loss: 0.2976\n",
      "Epoch [42/50], Loss: 0.2967\n",
      "Epoch [43/50], Loss: 0.2940\n",
      "Epoch [44/50], Loss: 0.2917\n",
      "Epoch [45/50], Loss: 0.2919\n",
      "Epoch [46/50], Loss: 0.2920\n",
      "Epoch [47/50], Loss: 0.2926\n",
      "Epoch [48/50], Loss: 0.2907\n",
      "Epoch [49/50], Loss: 0.2908\n",
      "Epoch [50/50], Loss: 0.2891\n",
      "Finished Training\n",
      "Test Loss for Sparse Autoencoder with just weight initialization: 0.420213\n",
      "Number of dead neurons in Sparse Autoencoder with just weight initialization: 183 out of 256 (71.48%)\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with just weight initialization ===\n",
      "Total features: 256\n",
      "Dead features: 183 (71.48%)\n",
      "Active features: 73 (28.52%)\n",
      "Mean activation frequency: 0.0391\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.253303\n",
      "    Dead neurons: 183/256 (71.48%)\n",
      "  Training SAE_AuxLoss...\n",
      "Epoch [1/50], MSE Loss: 5.3998, Aux Loss: 0.0000, Total Loss: 5.3998\n",
      "Epoch [2/50], MSE Loss: 3.3155, Aux Loss: 0.0000, Total Loss: 3.3155\n",
      "Epoch [3/50], MSE Loss: 1.9677, Aux Loss: 0.0000, Total Loss: 1.9677\n",
      "Epoch [4/50], MSE Loss: 1.3056, Aux Loss: 0.0000, Total Loss: 1.3056\n",
      "Epoch [5/50], MSE Loss: 0.9913, Aux Loss: 0.0000, Total Loss: 0.9913\n",
      "Epoch [6/50], MSE Loss: 0.8406, Aux Loss: 0.0000, Total Loss: 0.8406\n",
      "Epoch [7/50], MSE Loss: 0.7562, Aux Loss: 0.0000, Total Loss: 0.7562\n",
      "Epoch [8/50], MSE Loss: 0.6998, Aux Loss: 0.0000, Total Loss: 0.6998\n",
      "Epoch [9/50], MSE Loss: 0.6576, Aux Loss: 0.0000, Total Loss: 0.6576\n",
      "Epoch [10/50], MSE Loss: 0.6209, Aux Loss: 0.0000, Total Loss: 0.6209\n",
      "Epoch [11/50], MSE Loss: 0.5901, Aux Loss: 0.0000, Total Loss: 0.5901\n",
      "Epoch [12/50], MSE Loss: 0.5636, Aux Loss: 0.0000, Total Loss: 0.5636\n",
      "Epoch [13/50], MSE Loss: 0.5409, Aux Loss: 0.0000, Total Loss: 0.5409\n",
      "Epoch [14/50], MSE Loss: 0.5203, Aux Loss: 0.0000, Total Loss: 0.5203\n",
      "Epoch [15/50], MSE Loss: 0.5025, Aux Loss: 0.0000, Total Loss: 0.5025\n",
      "Epoch [16/50], MSE Loss: 0.4863, Aux Loss: 0.0000, Total Loss: 0.4863\n",
      "Epoch [17/50], MSE Loss: 0.4728, Aux Loss: 0.0000, Total Loss: 0.4728\n",
      "Epoch [18/50], MSE Loss: 0.4611, Aux Loss: 0.0000, Total Loss: 0.4611\n",
      "Epoch [19/50], MSE Loss: 0.4514, Aux Loss: 0.0000, Total Loss: 0.4514\n",
      "Epoch [20/50], MSE Loss: 0.4421, Aux Loss: 0.0000, Total Loss: 0.4421\n",
      "Epoch [21/50], MSE Loss: 0.4348, Aux Loss: 0.0000, Total Loss: 0.4348\n",
      "Epoch [22/50], MSE Loss: 0.4729, Aux Loss: 0.0439, Total Loss: 0.5168\n",
      "Epoch [23/50], MSE Loss: 0.4657, Aux Loss: 0.0390, Total Loss: 0.5047\n",
      "Epoch [24/50], MSE Loss: 0.4582, Aux Loss: 0.0383, Total Loss: 0.4965\n",
      "Epoch [25/50], MSE Loss: 0.4553, Aux Loss: 0.0370, Total Loss: 0.4923\n",
      "Epoch [26/50], MSE Loss: 0.4488, Aux Loss: 0.0354, Total Loss: 0.4843\n",
      "Epoch [27/50], MSE Loss: 0.4448, Aux Loss: 0.0348, Total Loss: 0.4796\n",
      "Epoch [28/50], MSE Loss: 0.4415, Aux Loss: 0.0342, Total Loss: 0.4757\n",
      "Epoch [29/50], MSE Loss: 0.4374, Aux Loss: 0.0340, Total Loss: 0.4714\n",
      "Epoch [30/50], MSE Loss: 0.4349, Aux Loss: 0.0338, Total Loss: 0.4687\n",
      "Epoch [31/50], MSE Loss: 0.4339, Aux Loss: 0.0334, Total Loss: 0.4673\n",
      "Epoch [32/50], MSE Loss: 0.4316, Aux Loss: 0.0335, Total Loss: 0.4651\n",
      "Epoch [33/50], MSE Loss: 0.4295, Aux Loss: 0.0330, Total Loss: 0.4625\n",
      "Epoch [34/50], MSE Loss: 0.4267, Aux Loss: 0.0333, Total Loss: 0.4600\n",
      "Epoch [35/50], MSE Loss: 0.4263, Aux Loss: 0.0335, Total Loss: 0.4598\n",
      "Epoch [36/50], MSE Loss: 0.4255, Aux Loss: 0.0329, Total Loss: 0.4584\n",
      "Epoch [37/50], MSE Loss: 0.4230, Aux Loss: 0.0334, Total Loss: 0.4564\n",
      "Epoch [38/50], MSE Loss: 0.4211, Aux Loss: 0.0331, Total Loss: 0.4542\n",
      "Epoch [39/50], MSE Loss: 0.4207, Aux Loss: 0.0331, Total Loss: 0.4538\n",
      "Epoch [40/50], MSE Loss: 0.4196, Aux Loss: 0.0332, Total Loss: 0.4528\n",
      "Epoch [41/50], MSE Loss: 0.4194, Aux Loss: 0.0333, Total Loss: 0.4526\n",
      "Epoch [42/50], MSE Loss: 0.4180, Aux Loss: 0.0331, Total Loss: 0.4511\n",
      "Epoch [43/50], MSE Loss: 0.4163, Aux Loss: 0.0329, Total Loss: 0.4493\n",
      "Epoch [44/50], MSE Loss: 0.4157, Aux Loss: 0.0330, Total Loss: 0.4487\n",
      "Epoch [45/50], MSE Loss: 0.4148, Aux Loss: 0.0335, Total Loss: 0.4484\n",
      "Epoch [46/50], MSE Loss: 0.4134, Aux Loss: 0.0329, Total Loss: 0.4463\n",
      "Epoch [47/50], MSE Loss: 0.4136, Aux Loss: 0.0333, Total Loss: 0.4469\n",
      "Epoch [48/50], MSE Loss: 0.4131, Aux Loss: 0.0331, Total Loss: 0.4462\n",
      "Epoch [49/50], MSE Loss: 0.4118, Aux Loss: 0.0333, Total Loss: 0.4451\n",
      "Epoch [50/50], MSE Loss: 0.4118, Aux Loss: 0.0331, Total Loss: 0.4450\n",
      "Finished Training\n",
      "Test Loss for Sparse Autoencoder with Auxiliary Loss: 0.902022\n",
      "Number of dead neurons in Sparse Autoencoder with Auxiliary Loss: 226 out of 256 (88.28%)\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with Auxiliary Loss ===\n",
      "Total features: 256\n",
      "Dead features: 226 (88.28%)\n",
      "Active features: 30 (11.72%)\n",
      "Mean activation frequency: 0.0389\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.118681\n",
      "    Dead neurons: 226/256 (88.28%)\n",
      "  Training Complete...\n",
      "Epoch [1/50], MSE Loss: 4.7098, Aux Loss: 0.0000, Total Loss: 4.7098\n",
      "Epoch [2/50], MSE Loss: 2.4451, Aux Loss: 0.0000, Total Loss: 2.4451\n",
      "Epoch [3/50], MSE Loss: 1.5524, Aux Loss: 0.0000, Total Loss: 1.5524\n",
      "Epoch [4/50], MSE Loss: 1.1020, Aux Loss: 0.0000, Total Loss: 1.1020\n",
      "Epoch [5/50], MSE Loss: 0.8400, Aux Loss: 0.0000, Total Loss: 0.8400\n",
      "Epoch [6/50], MSE Loss: 0.6929, Aux Loss: 0.0000, Total Loss: 0.6929\n",
      "Epoch [7/50], MSE Loss: 0.6010, Aux Loss: 0.0000, Total Loss: 0.6010\n",
      "Epoch [8/50], MSE Loss: 0.5437, Aux Loss: 0.0000, Total Loss: 0.5437\n",
      "Epoch [9/50], MSE Loss: 0.5034, Aux Loss: 0.0000, Total Loss: 0.5034\n",
      "Epoch [10/50], MSE Loss: 0.4700, Aux Loss: 0.0000, Total Loss: 0.4700\n",
      "Epoch [11/50], MSE Loss: 0.4465, Aux Loss: 0.0000, Total Loss: 0.4465\n",
      "Epoch [12/50], MSE Loss: 0.4273, Aux Loss: 0.0000, Total Loss: 0.4273\n",
      "Epoch [13/50], MSE Loss: 0.4117, Aux Loss: 0.0000, Total Loss: 0.4117\n",
      "Epoch [14/50], MSE Loss: 0.3993, Aux Loss: 0.0000, Total Loss: 0.3993\n",
      "Epoch [15/50], MSE Loss: 0.3895, Aux Loss: 0.0000, Total Loss: 0.3895\n",
      "Epoch [16/50], MSE Loss: 0.3802, Aux Loss: 0.0000, Total Loss: 0.3802\n",
      "Epoch [17/50], MSE Loss: 0.3716, Aux Loss: 0.0000, Total Loss: 0.3716\n",
      "Epoch [18/50], MSE Loss: 0.3652, Aux Loss: 0.0000, Total Loss: 0.3652\n",
      "Epoch [19/50], MSE Loss: 0.3574, Aux Loss: 0.0000, Total Loss: 0.3574\n",
      "Epoch [20/50], MSE Loss: 0.3551, Aux Loss: 0.0000, Total Loss: 0.3551\n",
      "Epoch [21/50], MSE Loss: 0.3475, Aux Loss: 0.0092, Total Loss: 0.3568\n",
      "Epoch [22/50], MSE Loss: 0.3444, Aux Loss: 0.0085, Total Loss: 0.3529\n",
      "Epoch [23/50], MSE Loss: 0.3387, Aux Loss: 0.0084, Total Loss: 0.3471\n",
      "Epoch [24/50], MSE Loss: 0.3357, Aux Loss: 0.0083, Total Loss: 0.3440\n",
      "Epoch [25/50], MSE Loss: 0.3313, Aux Loss: 0.0081, Total Loss: 0.3394\n",
      "Epoch [26/50], MSE Loss: 0.3277, Aux Loss: 0.0081, Total Loss: 0.3358\n",
      "Epoch [27/50], MSE Loss: 0.3268, Aux Loss: 0.0080, Total Loss: 0.3349\n",
      "Epoch [28/50], MSE Loss: 0.3213, Aux Loss: 0.0079, Total Loss: 0.3292\n",
      "Epoch [29/50], MSE Loss: 0.3176, Aux Loss: 0.0077, Total Loss: 0.3253\n",
      "Epoch [30/50], MSE Loss: 0.3165, Aux Loss: 0.0078, Total Loss: 0.3244\n",
      "Epoch [31/50], MSE Loss: 0.3142, Aux Loss: 0.0077, Total Loss: 0.3219\n",
      "Epoch [32/50], MSE Loss: 0.3110, Aux Loss: 0.0077, Total Loss: 0.3187\n",
      "Epoch [33/50], MSE Loss: 0.3095, Aux Loss: 0.0079, Total Loss: 0.3174\n",
      "Epoch [34/50], MSE Loss: 0.3071, Aux Loss: 0.0079, Total Loss: 0.3150\n",
      "Epoch [35/50], MSE Loss: 0.3048, Aux Loss: 0.0080, Total Loss: 0.3128\n",
      "Epoch [36/50], MSE Loss: 0.3032, Aux Loss: 0.0078, Total Loss: 0.3110\n",
      "Epoch [37/50], MSE Loss: 0.3028, Aux Loss: 0.0080, Total Loss: 0.3107\n",
      "Epoch [38/50], MSE Loss: 0.3019, Aux Loss: 0.0081, Total Loss: 0.3100\n",
      "Epoch [39/50], MSE Loss: 0.2992, Aux Loss: 0.0080, Total Loss: 0.3072\n",
      "Epoch [40/50], MSE Loss: 0.2981, Aux Loss: 0.0080, Total Loss: 0.3061\n",
      "Epoch [41/50], MSE Loss: 0.2965, Aux Loss: 0.0081, Total Loss: 0.3046\n",
      "Epoch [42/50], MSE Loss: 0.2941, Aux Loss: 0.0080, Total Loss: 0.3021\n",
      "Epoch [43/50], MSE Loss: 0.2953, Aux Loss: 0.0082, Total Loss: 0.3035\n",
      "Epoch [44/50], MSE Loss: 0.2938, Aux Loss: 0.0081, Total Loss: 0.3018\n",
      "Epoch [45/50], MSE Loss: 0.2948, Aux Loss: 0.0082, Total Loss: 0.3030\n",
      "Epoch [46/50], MSE Loss: 0.2918, Aux Loss: 0.0081, Total Loss: 0.2999\n",
      "Epoch [47/50], MSE Loss: 0.2914, Aux Loss: 0.0082, Total Loss: 0.2996\n",
      "Epoch [48/50], MSE Loss: 0.2905, Aux Loss: 0.0081, Total Loss: 0.2986\n",
      "Epoch [49/50], MSE Loss: 0.2888, Aux Loss: 0.0082, Total Loss: 0.2970\n",
      "Epoch [50/50], MSE Loss: 0.2893, Aux Loss: 0.0083, Total Loss: 0.2976\n",
      "Finished Training\n",
      "Test Loss for Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss: 0.843712\n",
      "Number of dead neurons in Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss: 183 out of 256 (71.48%)\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss ===\n",
      "Total features: 256\n",
      "Dead features: 183 (71.48%)\n",
      "Active features: 73 (28.52%)\n",
      "Mean activation frequency: 0.0391\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.252055\n",
      "    Dead neurons: 183/256 (71.48%)\n",
      "\n",
      "### Processing LFW Dataset ###\n",
      "Original LFW shape: (3023, 125, 94)\n",
      "Resized LFW shape: (3023, 4096)\n",
      "LFW Dataset loaded: 2418 train, 605 test\n",
      "Image size: 64×64, Input dimension: 4096\n",
      "\n",
      "Hidden Size: 512, k_top: 10\n",
      "  Training SAE...\n",
      "Epoch [1/50], Loss: 1.6715\n",
      "Epoch [2/50], Loss: 0.6771\n",
      "Epoch [3/50], Loss: 0.5395\n",
      "Epoch [4/50], Loss: 0.4880\n",
      "Epoch [5/50], Loss: 0.4676\n",
      "Epoch [6/50], Loss: 0.4567\n",
      "Epoch [7/50], Loss: 0.4515\n",
      "Epoch [8/50], Loss: 0.4468\n",
      "Epoch [9/50], Loss: 0.4444\n",
      "Epoch [10/50], Loss: 0.4410\n",
      "Epoch [11/50], Loss: 0.4397\n",
      "Epoch [12/50], Loss: 0.4394\n",
      "Epoch [13/50], Loss: 0.4373\n",
      "Epoch [14/50], Loss: 0.4367\n",
      "Epoch [15/50], Loss: 0.4360\n",
      "Epoch [16/50], Loss: 0.4352\n",
      "Epoch [17/50], Loss: 0.4358\n",
      "Epoch [18/50], Loss: 0.4350\n",
      "Epoch [19/50], Loss: 0.4343\n",
      "Epoch [20/50], Loss: 0.4345\n",
      "Epoch [21/50], Loss: 0.4343\n",
      "Epoch [22/50], Loss: 0.4341\n",
      "Epoch [23/50], Loss: 0.4341\n",
      "Epoch [24/50], Loss: 0.4344\n",
      "Epoch [25/50], Loss: 0.4335\n",
      "Epoch [26/50], Loss: 0.4343\n",
      "Epoch [27/50], Loss: 0.4350\n",
      "Epoch [28/50], Loss: 0.4341\n",
      "Epoch [29/50], Loss: 0.4352\n",
      "Epoch [30/50], Loss: 0.4348\n",
      "Epoch [31/50], Loss: 0.4351\n",
      "Epoch [32/50], Loss: 0.4341\n",
      "Epoch [33/50], Loss: 0.4339\n",
      "Epoch [34/50], Loss: 0.4339\n",
      "Epoch [35/50], Loss: 0.4337\n",
      "Epoch [36/50], Loss: 0.4334\n",
      "Epoch [37/50], Loss: 0.4337\n",
      "Epoch [38/50], Loss: 0.4336\n",
      "Epoch [39/50], Loss: 0.4342\n",
      "Epoch [40/50], Loss: 0.4332\n",
      "Epoch [41/50], Loss: 0.4335\n",
      "Epoch [42/50], Loss: 0.4333\n",
      "Epoch [43/50], Loss: 0.4334\n",
      "Epoch [44/50], Loss: 0.4332\n",
      "Epoch [45/50], Loss: 0.4332\n",
      "Epoch [46/50], Loss: 0.4328\n",
      "Epoch [47/50], Loss: 0.4330\n",
      "Epoch [48/50], Loss: 0.4333\n",
      "Epoch [49/50], Loss: 0.4332\n",
      "Epoch [50/50], Loss: 0.4337\n",
      "Finished Training\n",
      "Test Loss for Default Sparse Autoencoder: 0.449836\n",
      "Number of dead neurons in Default Sparse Autoencoder: 480 out of 512 (93.75%)\n",
      "\n",
      "=== Activation Statistics for Default Sparse Autoencoder ===\n",
      "Total features: 512\n",
      "Dead features: 480 (93.75%)\n",
      "Active features: 32 (6.25%)\n",
      "Mean activation frequency: 0.0195\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.109464\n",
      "    Dead neurons: 480/512 (93.75%)\n",
      "  Training SAE_Init...\n",
      "Epoch [1/50], Loss: 1.4471\n",
      "Epoch [2/50], Loss: 0.5777\n",
      "Epoch [3/50], Loss: 0.4917\n",
      "Epoch [4/50], Loss: 0.4649\n",
      "Epoch [5/50], Loss: 0.4512\n",
      "Epoch [6/50], Loss: 0.4433\n",
      "Epoch [7/50], Loss: 0.4394\n",
      "Epoch [8/50], Loss: 0.4332\n",
      "Epoch [9/50], Loss: 0.4306\n",
      "Epoch [10/50], Loss: 0.4311\n",
      "Epoch [11/50], Loss: 0.4301\n",
      "Epoch [12/50], Loss: 0.4236\n",
      "Epoch [13/50], Loss: 0.4298\n",
      "Epoch [14/50], Loss: 0.4358\n",
      "Epoch [15/50], Loss: 0.4518\n",
      "Epoch [16/50], Loss: 0.4330\n",
      "Epoch [17/50], Loss: 0.4297\n",
      "Epoch [18/50], Loss: 0.4456\n",
      "Epoch [19/50], Loss: 0.4327\n",
      "Epoch [20/50], Loss: 0.4274\n",
      "Epoch [21/50], Loss: 0.4336\n",
      "Epoch [22/50], Loss: 0.4341\n",
      "Epoch [23/50], Loss: 0.4420\n",
      "Epoch [24/50], Loss: 0.4549\n",
      "Epoch [25/50], Loss: 0.4417\n",
      "Epoch [26/50], Loss: 0.4528\n",
      "Epoch [27/50], Loss: 0.4526\n",
      "Epoch [28/50], Loss: 0.4490\n",
      "Epoch [29/50], Loss: 0.4320\n",
      "Epoch [30/50], Loss: 0.4371\n",
      "Epoch [31/50], Loss: 0.4618\n",
      "Epoch [32/50], Loss: 0.4424\n",
      "Epoch [33/50], Loss: 0.4233\n",
      "Epoch [34/50], Loss: 0.4165\n",
      "Epoch [35/50], Loss: 0.4130\n",
      "Epoch [36/50], Loss: 0.4214\n",
      "Epoch [37/50], Loss: 0.4387\n",
      "Epoch [38/50], Loss: 0.4360\n",
      "Epoch [39/50], Loss: 0.4742\n",
      "Epoch [40/50], Loss: 0.4577\n",
      "Epoch [41/50], Loss: 0.4412\n",
      "Epoch [42/50], Loss: 0.4301\n",
      "Epoch [43/50], Loss: 0.4258\n",
      "Epoch [44/50], Loss: 0.4192\n",
      "Epoch [45/50], Loss: 0.4286\n",
      "Epoch [46/50], Loss: 0.4269\n",
      "Epoch [47/50], Loss: 0.4283\n",
      "Epoch [48/50], Loss: 0.4158\n",
      "Epoch [49/50], Loss: 0.4225\n",
      "Epoch [50/50], Loss: 0.4940\n",
      "Finished Training\n",
      "Test Loss for Sparse Autoencoder with just weight initialization: 0.458363\n",
      "Number of dead neurons in Sparse Autoencoder with just weight initialization: 411 out of 512 (80.27%)\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with just weight initialization ===\n",
      "Total features: 512\n",
      "Dead features: 411 (80.27%)\n",
      "Active features: 101 (19.73%)\n",
      "Mean activation frequency: 0.0195\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.161662\n",
      "    Dead neurons: 411/512 (80.27%)\n",
      "  Training SAE_AuxLoss...\n",
      "Epoch [1/50], MSE Loss: 1.6729, Aux Loss: 0.0000, Total Loss: 1.6729\n",
      "Epoch [2/50], MSE Loss: 0.6936, Aux Loss: 0.0000, Total Loss: 0.6936\n",
      "Epoch [3/50], MSE Loss: 0.5661, Aux Loss: 0.0000, Total Loss: 0.5661\n",
      "Epoch [4/50], MSE Loss: 0.5082, Aux Loss: 0.0000, Total Loss: 0.5082\n",
      "Epoch [5/50], MSE Loss: 0.4790, Aux Loss: 0.0000, Total Loss: 0.4790\n",
      "Epoch [6/50], MSE Loss: 0.4902, Aux Loss: 0.0252, Total Loss: 0.5155\n",
      "Epoch [7/50], MSE Loss: 0.4951, Aux Loss: 0.0347, Total Loss: 0.5299\n",
      "Epoch [8/50], MSE Loss: 0.4900, Aux Loss: 0.0321, Total Loss: 0.5221\n",
      "Epoch [9/50], MSE Loss: 0.4878, Aux Loss: 0.0311, Total Loss: 0.5189\n",
      "Epoch [10/50], MSE Loss: 0.4859, Aux Loss: 0.0305, Total Loss: 0.5165\n",
      "Epoch [11/50], MSE Loss: 0.4845, Aux Loss: 0.0302, Total Loss: 0.5147\n",
      "Epoch [12/50], MSE Loss: 0.4836, Aux Loss: 0.0300, Total Loss: 0.5136\n",
      "Epoch [13/50], MSE Loss: 0.4819, Aux Loss: 0.0296, Total Loss: 0.5115\n",
      "Epoch [14/50], MSE Loss: 0.4806, Aux Loss: 0.0295, Total Loss: 0.5100\n",
      "Epoch [15/50], MSE Loss: 0.4806, Aux Loss: 0.0294, Total Loss: 0.5100\n",
      "Epoch [16/50], MSE Loss: 0.4791, Aux Loss: 0.0291, Total Loss: 0.5082\n",
      "Epoch [17/50], MSE Loss: 0.4785, Aux Loss: 0.0290, Total Loss: 0.5075\n",
      "Epoch [18/50], MSE Loss: 0.4782, Aux Loss: 0.0288, Total Loss: 0.5070\n",
      "Epoch [19/50], MSE Loss: 0.4771, Aux Loss: 0.0287, Total Loss: 0.5058\n",
      "Epoch [20/50], MSE Loss: 0.4764, Aux Loss: 0.0286, Total Loss: 0.5050\n",
      "Epoch [21/50], MSE Loss: 0.4766, Aux Loss: 0.0286, Total Loss: 0.5053\n",
      "Epoch [22/50], MSE Loss: 0.4755, Aux Loss: 0.0283, Total Loss: 0.5038\n",
      "Epoch [23/50], MSE Loss: 0.4760, Aux Loss: 0.0283, Total Loss: 0.5044\n",
      "Epoch [24/50], MSE Loss: 0.4753, Aux Loss: 0.0282, Total Loss: 0.5035\n",
      "Epoch [25/50], MSE Loss: 0.4748, Aux Loss: 0.0282, Total Loss: 0.5030\n",
      "Epoch [26/50], MSE Loss: 0.4748, Aux Loss: 0.0282, Total Loss: 0.5031\n",
      "Epoch [27/50], MSE Loss: 0.4749, Aux Loss: 0.0281, Total Loss: 0.5030\n",
      "Epoch [28/50], MSE Loss: 0.4752, Aux Loss: 0.0281, Total Loss: 0.5033\n",
      "Epoch [29/50], MSE Loss: 0.4748, Aux Loss: 0.0280, Total Loss: 0.5028\n",
      "Epoch [30/50], MSE Loss: 0.4740, Aux Loss: 0.0279, Total Loss: 0.5019\n",
      "Epoch [31/50], MSE Loss: 0.4740, Aux Loss: 0.0279, Total Loss: 0.5018\n",
      "Epoch [32/50], MSE Loss: 0.4742, Aux Loss: 0.0278, Total Loss: 0.5020\n",
      "Epoch [33/50], MSE Loss: 0.4736, Aux Loss: 0.0277, Total Loss: 0.5014\n",
      "Epoch [34/50], MSE Loss: 0.4735, Aux Loss: 0.0279, Total Loss: 0.5013\n",
      "Epoch [35/50], MSE Loss: 0.4736, Aux Loss: 0.0277, Total Loss: 0.5013\n",
      "Epoch [36/50], MSE Loss: 0.4740, Aux Loss: 0.0277, Total Loss: 0.5017\n",
      "Epoch [37/50], MSE Loss: 0.4732, Aux Loss: 0.0277, Total Loss: 0.5009\n",
      "Epoch [38/50], MSE Loss: 0.4736, Aux Loss: 0.0278, Total Loss: 0.5014\n",
      "Epoch [39/50], MSE Loss: 0.4730, Aux Loss: 0.0276, Total Loss: 0.5006\n",
      "Epoch [40/50], MSE Loss: 0.4728, Aux Loss: 0.0277, Total Loss: 0.5005\n",
      "Epoch [41/50], MSE Loss: 0.4743, Aux Loss: 0.0276, Total Loss: 0.5020\n",
      "Epoch [42/50], MSE Loss: 0.4730, Aux Loss: 0.0277, Total Loss: 0.5007\n",
      "Epoch [43/50], MSE Loss: 0.4727, Aux Loss: 0.0277, Total Loss: 0.5003\n",
      "Epoch [44/50], MSE Loss: 0.4737, Aux Loss: 0.0276, Total Loss: 0.5013\n",
      "Epoch [45/50], MSE Loss: 0.4730, Aux Loss: 0.0276, Total Loss: 0.5005\n",
      "Epoch [46/50], MSE Loss: 0.4725, Aux Loss: 0.0276, Total Loss: 0.5001\n",
      "Epoch [47/50], MSE Loss: 0.4718, Aux Loss: 0.0275, Total Loss: 0.4993\n",
      "Epoch [48/50], MSE Loss: 0.4723, Aux Loss: 0.0275, Total Loss: 0.4998\n",
      "Epoch [49/50], MSE Loss: 0.4712, Aux Loss: 0.0275, Total Loss: 0.4987\n",
      "Epoch [50/50], MSE Loss: 0.4715, Aux Loss: 0.0275, Total Loss: 0.4990\n",
      "Finished Training\n",
      "Test Loss for Sparse Autoencoder with Auxiliary Loss: 0.911123\n",
      "Number of dead neurons in Sparse Autoencoder with Auxiliary Loss: 488 out of 512 (95.31%)\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with Auxiliary Loss ===\n",
      "Total features: 512\n",
      "Dead features: 488 (95.31%)\n",
      "Active features: 24 (4.69%)\n",
      "Mean activation frequency: 0.0195\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.102353\n",
      "    Dead neurons: 488/512 (95.31%)\n",
      "  Training Complete...\n",
      "Epoch [1/50], MSE Loss: 1.4471, Aux Loss: 0.0000, Total Loss: 1.4471\n",
      "Epoch [2/50], MSE Loss: 0.5777, Aux Loss: 0.0000, Total Loss: 0.5777\n",
      "Epoch [3/50], MSE Loss: 0.4917, Aux Loss: 0.0000, Total Loss: 0.4917\n",
      "Epoch [4/50], MSE Loss: 0.4649, Aux Loss: 0.0000, Total Loss: 0.4649\n",
      "Epoch [5/50], MSE Loss: 0.4512, Aux Loss: 0.0000, Total Loss: 0.4512\n",
      "Epoch [6/50], MSE Loss: 0.4435, Aux Loss: 0.0084, Total Loss: 0.4518\n",
      "Epoch [7/50], MSE Loss: 0.4382, Aux Loss: 0.0117, Total Loss: 0.4499\n",
      "Epoch [8/50], MSE Loss: 0.4380, Aux Loss: 0.0121, Total Loss: 0.4501\n",
      "Epoch [9/50], MSE Loss: 0.4322, Aux Loss: 0.0124, Total Loss: 0.4446\n",
      "Epoch [10/50], MSE Loss: 0.4317, Aux Loss: 0.0124, Total Loss: 0.4441\n",
      "Epoch [11/50], MSE Loss: 0.4308, Aux Loss: 0.0128, Total Loss: 0.4436\n",
      "Epoch [12/50], MSE Loss: 0.4280, Aux Loss: 0.0130, Total Loss: 0.4411\n",
      "Epoch [13/50], MSE Loss: 0.4264, Aux Loss: 0.0137, Total Loss: 0.4401\n",
      "Epoch [14/50], MSE Loss: 0.4247, Aux Loss: 0.0140, Total Loss: 0.4387\n",
      "Epoch [15/50], MSE Loss: 0.4297, Aux Loss: 0.0143, Total Loss: 0.4440\n",
      "Epoch [16/50], MSE Loss: 0.4537, Aux Loss: 0.0151, Total Loss: 0.4688\n",
      "Epoch [17/50], MSE Loss: 0.4344, Aux Loss: 0.0148, Total Loss: 0.4493\n",
      "Epoch [18/50], MSE Loss: 0.4249, Aux Loss: 0.0149, Total Loss: 0.4398\n",
      "Epoch [19/50], MSE Loss: 0.4218, Aux Loss: 0.0150, Total Loss: 0.4369\n",
      "Epoch [20/50], MSE Loss: 0.4176, Aux Loss: 0.0151, Total Loss: 0.4327\n",
      "Epoch [21/50], MSE Loss: 0.4283, Aux Loss: 0.0156, Total Loss: 0.4439\n",
      "Epoch [22/50], MSE Loss: 0.4262, Aux Loss: 0.0155, Total Loss: 0.4417\n",
      "Epoch [23/50], MSE Loss: 0.4406, Aux Loss: 0.0160, Total Loss: 0.4566\n",
      "Epoch [24/50], MSE Loss: 0.4518, Aux Loss: 0.0161, Total Loss: 0.4680\n",
      "Epoch [25/50], MSE Loss: 0.4316, Aux Loss: 0.0156, Total Loss: 0.4472\n",
      "Epoch [26/50], MSE Loss: 0.4319, Aux Loss: 0.0157, Total Loss: 0.4476\n",
      "Epoch [27/50], MSE Loss: 0.4460, Aux Loss: 0.0160, Total Loss: 0.4620\n",
      "Epoch [28/50], MSE Loss: 0.4537, Aux Loss: 0.0163, Total Loss: 0.4700\n",
      "Epoch [29/50], MSE Loss: 0.4341, Aux Loss: 0.0159, Total Loss: 0.4500\n",
      "Epoch [30/50], MSE Loss: 0.4286, Aux Loss: 0.0164, Total Loss: 0.4450\n",
      "Epoch [31/50], MSE Loss: 0.4527, Aux Loss: 0.0172, Total Loss: 0.4699\n",
      "Epoch [32/50], MSE Loss: 0.4371, Aux Loss: 0.0168, Total Loss: 0.4539\n",
      "Epoch [33/50], MSE Loss: 0.4310, Aux Loss: 0.0166, Total Loss: 0.4475\n",
      "Epoch [34/50], MSE Loss: 0.4299, Aux Loss: 0.0167, Total Loss: 0.4466\n",
      "Epoch [35/50], MSE Loss: 0.4415, Aux Loss: 0.0172, Total Loss: 0.4588\n",
      "Epoch [36/50], MSE Loss: 0.4430, Aux Loss: 0.0174, Total Loss: 0.4604\n",
      "Epoch [37/50], MSE Loss: 0.4496, Aux Loss: 0.0176, Total Loss: 0.4672\n",
      "Epoch [38/50], MSE Loss: 0.4297, Aux Loss: 0.0170, Total Loss: 0.4466\n",
      "Epoch [39/50], MSE Loss: 0.4239, Aux Loss: 0.0168, Total Loss: 0.4407\n",
      "Epoch [40/50], MSE Loss: 0.4483, Aux Loss: 0.0174, Total Loss: 0.4657\n",
      "Epoch [41/50], MSE Loss: 0.4262, Aux Loss: 0.0170, Total Loss: 0.4432\n",
      "Epoch [42/50], MSE Loss: 0.4252, Aux Loss: 0.0172, Total Loss: 0.4423\n",
      "Epoch [43/50], MSE Loss: 0.4342, Aux Loss: 0.0176, Total Loss: 0.4518\n",
      "Epoch [44/50], MSE Loss: 0.4212, Aux Loss: 0.0171, Total Loss: 0.4383\n",
      "Epoch [45/50], MSE Loss: 0.4605, Aux Loss: 0.0181, Total Loss: 0.4786\n",
      "Epoch [46/50], MSE Loss: 0.4286, Aux Loss: 0.0175, Total Loss: 0.4461\n",
      "Epoch [47/50], MSE Loss: 0.4299, Aux Loss: 0.0176, Total Loss: 0.4475\n",
      "Epoch [48/50], MSE Loss: 0.4255, Aux Loss: 0.0173, Total Loss: 0.4428\n",
      "Epoch [49/50], MSE Loss: 0.4173, Aux Loss: 0.0173, Total Loss: 0.4345\n",
      "Epoch [50/50], MSE Loss: 0.4202, Aux Loss: 0.0175, Total Loss: 0.4377\n",
      "Finished Training\n",
      "Test Loss for Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss: 0.891949\n",
      "Number of dead neurons in Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss: 400 out of 512 (78.12%)\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss ===\n",
      "Total features: 512\n",
      "Dead features: 400 (78.12%)\n",
      "Active features: 112 (21.88%)\n",
      "Mean activation frequency: 0.0195\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.158158\n",
      "    Dead neurons: 400/512 (78.12%)\n",
      "\n",
      "Experiment 3 results saved to 'experiment3_combined_effects_results.csv'\n",
      "\n",
      "================================================================================\n",
      "GENERATING PUBLICATION-QUALITY FIGURES\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_init' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 100\u001B[39m\n\u001B[32m     97\u001B[39m fig, axes = plt.subplots(\u001B[32m1\u001B[39m, \u001B[32m3\u001B[39m, figsize=(\u001B[32m18\u001B[39m, \u001B[32m5\u001B[39m))\n\u001B[32m     99\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m idx, dataset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m([\u001B[33m'\u001B[39m\u001B[33mmnist\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33molivetti\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mlfw\u001B[39m\u001B[33m'\u001B[39m]):\n\u001B[32m--> \u001B[39m\u001B[32m100\u001B[39m     df_subset = \u001B[43mdf_init\u001B[49m[df_init[\u001B[33m'\u001B[39m\u001B[33mdataset\u001B[39m\u001B[33m'\u001B[39m] == dataset]\n\u001B[32m    102\u001B[39m     \u001B[38;5;66;03m# Group by initialization and hidden size\u001B[39;00m\n\u001B[32m    103\u001B[39m     pivot_data = df_subset.pivot_table(\n\u001B[32m    104\u001B[39m         values=\u001B[33m'\u001B[39m\u001B[33mdead_neuron_pct\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    105\u001B[39m         index=\u001B[33m'\u001B[39m\u001B[33mhidden_size\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    106\u001B[39m         columns=\u001B[33m'\u001B[39m\u001B[33minitialization\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    107\u001B[39m         aggfunc=\u001B[33m'\u001B[39m\u001B[33mmean\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    108\u001B[39m     )\n",
      "\u001B[31mNameError\u001B[39m: name 'df_init' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x500 with 3 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T06:43:28.041553Z",
     "start_time": "2025-10-23T06:38:53.945634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Check if CSV files exist\n",
    "required_files = [\n",
    "    'experiment1_initialization_results.csv',\n",
    "    'experiment2_auxiliary_loss_results.csv',\n",
    "    'experiment3_combined_effects_results.csv'\n",
    "]\n",
    "\n",
    "for file in required_files:\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(\n",
    "            f\"CSV file '{file}' not found. \"\n",
    "            f\"Please run main() first to generate experiment results.\"\n",
    "        )\n",
    "\n",
    "# Load the dataframes\n",
    "df_init = pd.read_csv('experiment1_initialization_results.csv')\n",
    "df_aux = pd.read_csv('experiment2_auxiliary_loss_results.csv')\n",
    "df_combined = pd.read_csv('experiment3_combined_effects_results.csv')\n",
    "\n",
    "print(f\"Loaded experiment results:\")\n",
    "print(f\"  • df_init: {len(df_init)} rows, {len(df_init.columns)} columns\")\n",
    "print(f\"  • df_aux: {len(df_aux)} rows, {len(df_aux.columns)} columns\")\n",
    "print(f\"  • df_combined: {len(df_combined)} rows, {len(df_combined.columns)} columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# FIGURE GENERATION: Publication-Quality Plots\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING PUBLICATION-QUALITY FIGURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set style for publication\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Figure 1: Dead Neuron Percentage by Initialization Strategy\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "    df_subset = df_init[df_init['dataset'] == dataset]\n",
    "\n",
    "    # Group by initialization and hidden size\n",
    "    pivot_data = df_subset.pivot_table(\n",
    "        values='dead_neuron_pct',\n",
    "        index='hidden_size',\n",
    "        columns='initialization',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "\n",
    "    pivot_data.plot(kind='bar', ax=axes[idx], width=0.8)\n",
    "    axes[idx].set_title(f'{dataset.upper()} Dataset', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Hidden Size', fontsize=12)\n",
    "    axes[idx].set_ylabel('Dead Neurons (%)', fontsize=12)\n",
    "    axes[idx].legend(title='Initialization', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    axes[idx].set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure1_dead_neurons_by_initialization.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure1_dead_neurons_by_initialization.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure1_dead_neurons_by_initialization.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 2: Auxiliary Loss Impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel A: Dead neuron percentage\n",
    "ax = axes[0]\n",
    "df_aux_grouped = df_aux.groupby(['dataset', 'aux_loss'])['dead_neuron_pct'].mean().reset_index()\n",
    "pivot_aux = df_aux_grouped.pivot(index='dataset', columns='aux_loss', values='dead_neuron_pct')\n",
    "pivot_aux.plot(kind='bar', ax=ax, width=0.7, color=['#d62728', '#2ca02c'])\n",
    "ax.set_title('Dead Neuron Percentage\\nWith/Without Auxiliary Loss', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Dead Neurons (%)', fontsize=12)\n",
    "ax.legend(title='Auxiliary Loss', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel B: Test loss comparison\n",
    "ax = axes[1]\n",
    "df_loss_grouped = df_aux.groupby(['dataset', 'aux_loss'])['test_loss'].mean().reset_index()\n",
    "pivot_loss = df_loss_grouped.pivot(index='dataset', columns='aux_loss', values='test_loss')\n",
    "pivot_loss.plot(kind='bar', ax=ax, width=0.7, color=['#d62728', '#2ca02c'])\n",
    "ax.set_title('Reconstruction Loss\\nWith/Without Auxiliary Loss', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Test Loss (MSE)', fontsize=12)\n",
    "ax.legend(title='Auxiliary Loss', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure2_auxiliary_loss_effects.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure2_auxiliary_loss_effects.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure2_auxiliary_loss_effects.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 3: Combined Effects Summary\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Panel A: Dead neurons by model type\n",
    "ax = axes[0, 0]\n",
    "df_combined_grouped = df_combined.groupby(['dataset', 'model_type'])['dead_neuron_pct'].mean().reset_index()\n",
    "pivot_combined = df_combined_grouped.pivot(index='dataset', columns='model_type', values='dead_neuron_pct')\n",
    "pivot_combined.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Dead Neuron Percentage by Model Configuration', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Dead Neurons (%)', fontsize=12)\n",
    "ax.legend(title='Model Type', fontsize=9, loc='upper right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel B: Test loss by model type\n",
    "ax = axes[0, 1]\n",
    "df_loss_combined = df_combined.groupby(['dataset', 'model_type'])['test_loss'].mean().reset_index()\n",
    "pivot_loss_combined = df_loss_combined.pivot(index='dataset', columns='model_type', values='test_loss')\n",
    "pivot_loss_combined.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Reconstruction Loss by Model Configuration', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Test Loss (MSE)', fontsize=12)\n",
    "ax.legend(title='Model Type', fontsize=9, loc='upper right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Panel C: Scatter plot - Dead neurons vs Test loss\n",
    "ax = axes[1, 0]\n",
    "for model_type in df_combined['model_type'].unique():\n",
    "    df_model = df_combined[df_combined['model_type'] == model_type]\n",
    "    ax.scatter(df_model['dead_neuron_pct'], df_model['test_loss'],\n",
    "               label=model_type, s=100, alpha=0.7)\n",
    "ax.set_xlabel('Dead Neurons (%)', fontsize=12)\n",
    "ax.set_ylabel('Test Loss (MSE)', fontsize=12)\n",
    "ax.set_title('Dead Neurons vs Reconstruction Performance', fontsize=14, fontweight='bold')\n",
    "ax.legend(title='Model Type', fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Panel D: Active features comparison\n",
    "ax = axes[1, 1]\n",
    "df_active = df_combined.groupby(['dataset', 'model_type'])['active_features'].mean().reset_index()\n",
    "pivot_active = df_active.pivot(index='dataset', columns='model_type', values='active_features')\n",
    "pivot_active.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Active Feature Count by Model Configuration', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Number of Active Features', fontsize=12)\n",
    "ax.legend(title='Model Type', fontsize=9)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure3_combined_effects_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure3_combined_effects_summary.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure3_combined_effects_summary.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# Figure 4: Activation Statistics Histograms\n",
    "# Compare best vs worst configurations\n",
    "print(\"\\nGenerating activation histograms for best/worst models...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for idx, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "    print(f\"  Processing {dataset}...\")\n",
    "\n",
    "    config = experiment_configs[dataset]\n",
    "    train_loader, test_loader, dataset_mean = load_data(\n",
    "        dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        img_size=64\n",
    "    )\n",
    "\n",
    "    hidden_size = config['hidden_sizes'][0]\n",
    "    k_top = config['k_tops'][1]\n",
    "\n",
    "    # Train baseline (worst) and complete (best) models\n",
    "    set_seeds(42)\n",
    "    model_baseline = train_sparse_autoencoder(\n",
    "        train_loader, num_epochs=config['num_epochs'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        input_size=config['input_size'], hidden_size=hidden_size,\n",
    "        k_top=k_top, modelType='SAE', dataset_type=dataset\n",
    "    )\n",
    "\n",
    "    set_seeds(42)\n",
    "    model_complete = train_sparse_autoencoder(\n",
    "        train_loader, num_epochs=config['num_epochs'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        input_size=config['input_size'], hidden_size=hidden_size,\n",
    "        k_top=k_top, modelType='Complete', dataset_type=dataset,\n",
    "        k_aux=2 * k_top, k_aux_param=1 / 32, dead_feature_threshold=1000,\n",
    "        JumpReLU=0.1\n",
    "    )\n",
    "\n",
    "    # Get activation statistics\n",
    "    stats_baseline = get_activation_statistics(model_baseline, train_loader, dataset)\n",
    "    stats_complete = get_activation_statistics(model_complete, train_loader, dataset)\n",
    "\n",
    "    # Plot histograms\n",
    "    ax = axes[0, idx]\n",
    "    ax.hist(stats_baseline['activation_frequencies'], bins=50, alpha=0.6,\n",
    "            label='Baseline SAE', color='red', edgecolor='black')\n",
    "    ax.hist(stats_complete['activation_frequencies'], bins=50, alpha=0.6,\n",
    "            label='Complete SAE', color='green', edgecolor='black')\n",
    "    ax.set_xlabel('Activation Frequency', fontsize=11)\n",
    "    ax.set_ylabel('Number of Features', fontsize=11)\n",
    "    ax.set_title(f'{dataset.upper()}: Activation Frequencies', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "    ax = axes[1, idx]\n",
    "    active_base = stats_baseline['activation_strengths'][stats_baseline['activation_strengths'] > 0]\n",
    "    active_complete = stats_complete['activation_strengths'][stats_complete['activation_strengths'] > 0]\n",
    "    ax.hist(active_base, bins=50, alpha=0.6, label='Baseline SAE',\n",
    "            color='red', edgecolor='black')\n",
    "    ax.hist(active_complete, bins=50, alpha=0.6, label='Complete SAE',\n",
    "            color='green', edgecolor='black')\n",
    "    ax.set_xlabel('Mean Activation Strength', fontsize=11)\n",
    "    ax.set_ylabel('Number of Features', fontsize=11)\n",
    "    ax.set_title(f'{dataset.upper()}: Activation Strengths', fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figure4_activation_histograms.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig('figure4_activation_histograms.pdf', bbox_inches='tight')\n",
    "print(\"Saved: figure4_activation_histograms.png/pdf\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# Generate Summary Statistics Table (LaTeX format)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING SUMMARY STATISTICS TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comprehensive summary table\n",
    "summary_stats = []\n",
    "\n",
    "for dataset in ['mnist', 'olivetti', 'lfw']:\n",
    "    for model_type in ['SAE', 'SAE_Init', 'SAE_AuxLoss', 'Complete']:\n",
    "        df_subset = df_combined[\n",
    "            (df_combined['dataset'] == dataset) &\n",
    "            (df_combined['model_type'] == model_type)\n",
    "            ]\n",
    "\n",
    "        if len(df_subset) > 0:\n",
    "            summary_stats.append({\n",
    "                'Dataset': dataset.upper(),\n",
    "                'Model': model_type,\n",
    "                'Dead (%)': f\"{df_subset['dead_neuron_pct'].mean():.2f} ± {df_subset['dead_neuron_pct'].std():.2f}\",\n",
    "                'Test Loss': f\"{df_subset['test_loss'].mean():.6f} ± {df_subset['test_loss'].std():.6f}\",\n",
    "                'Active Features': f\"{df_subset['active_features'].mean():.0f}\"\n",
    "            })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_stats)\n",
    "\n",
    "# Save as CSV\n",
    "df_summary.to_csv('summary_statistics.csv', index=False)\n",
    "print(\"Saved: summary_statistics.csv\")\n",
    "\n",
    "# Save as LaTeX table\n",
    "latex_table = df_summary.to_latex(index=False, escape=False, column_format='lllll')\n",
    "with open('results/dead neurons/summary_statistics.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "print(\"Saved: summary_statistics.tex\")\n",
    "\n",
    "# Print to console\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL EXPERIMENTS COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(\"  - experiment1_initialization_results.csv\")\n",
    "print(\"  - experiment2_auxiliary_loss_results.csv\")\n",
    "print(\"  - experiment3_combined_effects_results.csv\")\n",
    "print(\"  - figure1_dead_neurons_by_initialization.png/pdf\")\n",
    "print(\"  - figure2_auxiliary_loss_effects.png/pdf\")\n",
    "print(\"  - figure3_combined_effects_summary.png/pdf\")\n",
    "print(\"  - figure4_activation_histograms.png/pdf\")\n",
    "print(\"  - summary_statistics.csv\")\n",
    "print(\"  - summary_statistics.tex\")"
   ],
   "id": "5069d932b358bdb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded experiment results:\n",
      "  • df_init: 364 rows, 10 columns\n",
      "  • df_aux: 14 rows, 9 columns\n",
      "  • df_combined: 12 rows, 8 columns\n",
      "\n",
      "================================================================================\n",
      "GENERATING PUBLICATION-QUALITY FIGURES\n",
      "================================================================================\n",
      "Saved: figure1_dead_neurons_by_initialization.png/pdf\n",
      "Saved: figure2_auxiliary_loss_effects.png/pdf\n",
      "Saved: figure3_combined_effects_summary.png/pdf\n",
      "\n",
      "Generating activation histograms for best/worst models...\n",
      "  Processing mnist...\n",
      "Epoch [1/50], Loss: 0.9370\n",
      "Epoch [2/50], Loss: 0.5011\n",
      "Epoch [3/50], Loss: 0.4658\n",
      "Epoch [4/50], Loss: 0.4542\n",
      "Epoch [5/50], Loss: 0.4495\n",
      "Epoch [6/50], Loss: 0.4475\n",
      "Epoch [7/50], Loss: 0.4462\n",
      "Epoch [8/50], Loss: 0.4449\n",
      "Epoch [9/50], Loss: 0.4433\n",
      "Epoch [10/50], Loss: 0.4422\n",
      "Epoch [11/50], Loss: 0.4415\n",
      "Epoch [12/50], Loss: 0.4408\n",
      "Epoch [13/50], Loss: 0.4402\n",
      "Epoch [14/50], Loss: 0.4394\n",
      "Epoch [15/50], Loss: 0.4387\n",
      "Epoch [16/50], Loss: 0.4379\n",
      "Epoch [17/50], Loss: 0.4372\n",
      "Epoch [18/50], Loss: 0.4369\n",
      "Epoch [19/50], Loss: 0.4364\n",
      "Epoch [20/50], Loss: 0.4361\n",
      "Epoch [21/50], Loss: 0.4359\n",
      "Epoch [22/50], Loss: 0.4357\n",
      "Epoch [23/50], Loss: 0.4355\n",
      "Epoch [24/50], Loss: 0.4352\n",
      "Epoch [25/50], Loss: 0.4350\n",
      "Epoch [26/50], Loss: 0.4348\n",
      "Epoch [27/50], Loss: 0.4345\n",
      "Epoch [28/50], Loss: 0.4342\n",
      "Epoch [29/50], Loss: 0.4341\n",
      "Epoch [30/50], Loss: 0.4340\n",
      "Epoch [31/50], Loss: 0.4339\n",
      "Epoch [32/50], Loss: 0.4340\n",
      "Epoch [33/50], Loss: 0.4340\n",
      "Epoch [34/50], Loss: 0.4341\n",
      "Epoch [35/50], Loss: 0.4341\n",
      "Epoch [36/50], Loss: 0.4339\n",
      "Epoch [37/50], Loss: 0.4339\n",
      "Epoch [38/50], Loss: 0.4339\n",
      "Epoch [39/50], Loss: 0.4338\n",
      "Epoch [40/50], Loss: 0.4337\n",
      "Epoch [41/50], Loss: 0.4337\n",
      "Epoch [42/50], Loss: 0.4337\n",
      "Epoch [43/50], Loss: 0.4336\n",
      "Epoch [44/50], Loss: 0.4335\n",
      "Epoch [45/50], Loss: 0.4334\n",
      "Epoch [46/50], Loss: 0.4335\n",
      "Epoch [47/50], Loss: 0.4334\n",
      "Epoch [48/50], Loss: 0.4334\n",
      "Epoch [49/50], Loss: 0.4333\n",
      "Epoch [50/50], Loss: 0.4334\n",
      "Finished Training\n",
      "Epoch [1/50], MSE Loss: 0.7793, Aux Loss: 0.0000, Total Loss: 0.7793\n",
      "Epoch [2/50], MSE Loss: 0.3920, Aux Loss: 0.0000, Total Loss: 0.3920\n",
      "Epoch [3/50], MSE Loss: 0.3638, Aux Loss: 0.0000, Total Loss: 0.3638\n",
      "Epoch [4/50], MSE Loss: 0.3531, Aux Loss: 0.0000, Total Loss: 0.3531\n",
      "Epoch [5/50], MSE Loss: 0.3534, Aux Loss: 0.1762, Total Loss: 0.5296\n",
      "Epoch [6/50], MSE Loss: 0.3509, Aux Loss: 0.2129, Total Loss: 0.5638\n",
      "Epoch [7/50], MSE Loss: 0.3482, Aux Loss: 0.2037, Total Loss: 0.5519\n",
      "Epoch [8/50], MSE Loss: 0.3465, Aux Loss: 0.1984, Total Loss: 0.5449\n",
      "Epoch [9/50], MSE Loss: 0.3452, Aux Loss: 0.1953, Total Loss: 0.5405\n",
      "Epoch [10/50], MSE Loss: 0.3444, Aux Loss: 0.1936, Total Loss: 0.5380\n",
      "Epoch [11/50], MSE Loss: 0.3437, Aux Loss: 0.1923, Total Loss: 0.5359\n",
      "Epoch [12/50], MSE Loss: 0.3432, Aux Loss: 0.1915, Total Loss: 0.5347\n",
      "Epoch [13/50], MSE Loss: 0.3431, Aux Loss: 0.1908, Total Loss: 0.5339\n",
      "Epoch [14/50], MSE Loss: 0.3430, Aux Loss: 0.1903, Total Loss: 0.5332\n",
      "Epoch [15/50], MSE Loss: 0.3429, Aux Loss: 0.1900, Total Loss: 0.5330\n",
      "Epoch [16/50], MSE Loss: 0.3429, Aux Loss: 0.1913, Total Loss: 0.5342\n",
      "Epoch [17/50], MSE Loss: 0.3432, Aux Loss: 0.1911, Total Loss: 0.5343\n",
      "Epoch [18/50], MSE Loss: 0.3432, Aux Loss: 0.1900, Total Loss: 0.5332\n",
      "Epoch [19/50], MSE Loss: 0.3435, Aux Loss: 0.1891, Total Loss: 0.5326\n",
      "Epoch [20/50], MSE Loss: 0.3439, Aux Loss: 0.1892, Total Loss: 0.5331\n",
      "Epoch [21/50], MSE Loss: 0.3441, Aux Loss: 0.1898, Total Loss: 0.5339\n",
      "Epoch [22/50], MSE Loss: 0.3444, Aux Loss: 0.1887, Total Loss: 0.5331\n",
      "Epoch [23/50], MSE Loss: 0.3447, Aux Loss: 0.1876, Total Loss: 0.5323\n",
      "Epoch [24/50], MSE Loss: 0.3449, Aux Loss: 0.1861, Total Loss: 0.5310\n",
      "Epoch [25/50], MSE Loss: 0.3451, Aux Loss: 0.1851, Total Loss: 0.5302\n",
      "Epoch [26/50], MSE Loss: 0.3453, Aux Loss: 0.1846, Total Loss: 0.5299\n",
      "Epoch [27/50], MSE Loss: 0.3456, Aux Loss: 0.1852, Total Loss: 0.5308\n",
      "Epoch [28/50], MSE Loss: 0.3457, Aux Loss: 0.1843, Total Loss: 0.5299\n",
      "Epoch [29/50], MSE Loss: 0.3456, Aux Loss: 0.1835, Total Loss: 0.5291\n",
      "Epoch [30/50], MSE Loss: 0.3459, Aux Loss: 0.1837, Total Loss: 0.5296\n",
      "Epoch [31/50], MSE Loss: 0.3461, Aux Loss: 0.1836, Total Loss: 0.5297\n",
      "Epoch [32/50], MSE Loss: 0.3462, Aux Loss: 0.1825, Total Loss: 0.5287\n",
      "Epoch [33/50], MSE Loss: 0.3463, Aux Loss: 0.1822, Total Loss: 0.5285\n",
      "Epoch [34/50], MSE Loss: 0.3468, Aux Loss: 0.1837, Total Loss: 0.5304\n",
      "Epoch [35/50], MSE Loss: 0.3468, Aux Loss: 0.1761, Total Loss: 0.5230\n",
      "Epoch [36/50], MSE Loss: 0.3471, Aux Loss: 0.1748, Total Loss: 0.5218\n",
      "Epoch [37/50], MSE Loss: 0.3477, Aux Loss: 0.1741, Total Loss: 0.5218\n",
      "Epoch [38/50], MSE Loss: 0.3472, Aux Loss: 0.1737, Total Loss: 0.5208\n",
      "Epoch [39/50], MSE Loss: 0.3473, Aux Loss: 0.1733, Total Loss: 0.5206\n",
      "Epoch [40/50], MSE Loss: 0.3476, Aux Loss: 0.1729, Total Loss: 0.5206\n",
      "Epoch [41/50], MSE Loss: 0.3478, Aux Loss: 0.1732, Total Loss: 0.5210\n",
      "Epoch [42/50], MSE Loss: 0.3482, Aux Loss: 0.1732, Total Loss: 0.5213\n",
      "Epoch [43/50], MSE Loss: 0.3484, Aux Loss: 0.1732, Total Loss: 0.5216\n",
      "Epoch [44/50], MSE Loss: 0.3487, Aux Loss: 0.1731, Total Loss: 0.5219\n",
      "Epoch [45/50], MSE Loss: 0.3490, Aux Loss: 0.1736, Total Loss: 0.5226\n",
      "Epoch [46/50], MSE Loss: 0.3493, Aux Loss: 0.1735, Total Loss: 0.5229\n",
      "Epoch [47/50], MSE Loss: 0.3496, Aux Loss: 0.1726, Total Loss: 0.5222\n",
      "Epoch [48/50], MSE Loss: 0.3496, Aux Loss: 0.1728, Total Loss: 0.5224\n",
      "Epoch [49/50], MSE Loss: 0.3501, Aux Loss: 0.1729, Total Loss: 0.5229\n",
      "Epoch [50/50], MSE Loss: 0.3505, Aux Loss: 0.1727, Total Loss: 0.5232\n",
      "Finished Training\n",
      "\n",
      "=== Activation Statistics for Default Sparse Autoencoder ===\n",
      "Total features: 128\n",
      "Dead features: 103 (80.47%)\n",
      "Active features: 25 (19.53%)\n",
      "Mean activation frequency: 0.0781\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.153683\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss ===\n",
      "Total features: 128\n",
      "Dead features: 57 (44.53%)\n",
      "Active features: 71 (55.47%)\n",
      "Mean activation frequency: 0.0781\n",
      "Median activation frequency: 0.0464\n",
      "Mean activation strength (when active): 0.168346\n",
      "  Processing olivetti...\n",
      "Epoch [1/50], Loss: 5.4370\n",
      "Epoch [2/50], Loss: 3.2216\n",
      "Epoch [3/50], Loss: 1.9254\n",
      "Epoch [4/50], Loss: 1.2879\n",
      "Epoch [5/50], Loss: 0.9879\n",
      "Epoch [6/50], Loss: 0.8370\n",
      "Epoch [7/50], Loss: 0.7493\n",
      "Epoch [8/50], Loss: 0.6905\n",
      "Epoch [9/50], Loss: 0.6442\n",
      "Epoch [10/50], Loss: 0.6054\n",
      "Epoch [11/50], Loss: 0.5739\n",
      "Epoch [12/50], Loss: 0.5458\n",
      "Epoch [13/50], Loss: 0.5220\n",
      "Epoch [14/50], Loss: 0.5026\n",
      "Epoch [15/50], Loss: 0.4852\n",
      "Epoch [16/50], Loss: 0.4700\n",
      "Epoch [17/50], Loss: 0.4557\n",
      "Epoch [18/50], Loss: 0.4459\n",
      "Epoch [19/50], Loss: 0.4366\n",
      "Epoch [20/50], Loss: 0.4267\n",
      "Epoch [21/50], Loss: 0.4195\n",
      "Epoch [22/50], Loss: 0.4123\n",
      "Epoch [23/50], Loss: 0.4059\n",
      "Epoch [24/50], Loss: 0.4001\n",
      "Epoch [25/50], Loss: 0.3978\n",
      "Epoch [26/50], Loss: 0.3934\n",
      "Epoch [27/50], Loss: 0.3887\n",
      "Epoch [28/50], Loss: 0.3871\n",
      "Epoch [29/50], Loss: 0.3827\n",
      "Epoch [30/50], Loss: 0.3808\n",
      "Epoch [31/50], Loss: 0.3777\n",
      "Epoch [32/50], Loss: 0.3763\n",
      "Epoch [33/50], Loss: 0.3746\n",
      "Epoch [34/50], Loss: 0.3736\n",
      "Epoch [35/50], Loss: 0.3741\n",
      "Epoch [36/50], Loss: 0.3726\n",
      "Epoch [37/50], Loss: 0.3699\n",
      "Epoch [38/50], Loss: 0.3701\n",
      "Epoch [39/50], Loss: 0.3672\n",
      "Epoch [40/50], Loss: 0.3667\n",
      "Epoch [41/50], Loss: 0.3672\n",
      "Epoch [42/50], Loss: 0.3650\n",
      "Epoch [43/50], Loss: 0.3653\n",
      "Epoch [44/50], Loss: 0.3631\n",
      "Epoch [45/50], Loss: 0.3634\n",
      "Epoch [46/50], Loss: 0.3626\n",
      "Epoch [47/50], Loss: 0.3616\n",
      "Epoch [48/50], Loss: 0.3605\n",
      "Epoch [49/50], Loss: 0.3618\n",
      "Epoch [50/50], Loss: 0.3611\n",
      "Finished Training\n",
      "Epoch [1/50], MSE Loss: 4.7098, Aux Loss: 0.0000, Total Loss: 4.7098\n",
      "Epoch [2/50], MSE Loss: 2.4451, Aux Loss: 0.0000, Total Loss: 2.4451\n",
      "Epoch [3/50], MSE Loss: 1.5524, Aux Loss: 0.0000, Total Loss: 1.5524\n",
      "Epoch [4/50], MSE Loss: 1.1020, Aux Loss: 0.0000, Total Loss: 1.1020\n",
      "Epoch [5/50], MSE Loss: 0.8400, Aux Loss: 0.0000, Total Loss: 0.8400\n",
      "Epoch [6/50], MSE Loss: 0.6929, Aux Loss: 0.0000, Total Loss: 0.6929\n",
      "Epoch [7/50], MSE Loss: 0.6010, Aux Loss: 0.0000, Total Loss: 0.6010\n",
      "Epoch [8/50], MSE Loss: 0.5437, Aux Loss: 0.0000, Total Loss: 0.5437\n",
      "Epoch [9/50], MSE Loss: 0.5034, Aux Loss: 0.0000, Total Loss: 0.5034\n",
      "Epoch [10/50], MSE Loss: 0.4700, Aux Loss: 0.0000, Total Loss: 0.4700\n",
      "Epoch [11/50], MSE Loss: 0.4465, Aux Loss: 0.0000, Total Loss: 0.4465\n",
      "Epoch [12/50], MSE Loss: 0.4273, Aux Loss: 0.0000, Total Loss: 0.4273\n",
      "Epoch [13/50], MSE Loss: 0.4117, Aux Loss: 0.0000, Total Loss: 0.4117\n",
      "Epoch [14/50], MSE Loss: 0.3993, Aux Loss: 0.0000, Total Loss: 0.3993\n",
      "Epoch [15/50], MSE Loss: 0.3895, Aux Loss: 0.0000, Total Loss: 0.3895\n",
      "Epoch [16/50], MSE Loss: 0.3802, Aux Loss: 0.0000, Total Loss: 0.3802\n",
      "Epoch [17/50], MSE Loss: 0.3716, Aux Loss: 0.0000, Total Loss: 0.3716\n",
      "Epoch [18/50], MSE Loss: 0.3652, Aux Loss: 0.0000, Total Loss: 0.3652\n",
      "Epoch [19/50], MSE Loss: 0.3574, Aux Loss: 0.0000, Total Loss: 0.3574\n",
      "Epoch [20/50], MSE Loss: 0.3551, Aux Loss: 0.0000, Total Loss: 0.3551\n",
      "Epoch [21/50], MSE Loss: 0.3472, Aux Loss: 0.0000, Total Loss: 0.3472\n",
      "Epoch [22/50], MSE Loss: 0.3439, Aux Loss: 0.0000, Total Loss: 0.3439\n",
      "Epoch [23/50], MSE Loss: 0.3387, Aux Loss: 0.0000, Total Loss: 0.3387\n",
      "Epoch [24/50], MSE Loss: 0.3361, Aux Loss: 0.0000, Total Loss: 0.3361\n",
      "Epoch [25/50], MSE Loss: 0.3309, Aux Loss: 0.0000, Total Loss: 0.3309\n",
      "Epoch [26/50], MSE Loss: 0.3270, Aux Loss: 0.0000, Total Loss: 0.3270\n",
      "Epoch [27/50], MSE Loss: 0.3263, Aux Loss: 0.0000, Total Loss: 0.3263\n",
      "Epoch [28/50], MSE Loss: 0.3225, Aux Loss: 0.0000, Total Loss: 0.3225\n",
      "Epoch [29/50], MSE Loss: 0.3169, Aux Loss: 0.0000, Total Loss: 0.3169\n",
      "Epoch [30/50], MSE Loss: 0.3150, Aux Loss: 0.0000, Total Loss: 0.3150\n",
      "Epoch [31/50], MSE Loss: 0.3131, Aux Loss: 0.0000, Total Loss: 0.3131\n",
      "Epoch [32/50], MSE Loss: 0.3103, Aux Loss: 0.0000, Total Loss: 0.3103\n",
      "Epoch [33/50], MSE Loss: 0.3080, Aux Loss: 0.0000, Total Loss: 0.3080\n",
      "Epoch [34/50], MSE Loss: 0.3058, Aux Loss: 0.0000, Total Loss: 0.3058\n",
      "Epoch [35/50], MSE Loss: 0.3058, Aux Loss: 0.0000, Total Loss: 0.3058\n",
      "Epoch [36/50], MSE Loss: 0.3028, Aux Loss: 0.0000, Total Loss: 0.3028\n",
      "Epoch [37/50], MSE Loss: 0.3024, Aux Loss: 0.0000, Total Loss: 0.3024\n",
      "Epoch [38/50], MSE Loss: 0.3001, Aux Loss: 0.0000, Total Loss: 0.3001\n",
      "Epoch [39/50], MSE Loss: 0.3004, Aux Loss: 0.0000, Total Loss: 0.3004\n",
      "Epoch [40/50], MSE Loss: 0.2976, Aux Loss: 0.0000, Total Loss: 0.2976\n",
      "Epoch [41/50], MSE Loss: 0.2976, Aux Loss: 0.0000, Total Loss: 0.2976\n",
      "Epoch [42/50], MSE Loss: 0.2967, Aux Loss: 0.0000, Total Loss: 0.2967\n",
      "Epoch [43/50], MSE Loss: 0.2940, Aux Loss: 0.0000, Total Loss: 0.2940\n",
      "Epoch [44/50], MSE Loss: 0.2917, Aux Loss: 0.0000, Total Loss: 0.2917\n",
      "Epoch [45/50], MSE Loss: 0.2919, Aux Loss: 0.0000, Total Loss: 0.2919\n",
      "Epoch [46/50], MSE Loss: 0.2920, Aux Loss: 0.0000, Total Loss: 0.2920\n",
      "Epoch [47/50], MSE Loss: 0.2926, Aux Loss: 0.0000, Total Loss: 0.2926\n",
      "Epoch [48/50], MSE Loss: 0.2907, Aux Loss: 0.0000, Total Loss: 0.2907\n",
      "Epoch [49/50], MSE Loss: 0.2908, Aux Loss: 0.0000, Total Loss: 0.2908\n",
      "Epoch [50/50], MSE Loss: 0.2891, Aux Loss: 0.0000, Total Loss: 0.2891\n",
      "Finished Training\n",
      "\n",
      "=== Activation Statistics for Default Sparse Autoencoder ===\n",
      "Total features: 256\n",
      "Dead features: 219 (85.55%)\n",
      "Active features: 37 (14.45%)\n",
      "Mean activation frequency: 0.0391\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.132538\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss ===\n",
      "Total features: 256\n",
      "Dead features: 183 (71.48%)\n",
      "Active features: 73 (28.52%)\n",
      "Mean activation frequency: 0.0391\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.253303\n",
      "  Processing lfw...\n",
      "Original LFW shape: (3023, 125, 94)\n",
      "Resized LFW shape: (3023, 4096)\n",
      "LFW Dataset loaded: 2418 train, 605 test\n",
      "Image size: 64×64, Input dimension: 4096\n",
      "Epoch [1/50], Loss: 1.6715\n",
      "Epoch [2/50], Loss: 0.6771\n",
      "Epoch [3/50], Loss: 0.5395\n",
      "Epoch [4/50], Loss: 0.4880\n",
      "Epoch [5/50], Loss: 0.4676\n",
      "Epoch [6/50], Loss: 0.4567\n",
      "Epoch [7/50], Loss: 0.4515\n",
      "Epoch [8/50], Loss: 0.4468\n",
      "Epoch [9/50], Loss: 0.4444\n",
      "Epoch [10/50], Loss: 0.4410\n",
      "Epoch [11/50], Loss: 0.4397\n",
      "Epoch [12/50], Loss: 0.4394\n",
      "Epoch [13/50], Loss: 0.4373\n",
      "Epoch [14/50], Loss: 0.4367\n",
      "Epoch [15/50], Loss: 0.4360\n",
      "Epoch [16/50], Loss: 0.4352\n",
      "Epoch [17/50], Loss: 0.4358\n",
      "Epoch [18/50], Loss: 0.4350\n",
      "Epoch [19/50], Loss: 0.4343\n",
      "Epoch [20/50], Loss: 0.4345\n",
      "Epoch [21/50], Loss: 0.4343\n",
      "Epoch [22/50], Loss: 0.4341\n",
      "Epoch [23/50], Loss: 0.4341\n",
      "Epoch [24/50], Loss: 0.4344\n",
      "Epoch [25/50], Loss: 0.4335\n",
      "Epoch [26/50], Loss: 0.4343\n",
      "Epoch [27/50], Loss: 0.4350\n",
      "Epoch [28/50], Loss: 0.4341\n",
      "Epoch [29/50], Loss: 0.4352\n",
      "Epoch [30/50], Loss: 0.4348\n",
      "Epoch [31/50], Loss: 0.4351\n",
      "Epoch [32/50], Loss: 0.4341\n",
      "Epoch [33/50], Loss: 0.4339\n",
      "Epoch [34/50], Loss: 0.4339\n",
      "Epoch [35/50], Loss: 0.4337\n",
      "Epoch [36/50], Loss: 0.4334\n",
      "Epoch [37/50], Loss: 0.4337\n",
      "Epoch [38/50], Loss: 0.4336\n",
      "Epoch [39/50], Loss: 0.4342\n",
      "Epoch [40/50], Loss: 0.4332\n",
      "Epoch [41/50], Loss: 0.4335\n",
      "Epoch [42/50], Loss: 0.4333\n",
      "Epoch [43/50], Loss: 0.4334\n",
      "Epoch [44/50], Loss: 0.4332\n",
      "Epoch [45/50], Loss: 0.4332\n",
      "Epoch [46/50], Loss: 0.4328\n",
      "Epoch [47/50], Loss: 0.4330\n",
      "Epoch [48/50], Loss: 0.4333\n",
      "Epoch [49/50], Loss: 0.4332\n",
      "Epoch [50/50], Loss: 0.4337\n",
      "Finished Training\n",
      "Epoch [1/50], MSE Loss: 1.4471, Aux Loss: 0.0000, Total Loss: 1.4471\n",
      "Epoch [2/50], MSE Loss: 0.5777, Aux Loss: 0.0000, Total Loss: 0.5777\n",
      "Epoch [3/50], MSE Loss: 0.4917, Aux Loss: 0.0000, Total Loss: 0.4917\n",
      "Epoch [4/50], MSE Loss: 0.4649, Aux Loss: 0.0000, Total Loss: 0.4649\n",
      "Epoch [5/50], MSE Loss: 0.4512, Aux Loss: 0.0000, Total Loss: 0.4512\n",
      "Epoch [6/50], MSE Loss: 0.4433, Aux Loss: 0.0000, Total Loss: 0.4433\n",
      "Epoch [7/50], MSE Loss: 0.4394, Aux Loss: 0.0000, Total Loss: 0.4394\n",
      "Epoch [8/50], MSE Loss: 0.4332, Aux Loss: 0.0000, Total Loss: 0.4332\n",
      "Epoch [9/50], MSE Loss: 0.4306, Aux Loss: 0.0000, Total Loss: 0.4306\n",
      "Epoch [10/50], MSE Loss: 0.4311, Aux Loss: 0.0000, Total Loss: 0.4311\n",
      "Epoch [11/50], MSE Loss: 0.4301, Aux Loss: 0.0000, Total Loss: 0.4301\n",
      "Epoch [12/50], MSE Loss: 0.4236, Aux Loss: 0.0000, Total Loss: 0.4236\n",
      "Epoch [13/50], MSE Loss: 0.4298, Aux Loss: 0.0000, Total Loss: 0.4298\n",
      "Epoch [14/50], MSE Loss: 0.4358, Aux Loss: 0.0000, Total Loss: 0.4358\n",
      "Epoch [15/50], MSE Loss: 0.4518, Aux Loss: 0.0000, Total Loss: 0.4518\n",
      "Epoch [16/50], MSE Loss: 0.4330, Aux Loss: 0.0000, Total Loss: 0.4330\n",
      "Epoch [17/50], MSE Loss: 0.4297, Aux Loss: 0.0000, Total Loss: 0.4297\n",
      "Epoch [18/50], MSE Loss: 0.4456, Aux Loss: 0.0000, Total Loss: 0.4456\n",
      "Epoch [19/50], MSE Loss: 0.4327, Aux Loss: 0.0000, Total Loss: 0.4327\n",
      "Epoch [20/50], MSE Loss: 0.4274, Aux Loss: 0.0000, Total Loss: 0.4274\n",
      "Epoch [21/50], MSE Loss: 0.4336, Aux Loss: 0.0000, Total Loss: 0.4336\n",
      "Epoch [22/50], MSE Loss: 0.4341, Aux Loss: 0.0000, Total Loss: 0.4341\n",
      "Epoch [23/50], MSE Loss: 0.4420, Aux Loss: 0.0000, Total Loss: 0.4420\n",
      "Epoch [24/50], MSE Loss: 0.4549, Aux Loss: 0.0000, Total Loss: 0.4549\n",
      "Epoch [25/50], MSE Loss: 0.4417, Aux Loss: 0.0000, Total Loss: 0.4417\n",
      "Epoch [26/50], MSE Loss: 0.4528, Aux Loss: 0.0000, Total Loss: 0.4528\n",
      "Epoch [27/50], MSE Loss: 0.4347, Aux Loss: 0.0110, Total Loss: 0.4458\n",
      "Epoch [28/50], MSE Loss: 0.4383, Aux Loss: 0.0161, Total Loss: 0.4544\n",
      "Epoch [29/50], MSE Loss: 0.4501, Aux Loss: 0.0167, Total Loss: 0.4668\n",
      "Epoch [30/50], MSE Loss: 0.4537, Aux Loss: 0.0165, Total Loss: 0.4701\n",
      "Epoch [31/50], MSE Loss: 0.4402, Aux Loss: 0.0161, Total Loss: 0.4563\n",
      "Epoch [32/50], MSE Loss: 0.4410, Aux Loss: 0.0159, Total Loss: 0.4569\n",
      "Epoch [33/50], MSE Loss: 0.4453, Aux Loss: 0.0161, Total Loss: 0.4614\n",
      "Epoch [34/50], MSE Loss: 0.4228, Aux Loss: 0.0155, Total Loss: 0.4382\n",
      "Epoch [35/50], MSE Loss: 0.4500, Aux Loss: 0.0161, Total Loss: 0.4661\n",
      "Epoch [36/50], MSE Loss: 0.4520, Aux Loss: 0.0163, Total Loss: 0.4683\n",
      "Epoch [37/50], MSE Loss: 0.4286, Aux Loss: 0.0157, Total Loss: 0.4443\n",
      "Epoch [38/50], MSE Loss: 0.4430, Aux Loss: 0.0160, Total Loss: 0.4589\n",
      "Epoch [39/50], MSE Loss: 0.4392, Aux Loss: 0.0158, Total Loss: 0.4550\n",
      "Epoch [40/50], MSE Loss: 0.4240, Aux Loss: 0.0156, Total Loss: 0.4396\n",
      "Epoch [41/50], MSE Loss: 0.4232, Aux Loss: 0.0152, Total Loss: 0.4384\n",
      "Epoch [42/50], MSE Loss: 0.4303, Aux Loss: 0.0155, Total Loss: 0.4458\n",
      "Epoch [43/50], MSE Loss: 0.4288, Aux Loss: 0.0156, Total Loss: 0.4444\n",
      "Epoch [44/50], MSE Loss: 0.4509, Aux Loss: 0.0159, Total Loss: 0.4668\n",
      "Epoch [45/50], MSE Loss: 0.4258, Aux Loss: 0.0152, Total Loss: 0.4410\n",
      "Epoch [46/50], MSE Loss: 0.4244, Aux Loss: 0.0157, Total Loss: 0.4401\n",
      "Epoch [47/50], MSE Loss: 0.4149, Aux Loss: 0.0155, Total Loss: 0.4304\n",
      "Epoch [48/50], MSE Loss: 0.4132, Aux Loss: 0.0157, Total Loss: 0.4288\n",
      "Epoch [49/50], MSE Loss: 0.4281, Aux Loss: 0.0163, Total Loss: 0.4444\n",
      "Epoch [50/50], MSE Loss: 0.4232, Aux Loss: 0.0162, Total Loss: 0.4394\n",
      "Finished Training\n",
      "\n",
      "=== Activation Statistics for Default Sparse Autoencoder ===\n",
      "Total features: 512\n",
      "Dead features: 480 (93.75%)\n",
      "Active features: 32 (6.25%)\n",
      "Mean activation frequency: 0.0195\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.109464\n",
      "\n",
      "=== Activation Statistics for Sparse Autoencoder with weight init., JumpReLU and Auxiliary Loss ===\n",
      "Total features: 512\n",
      "Dead features: 413 (80.66%)\n",
      "Active features: 99 (19.34%)\n",
      "Mean activation frequency: 0.0195\n",
      "Median activation frequency: 0.0000\n",
      "Mean activation strength (when active): 0.159873\n",
      "Saved: figure4_activation_histograms.png/pdf\n",
      "\n",
      "================================================================================\n",
      "GENERATING SUMMARY STATISTICS TABLE\n",
      "================================================================================\n",
      "Saved: summary_statistics.csv\n",
      "Saved: summary_statistics.tex\n",
      "\n",
      "Summary Statistics:\n",
      " Dataset       Model    Dead (%)      Test Loss Active Features\n",
      "   MNIST         SAE 80.47 ± nan 0.427059 ± nan              25\n",
      "   MNIST    SAE_Init 41.41 ± nan 0.333827 ± nan              75\n",
      "   MNIST SAE_AuxLoss 79.69 ± nan 0.877357 ± nan              26\n",
      "   MNIST    Complete 43.75 ± nan 0.687165 ± nan              72\n",
      "OLIVETTI         SAE 85.55 ± nan 0.452630 ± nan              37\n",
      "OLIVETTI    SAE_Init 71.48 ± nan 0.420213 ± nan              73\n",
      "OLIVETTI SAE_AuxLoss 88.28 ± nan 0.902022 ± nan              30\n",
      "OLIVETTI    Complete 71.48 ± nan 0.843712 ± nan              73\n",
      "     LFW         SAE 93.75 ± nan 0.449836 ± nan              32\n",
      "     LFW    SAE_Init 80.27 ± nan 0.458363 ± nan             101\n",
      "     LFW SAE_AuxLoss 95.31 ± nan 0.911123 ± nan              24\n",
      "     LFW    Complete 78.12 ± nan 0.891949 ± nan             112\n",
      "\n",
      "================================================================================\n",
      "ALL EXPERIMENTS COMPLETED SUCCESSFULLY\n",
      "================================================================================\n",
      "\n",
      "Generated Files:\n",
      "  - experiment1_initialization_results.csv\n",
      "  - experiment2_auxiliary_loss_results.csv\n",
      "  - experiment3_combined_effects_results.csv\n",
      "  - figure1_dead_neurons_by_initialization.png/pdf\n",
      "  - figure2_auxiliary_loss_effects.png/pdf\n",
      "  - figure3_combined_effects_summary.png/pdf\n",
      "  - figure4_activation_histograms.png/pdf\n",
      "  - summary_statistics.csv\n",
      "  - summary_statistics.tex\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T07:49:17.811976Z",
     "start_time": "2025-10-23T07:37:04.490750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Experiment 4: Effect of K on Reconstruction Quality and Parts-Based Representation\n",
    "Investigates the sparsity-reconstruction tradeoff by varying k_top parameter\n",
    "Includes both quantitative (MSE) and qualitative (interpretability) analysis\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')  # Non-interactive backend for cluster\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.datasets import fetch_olivetti_faces, fetch_lfw_people\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY\n",
    "# ============================================================================\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    \"\"\"Set all random seeds for reproducibility\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL ARCHITECTURES\n",
    "# ============================================================================\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    \"\"\"TopK Sparse Autoencoder\"\"\"\n",
    "\n",
    "    def __init__(self, input_size=784, hidden_size=64, k_top=20):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.k_top = k_top\n",
    "        self.name = f\"TopK-SAE (k={k_top})\"\n",
    "\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        # Initialize with tied weights\n",
    "        nn.init.kaiming_uniform_(self.encoder.weight, a=math.sqrt(5))\n",
    "        with torch.no_grad():\n",
    "            self.decoder.weight.copy_(self.encoder.weight.t())\n",
    "\n",
    "    def _topk_mask(self, activations):\n",
    "        k = max(0, min(self.k_top, activations.size(1)))\n",
    "        _, idx = torch.topk(activations, k, dim=1)\n",
    "        mask = torch.zeros_like(activations)\n",
    "        mask.scatter_(1, idx, 1.0)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        pre_activations = self.encoder(x)\n",
    "        pre_activations = torch.nn.functional.relu(pre_activations)\n",
    "        mask = self._topk_mask(pre_activations)\n",
    "        h = pre_activations * mask\n",
    "        x_hat = self.decoder(h)\n",
    "        return h, x_hat\n",
    "\n",
    "    def compute_loss(self, x, h, x_hat):\n",
    "        recon_loss = torch.mean((x - x_hat) ** 2)\n",
    "        return recon_loss\n",
    "\n",
    "\n",
    "class CompleteAutoencoder(nn.Module):\n",
    "    \"\"\"Dense autoencoder (no sparsity) for comparison\"\"\"\n",
    "\n",
    "    def __init__(self, input_size=784, hidden_size=64):\n",
    "        super(CompleteAutoencoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.name = \"Complete (No Sparsity)\"\n",
    "\n",
    "        self.encoder = nn.Linear(input_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.encoder.weight, a=math.sqrt(5))\n",
    "        with torch.no_grad():\n",
    "            self.decoder.weight.copy_(self.encoder.weight.t())\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.nn.functional.relu(self.encoder(x))\n",
    "        x_hat = self.decoder(h)\n",
    "        return h, x_hat\n",
    "\n",
    "    def compute_loss(self, x, h, x_hat):\n",
    "        return torch.mean((x - x_hat) ** 2)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_mnist_data(batch_size=256):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def load_olivetti_data(batch_size=32):\n",
    "    faces = fetch_olivetti_faces(shuffle=True, random_state=42)\n",
    "    data_tensor = torch.FloatTensor(faces.data)\n",
    "    dataset = TensorDataset(data_tensor)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size],\n",
    "                                     generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def load_lfw_data(batch_size=128, img_size=64):\n",
    "    lfw_people = fetch_lfw_people(min_faces_per_person=20, resize=1.0, color=False)\n",
    "\n",
    "    resized_images = []\n",
    "    for img in lfw_people.images:\n",
    "        pil_img = Image.fromarray((img * 255).astype(np.uint8))\n",
    "        pil_img = pil_img.resize((img_size, img_size), Image.LANCZOS)\n",
    "        resized = np.array(pil_img).astype(np.float32) / 255.0\n",
    "        resized_images.append(resized.flatten())\n",
    "\n",
    "    data_tensor = torch.FloatTensor(np.array(resized_images))\n",
    "\n",
    "    class LFWDataset(Dataset):\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return (self.data[idx],)\n",
    "\n",
    "    dataset = LFWDataset(data_tensor)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size],\n",
    "                                     generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING AND EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "def train_model(model, train_loader, dataset_type, num_epochs=50, learning_rate=0.001):\n",
    "    \"\"\"Train sparse autoencoder\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for data in train_loader:\n",
    "            if dataset_type in ['olivetti', 'lfw']:\n",
    "                inputs, = data\n",
    "            else:\n",
    "                inputs, _ = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            if len(inputs.shape) == 4:\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            h, outputs = model(inputs)\n",
    "            loss = model.compute_loss(inputs, h, outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Enforce non-negativity\n",
    "            with torch.no_grad():\n",
    "                model.encoder.weight.clamp_(0.0)\n",
    "                model.decoder.weight.clamp_(0.0)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'  Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.6f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_reconstruction_metrics(model, test_loader, dataset_type):\n",
    "    \"\"\"Compute MSE and L0 (average sparsity)\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    total_mse = 0.0\n",
    "    total_l0 = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            if dataset_type in ['olivetti', 'lfw']:\n",
    "                inputs, = data\n",
    "            else:\n",
    "                inputs, _ = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            if len(inputs.shape) == 4:\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            h, x_hat = model(inputs)\n",
    "\n",
    "            # MSE\n",
    "            mse = torch.mean((inputs - x_hat) ** 2, dim=1)\n",
    "            total_mse += mse.sum().item()\n",
    "\n",
    "            # L0: number of active features per sample\n",
    "            l0 = (h > 0).sum(dim=1).float()\n",
    "            total_l0 += l0.sum().item()\n",
    "\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_mse = total_mse / total_samples\n",
    "    avg_l0 = total_l0 / total_samples\n",
    "\n",
    "    return avg_mse, avg_l0\n",
    "\n",
    "\n",
    "def compute_explained_variance(model, test_loader, dataset_type):\n",
    "    \"\"\"Compute explained variance (1 - MSE/Var(X))\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    all_inputs = []\n",
    "    all_reconstructions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            if dataset_type in ['olivetti', 'lfw']:\n",
    "                inputs, = data\n",
    "            else:\n",
    "                inputs, _ = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            if len(inputs.shape) == 4:\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            h, x_hat = model(inputs)\n",
    "\n",
    "            all_inputs.append(inputs.cpu())\n",
    "            all_reconstructions.append(x_hat.cpu())\n",
    "\n",
    "    all_inputs = torch.cat(all_inputs, dim=0)\n",
    "    all_reconstructions = torch.cat(all_reconstructions, dim=0)\n",
    "\n",
    "    var_x = torch.var(all_inputs)\n",
    "    mse = torch.mean((all_inputs - all_reconstructions) ** 2)\n",
    "    explained_var = 1.0 - (mse / var_x)\n",
    "\n",
    "    return explained_var.item()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INTERPRETABILITY ANALYSIS\n",
    "# ============================================================================\n",
    "def visualize_decoder_features_grid(models_dict, dataset_name, test_loader, num_features_per_model=9):\n",
    "    \"\"\"\n",
    "    Visualize ACTIVE decoder features for multiple models in a grid\n",
    "    Each row shows features from one model (one K value)\n",
    "    Only shows features that are actually active on test data\n",
    "    \"\"\"\n",
    "    num_models = len(models_dict)\n",
    "\n",
    "    fig, axes = plt.subplots(num_models, num_features_per_model,\n",
    "                             figsize=(num_features_per_model * 2, num_models * 2.5))\n",
    "\n",
    "    if num_models == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    # Determine image shape and dataset type\n",
    "    if dataset_name == 'mnist':\n",
    "        img_shape = (28, 28)\n",
    "        dataset_type = 'mnist'\n",
    "    elif dataset_name == 'olivetti':\n",
    "        img_shape = (64, 64)\n",
    "        dataset_type = 'olivetti'\n",
    "    else:\n",
    "        img_shape = (64, 64)\n",
    "        dataset_type = 'lfw'\n",
    "\n",
    "    # Process each model\n",
    "    for row_idx, (model_name, model) in enumerate(sorted(models_dict.items())):\n",
    "        device = next(model.parameters()).device\n",
    "        model.eval()\n",
    "\n",
    "        # Collect activations on test data to identify active features\n",
    "        all_activations = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                if dataset_type in ['olivetti', 'lfw']:\n",
    "                    inputs, = data\n",
    "                else:\n",
    "                    inputs, _ = data\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                if len(inputs.shape) == 4:\n",
    "                    inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "                h, _ = model(inputs)\n",
    "                all_activations.append(h.cpu())\n",
    "\n",
    "        all_activations = torch.cat(all_activations, dim=0)  # (N_samples, hidden_size)\n",
    "\n",
    "        # Identify active features (activated at least once)\n",
    "        feature_activations = all_activations.sum(dim=0)  # Sum over all samples\n",
    "        active_mask = feature_activations > 0\n",
    "        active_indices = torch.where(active_mask)[0].numpy()\n",
    "\n",
    "        num_active = len(active_indices)\n",
    "\n",
    "        # Get decoder weights\n",
    "        decoder_weights = model.decoder.weight.data.cpu().numpy()  # (input_size, hidden_size)\n",
    "\n",
    "        # Select top active features by activation frequency\n",
    "        if num_active > 0:\n",
    "            active_feature_freqs = feature_activations[active_mask].numpy()\n",
    "            # Sort by activation frequency (most frequently active first)\n",
    "            sorted_active_idx = active_indices[np.argsort(active_feature_freqs)[::-1]]\n",
    "\n",
    "            # Select top features to display\n",
    "            num_to_display = min(num_features_per_model, len(sorted_active_idx))\n",
    "            top_features = sorted_active_idx[:num_to_display]\n",
    "        else:\n",
    "            # Fallback: use highest norm features\n",
    "            feature_norms = np.linalg.norm(decoder_weights, axis=0)\n",
    "            top_features = np.argsort(feature_norms)[-num_features_per_model:][::-1]\n",
    "            num_active = 0\n",
    "\n",
    "        # Visualize selected features\n",
    "        for col_idx in range(num_features_per_model):\n",
    "            ax = axes[row_idx, col_idx]\n",
    "\n",
    "            if col_idx < len(top_features):\n",
    "                feature_idx = top_features[col_idx]\n",
    "                feature_vector = decoder_weights[:, feature_idx]\n",
    "                feature_img = feature_vector.reshape(img_shape)\n",
    "\n",
    "                # Normalize for visualization\n",
    "                feature_img = (feature_img - feature_img.min()) / (feature_img.max() - feature_img.min() + 1e-8)\n",
    "\n",
    "                ax.imshow(feature_img, cmap='gray')\n",
    "                ax.axis('off')\n",
    "\n",
    "                # Add feature index label with activation frequency\n",
    "                activation_freq = feature_activations[feature_idx].item()\n",
    "                ax.set_title(f'F{feature_idx}\\n({activation_freq:.0f}x)',\n",
    "                            fontsize=8, pad=2)\n",
    "            else:\n",
    "                # Empty subplot if not enough active features\n",
    "                ax.axis('off')\n",
    "\n",
    "            # Add row label with k value and active feature count\n",
    "            if col_idx == 0:\n",
    "                if 'k=' in model_name:\n",
    "                    k_val = model_name.split('k=')[1].split(')')[0]\n",
    "                    label_text = f'k={k_val}\\n({num_active} active)'\n",
    "                else:\n",
    "                    label_text = f'Dense\\n({num_active} active)'\n",
    "\n",
    "                ax.text(-0.15, 0.5, label_text,\n",
    "                       transform=ax.transAxes,\n",
    "                       fontsize=11, fontweight='bold',\n",
    "                       rotation=0, va='center', ha='right')\n",
    "\n",
    "    title = f'Active Learned Features Across Sparsity Levels\\n{dataset_name.upper()} Dataset'\n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "    # Add subtitle explaining the visualization\n",
    "    fig.text(0.5, 0.96,\n",
    "            'Each row shows most frequently activated features for given sparsity level\\n' +\n",
    "            'Numbers in parentheses show activation frequency on test set',\n",
    "            ha='center', fontsize=10, style='italic')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def compute_feature_interpretability_metrics(model, test_loader, dataset_type, top_k_examples=100):\n",
    "    \"\"\"\n",
    "    Compute monosemanticity scores for all features\n",
    "    Returns mean and std of monosemanticity across features\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    # Collect all activations\n",
    "    all_activations = []\n",
    "    all_inputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            if dataset_type in ['olivetti', 'lfw']:\n",
    "                inputs, = data\n",
    "            else:\n",
    "                inputs, _ = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            if len(inputs.shape) == 4:\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "            h, _ = model(inputs)\n",
    "\n",
    "            all_activations.append(h.cpu())\n",
    "            all_inputs.append(inputs.cpu())\n",
    "\n",
    "    all_activations = torch.cat(all_activations, dim=0)  # (N, hidden_size)\n",
    "    all_inputs = torch.cat(all_inputs, dim=0)  # (N, input_size)\n",
    "\n",
    "    # For each feature, compute monosemanticity\n",
    "    hidden_size = all_activations.size(1)\n",
    "    monosemanticity_scores = []\n",
    "\n",
    "    active_features = (all_activations.sum(dim=0) > 0).numpy()\n",
    "\n",
    "    for feat_idx in range(hidden_size):\n",
    "        if not active_features[feat_idx]:\n",
    "            continue\n",
    "\n",
    "        # Get activations for this feature\n",
    "        feat_acts = all_activations[:, feat_idx]\n",
    "\n",
    "        # Find top-k activating samples\n",
    "        nonzero_mask = feat_acts > 0\n",
    "        if nonzero_mask.sum() < 10:  # Too few activations\n",
    "            continue\n",
    "\n",
    "        nonzero_acts = feat_acts[nonzero_mask]\n",
    "        nonzero_inputs = all_inputs[nonzero_mask]\n",
    "\n",
    "        if len(nonzero_acts) < top_k_examples:\n",
    "            top_k_actual = len(nonzero_acts)\n",
    "        else:\n",
    "            top_k_actual = top_k_examples\n",
    "\n",
    "        top_k_indices = torch.argsort(nonzero_acts, descending=True)[:top_k_actual]\n",
    "        top_inputs = nonzero_inputs[top_k_indices]\n",
    "\n",
    "        # Compute pairwise cosine similarities\n",
    "        similarities = torch.nn.functional.cosine_similarity(\n",
    "            top_inputs.unsqueeze(1),\n",
    "            top_inputs.unsqueeze(0),\n",
    "            dim=2\n",
    "        )\n",
    "\n",
    "        # Average similarity (excluding diagonal)\n",
    "        mask = ~torch.eye(similarities.size(0), dtype=torch.bool)\n",
    "        avg_similarity = similarities[mask].mean().item()\n",
    "\n",
    "        monosemanticity_scores.append(avg_similarity)\n",
    "\n",
    "    if len(monosemanticity_scores) == 0:\n",
    "        return 0.0, 0.0, 0\n",
    "\n",
    "    return np.mean(monosemanticity_scores), np.std(monosemanticity_scores), len(monosemanticity_scores)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 4: EFFECT OF K ON RECONSTRUCTION AND INTERPRETABILITY\n",
    "# ============================================================================\n",
    "\n",
    "def run_experiment_4():\n",
    "    \"\"\"\n",
    "    Main experiment: Vary K and measure reconstruction quality + interpretability\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXPERIMENT 4: EFFECT OF K ON RECONSTRUCTION AND INTERPRETABILITY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Configuration\n",
    "    datasets = {\n",
    "        'mnist': {'input_size': 784, 'batch_size': 256, 'img_shape': (28, 28)},\n",
    "        'olivetti': {'input_size': 4096, 'batch_size': 32, 'img_shape': (64, 64)},\n",
    "        'lfw': {'input_size': 4096, 'batch_size': 128, 'img_shape': (64, 64)}\n",
    "    }\n",
    "\n",
    "    hidden_size = 256\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # K values to test (sparsity levels from very sparse to dense)\n",
    "    k_values = [5, 10, 20, 40, 64, 128, 256]  # 256 = dense (all features)\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    trained_models = {}\n",
    "\n",
    "    for dataset_name, config in datasets.items():\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"Processing {dataset_name.upper()} Dataset\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "\n",
    "        # Load data\n",
    "        if dataset_name == 'mnist':\n",
    "            train_loader, test_loader = load_mnist_data(config['batch_size'])\n",
    "        elif dataset_name == 'olivetti':\n",
    "            train_loader, test_loader = load_olivetti_data(config['batch_size'])\n",
    "        else:\n",
    "            train_loader, test_loader = load_lfw_data(config['batch_size'])\n",
    "\n",
    "        dataset_models = {}\n",
    "\n",
    "        # Train Complete model (no sparsity baseline)\n",
    "        print(f\"\\nTraining Complete (Dense) Model...\")\n",
    "        set_seeds(42)\n",
    "        complete_model = CompleteAutoencoder(input_size=config['input_size'],\n",
    "                                             hidden_size=hidden_size)\n",
    "        complete_model = train_model(complete_model, train_loader, dataset_name,\n",
    "                                     num_epochs, learning_rate)\n",
    "\n",
    "        mse, l0 = compute_reconstruction_metrics(complete_model, test_loader, dataset_name)\n",
    "        explained_var = compute_explained_variance(complete_model, test_loader, dataset_name)\n",
    "        mono_mean, mono_std, num_active = compute_feature_interpretability_metrics(\n",
    "            complete_model, test_loader, dataset_name\n",
    "        )\n",
    "\n",
    "        results['dataset'].append(dataset_name)\n",
    "        results['k'].append(hidden_size)  # All features active\n",
    "        results['model_type'].append('Complete')\n",
    "        results['mse'].append(mse)\n",
    "        results['l0'].append(l0)\n",
    "        results['explained_variance'].append(explained_var)\n",
    "        results['monosemanticity_mean'].append(mono_mean)\n",
    "        results['monosemanticity_std'].append(mono_std)\n",
    "        results['num_active_features'].append(num_active)\n",
    "\n",
    "        dataset_models['Complete'] = complete_model\n",
    "\n",
    "        print(f\"  MSE: {mse:.6f}, L0: {l0:.2f}, Explained Var: {explained_var:.4f}\")\n",
    "        print(f\"  Monosemanticity: {mono_mean:.4f} ± {mono_std:.4f} ({num_active} features)\")\n",
    "\n",
    "        # Train TopK SAE for each K value\n",
    "        for k in k_values:\n",
    "            if k >= hidden_size:\n",
    "                continue  # Skip if k >= hidden_size\n",
    "\n",
    "            print(f\"\\nTraining TopK-SAE with k={k}...\")\n",
    "            set_seeds(42)\n",
    "\n",
    "            model = SparseAutoencoder(input_size=config['input_size'],\n",
    "                                      hidden_size=hidden_size,\n",
    "                                      k_top=k)\n",
    "            model = train_model(model, train_loader, dataset_name, num_epochs, learning_rate)\n",
    "\n",
    "            # Evaluate\n",
    "            mse, l0 = compute_reconstruction_metrics(model, test_loader, dataset_name)\n",
    "            explained_var = compute_explained_variance(model, test_loader, dataset_name)\n",
    "            mono_mean, mono_std, num_active = compute_feature_interpretability_metrics(\n",
    "                model, test_loader, dataset_name\n",
    "            )\n",
    "\n",
    "            results['dataset'].append(dataset_name)\n",
    "            results['k'].append(k)\n",
    "            results['model_type'].append('TopK-SAE')\n",
    "            results['mse'].append(mse)\n",
    "            results['l0'].append(l0)\n",
    "            results['explained_variance'].append(explained_var)\n",
    "            results['monosemanticity_mean'].append(mono_mean)\n",
    "            results['monosemanticity_std'].append(mono_std)\n",
    "            results['num_active_features'].append(num_active)\n",
    "\n",
    "            dataset_models[f'k={k}'] = model\n",
    "\n",
    "            print(f\"  MSE: {mse:.6f}, L0: {l0:.2f}, Explained Var: {explained_var:.4f}\")\n",
    "            print(f\"  Monosemanticity: {mono_mean:.4f} ± {mono_std:.4f} ({num_active} features)\")\n",
    "\n",
    "        # Store models for visualization\n",
    "        trained_models[dataset_name] = dataset_models\n",
    "\n",
    "    # Save results\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv('experiment4_k_effects_results.csv', index=False)\n",
    "    print(\"\\n✓ Results saved to 'experiment4_k_effects_results.csv'\")\n",
    "\n",
    "    return df, trained_models\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def create_comprehensive_figures(df, trained_models):\n",
    "    \"\"\"Create publication-quality figures\"\"\"\n",
    "\n",
    "    # Figure 1: Sparsity-Reconstruction Tradeoff (L0 vs MSE)\n",
    "    print(\"\\nGenerating Figure 1: Sparsity-Reconstruction Tradeoff...\")\n",
    "    fig1, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for idx, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "        df_dataset = df[df['dataset'] == dataset]\n",
    "\n",
    "        # Plot TopK points\n",
    "        topk_data = df_dataset[df_dataset['model_type'] == 'TopK-SAE']\n",
    "        axes[idx].plot(topk_data['l0'], topk_data['mse'],\n",
    "                       'o-', linewidth=2, markersize=8,\n",
    "                       label='TopK-SAE', color='steelblue')\n",
    "\n",
    "        # Plot Complete point\n",
    "        complete_data = df_dataset[df_dataset['model_type'] == 'Complete']\n",
    "        axes[idx].plot(complete_data['l0'], complete_data['mse'],\n",
    "                       's', markersize=12, label='Dense (No Sparsity)',\n",
    "                       color='coral', zorder=10)\n",
    "\n",
    "        # Annotate K values\n",
    "        for _, row in topk_data.iterrows():\n",
    "            axes[idx].annotate(f\"k={int(row['k'])}\",\n",
    "                               (row['l0'], row['mse']),\n",
    "                               textcoords=\"offset points\",\n",
    "                               xytext=(0, 10), ha='center', fontsize=8)\n",
    "\n",
    "        axes[idx].set_xlabel('Average L0 (Active Features)', fontsize=12)\n",
    "        axes[idx].set_ylabel('MSE (Test)', fontsize=12)\n",
    "        axes[idx].set_title(f'{dataset.upper()}', fontsize=14, fontweight='bold')\n",
    "        axes[idx].legend(fontsize=10)\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig1.savefig('figure_exp4_1_sparsity_reconstruction_tradeoff.png', dpi=300, bbox_inches='tight')\n",
    "    fig1.savefig('figure_exp4_1_sparsity_reconstruction_tradeoff.pdf', bbox_inches='tight')\n",
    "    print(\"  ✓ Saved: figure_exp4_1_sparsity_reconstruction_tradeoff.png/pdf\")\n",
    "\n",
    "    # Figure 2: Explained Variance vs Sparsity\n",
    "    print(\"\\nGenerating Figure 2: Explained Variance vs Sparsity...\")\n",
    "    fig2, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for idx, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "        df_dataset = df[df['dataset'] == dataset]\n",
    "\n",
    "        topk_data = df_dataset[df_dataset['model_type'] == 'TopK-SAE']\n",
    "        axes[idx].plot(topk_data['k'], topk_data['explained_variance'],\n",
    "                       'o-', linewidth=2, markersize=8,\n",
    "                       label='TopK-SAE', color='forestgreen')\n",
    "\n",
    "        complete_data = df_dataset[df_dataset['model_type'] == 'Complete']\n",
    "        axes[idx].axhline(complete_data['explained_variance'].values[0],\n",
    "                          linestyle='--', color='coral', linewidth=2,\n",
    "                          label='Dense Baseline')\n",
    "\n",
    "        axes[idx].set_xlabel('K (Sparsity Level)', fontsize=12)\n",
    "        axes[idx].set_ylabel('Explained Variance', fontsize=12)\n",
    "        axes[idx].set_title(f'{dataset.upper()}', fontsize=14, fontweight='bold')\n",
    "        axes[idx].legend(fontsize=10)\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "        axes[idx].set_xscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig2.savefig('figure_exp4_2_explained_variance.png', dpi=300, bbox_inches='tight')\n",
    "    fig2.savefig('figure_exp4_2_explained_variance.pdf', bbox_inches='tight')\n",
    "    print(\"  ✓ Saved: figure_exp4_2_explained_variance.png/pdf\")\n",
    "\n",
    "    # Figure 3: Monosemanticity vs Sparsity\n",
    "    print(\"\\nGenerating Figure 3: Monosemanticity vs Sparsity...\")\n",
    "    fig3, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    for idx, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "        df_dataset = df[df['dataset'] == dataset]\n",
    "\n",
    "        topk_data = df_dataset[df_dataset['model_type'] == 'TopK-SAE']\n",
    "        axes[idx].errorbar(topk_data['k'], topk_data['monosemanticity_mean'],\n",
    "                           yerr=topk_data['monosemanticity_std'],\n",
    "                           fmt='o-', linewidth=2, markersize=8, capsize=5,\n",
    "                           label='TopK-SAE', color='purple')\n",
    "\n",
    "        complete_data = df_dataset[df_dataset['model_type'] == 'Complete']\n",
    "        axes[idx].axhline(complete_data['monosemanticity_mean'].values[0],\n",
    "                          linestyle='--', color='coral', linewidth=2,\n",
    "                          label='Dense Baseline')\n",
    "\n",
    "        axes[idx].set_xlabel('K (Sparsity Level)', fontsize=12)\n",
    "        axes[idx].set_ylabel('Monosemanticity Score', fontsize=12)\n",
    "        axes[idx].set_title(f'{dataset.upper()}', fontsize=14, fontweight='bold')\n",
    "        axes[idx].legend(fontsize=10)\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "        axes[idx].set_xscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig3.savefig('figure_exp4_3_monosemanticity.png', dpi=300, bbox_inches='tight')\n",
    "    fig3.savefig('figure_exp4_3_monosemanticity.pdf', bbox_inches='tight')\n",
    "    print(\"  ✓ Saved: figure_exp4_3_monosemanticity.png/pdf\")\n",
    "\n",
    "    # Figure 4: Decoder Feature Visualization Grid\n",
    "    # Need to reload test data for visualization\n",
    "    datasets_config = {\n",
    "        'mnist': {'batch_size': 256},\n",
    "        'olivetti': {'batch_size': 32},\n",
    "        'lfw': {'batch_size': 128}\n",
    "    }\n",
    "    print(\"\\nGenerating Figure 4: Decoder Feature Visualizations...\")\n",
    "    for dataset_name, models_dict in trained_models.items():\n",
    "        # Get test_loader for this dataset\n",
    "        if dataset_name == 'mnist':\n",
    "            _, test_loader = load_mnist_data(datasets_config[dataset_name]['batch_size'])\n",
    "        elif dataset_name == 'olivetti':\n",
    "            _, test_loader = load_olivetti_data(datasets_config[dataset_name]['batch_size'])\n",
    "        else:  # lfw\n",
    "            _, test_loader = load_lfw_data(datasets_config[dataset_name]['batch_size'])\n",
    "\n",
    "        # Generate visualization with test data\n",
    "        fig = visualize_decoder_features_grid(models_dict, dataset_name, test_loader,\n",
    "                                              num_features_per_model=9)\n",
    "        fig.savefig(f'figure_exp4_4_features_{dataset_name}.png', dpi=300, bbox_inches='tight')\n",
    "        fig.savefig(f'figure_exp4_4_features_{dataset_name}.pdf', bbox_inches='tight')\n",
    "        print(f\"  ✓ Saved: figure_exp4_4_features_{dataset_name}.png/pdf\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Figure 5: Combined Multi-Panel Summary\n",
    "    print(\"\\nGenerating Figure 5: Comprehensive Summary...\")\n",
    "    fig5 = plt.figure(figsize=(20, 12))\n",
    "    gs = fig5.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    # Row 1: Sparsity-Reconstruction Tradeoff\n",
    "    for col, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "        ax = fig5.add_subplot(gs[0, col])\n",
    "        df_dataset = df[df['dataset'] == dataset]\n",
    "        topk_data = df_dataset[df_dataset['model_type'] == 'TopK-SAE']\n",
    "        complete_data = df_dataset[df_dataset['model_type'] == 'Complete']\n",
    "\n",
    "        ax.plot(topk_data['l0'], topk_data['mse'], 'o-', linewidth=2, markersize=8)\n",
    "        ax.plot(complete_data['l0'], complete_data['mse'], 's', markersize=12, color='coral')\n",
    "        ax.set_xlabel('L0 (Active Features)', fontsize=10)\n",
    "        ax.set_ylabel('MSE', fontsize=10)\n",
    "        ax.set_title(f'{dataset.upper()}: MSE vs L0', fontsize=11, fontweight='bold')\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    # Row 2: Monosemanticity\n",
    "    for col, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "        ax = fig5.add_subplot(gs[1, col])\n",
    "        df_dataset = df[df['dataset'] == dataset]\n",
    "        topk_data = df_dataset[df_dataset['model_type'] == 'TopK-SAE']\n",
    "\n",
    "        ax.plot(topk_data['k'], topk_data['monosemanticity_mean'],\n",
    "                'o-', linewidth=2, markersize=8, color='purple')\n",
    "        ax.set_xlabel('K (Sparsity)', fontsize=10)\n",
    "        ax.set_ylabel('Monosemanticity', fontsize=10)\n",
    "        ax.set_title(f'{dataset.upper()}: Interpretability', fontsize=11, fontweight='bold')\n",
    "        ax.set_xscale('log')\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    # Row 3: Reconstruction Quality vs Interpretability Trade-off\n",
    "    for col, dataset in enumerate(['mnist', 'olivetti', 'lfw']):\n",
    "        ax = fig5.add_subplot(gs[2, col])\n",
    "        df_dataset = df[df['dataset'] == dataset]\n",
    "        topk_data = df_dataset[df_dataset['model_type'] == 'TopK-SAE']\n",
    "\n",
    "        scatter = ax.scatter(topk_data['mse'], topk_data['monosemanticity_mean'],\n",
    "                             c=topk_data['k'], cmap='viridis', s=100, alpha=0.7)\n",
    "\n",
    "        # Annotate k values\n",
    "        for _, row in topk_data.iterrows():\n",
    "            ax.annotate(f\"{int(row['k'])}\", (row['mse'], row['monosemanticity_mean']),\n",
    "                        fontsize=8, ha='center')\n",
    "\n",
    "        ax.set_xlabel('MSE (Lower = Better)', fontsize=10)\n",
    "        ax.set_ylabel('Monosemanticity (Higher = Better)', fontsize=10)\n",
    "        ax.set_title(f'{dataset.upper()}: Quality vs Interpretability', fontsize=11, fontweight='bold')\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "        if col == 2:\n",
    "            cbar = plt.colorbar(scatter, ax=ax)\n",
    "            cbar.set_label('K (Sparsity)', fontsize=10)\n",
    "\n",
    "    plt.suptitle('Effect of K on Reconstruction and Interpretability',\n",
    "                 fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "    fig5.savefig('figure_exp4_5_comprehensive_summary.png', dpi=300, bbox_inches='tight')\n",
    "    fig5.savefig('figure_exp4_5_comprehensive_summary.pdf', bbox_inches='tight')\n",
    "    print(\"  ✓ Saved: figure_exp4_5_comprehensive_summary.png/pdf\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SPARSE AUTOENCODER: EFFECT OF K ON RECONSTRUCTION AND INTERPRETABILITY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Run experiment\n",
    "    df, trained_models = run_experiment_4()\n",
    "\n",
    "    # Create figures\n",
    "    create_comprehensive_figures(df, trained_models)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPERIMENT 4 COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"- experiment4_k_effects_results.csv\")\n",
    "    print(\"- figure_exp4_1_sparsity_reconstruction_tradeoff.png/pdf\")\n",
    "    print(\"- figure_exp4_2_explained_variance.png/pdf\")\n",
    "    print(\"- figure_exp4_3_monosemanticity.png/pdf\")\n",
    "    print(\"- figure_exp4_4_features_mnist.png/pdf\")\n",
    "    print(\"- figure_exp4_4_features_olivetti.png/pdf\")\n",
    "    print(\"- figure_exp4_4_features_lfw.png/pdf\")\n",
    "    print(\"- figure_exp4_5_comprehensive_summary.png/pdf\")\n",
    "    print(\"=\" * 80)\n"
   ],
   "id": "49dba47fc809f3d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPARSE AUTOENCODER: EFFECT OF K ON RECONSTRUCTION AND INTERPRETABILITY\n",
      "================================================================================\n",
      "================================================================================\n",
      "EXPERIMENT 4: EFFECT OF K ON RECONSTRUCTION AND INTERPRETABILITY\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Processing MNIST Dataset\n",
      "================================================================================\n",
      "\n",
      "Training Complete (Dense) Model...\n",
      "  Epoch [10/50], Loss: 0.012615\n",
      "  Epoch [20/50], Loss: 0.005401\n",
      "  Epoch [30/50], Loss: 0.003446\n",
      "  Epoch [40/50], Loss: 0.002913\n",
      "  Epoch [50/50], Loss: 0.002793\n",
      "  MSE: 0.002707, L0: 115.97, Explained Var: 0.9719\n",
      "  Monosemanticity: 0.5400 ± 0.0489 (256 features)\n",
      "\n",
      "Training TopK-SAE with k=5...\n",
      "  Epoch [10/50], Loss: 0.027739\n",
      "  Epoch [20/50], Loss: 0.027263\n",
      "  Epoch [30/50], Loss: 0.027189\n",
      "  Epoch [40/50], Loss: 0.027192\n",
      "  Epoch [50/50], Loss: 0.026843\n",
      "  MSE: 0.028696, L0: 5.00, Explained Var: 0.7023\n",
      "  Monosemanticity: 0.6754 ± 0.0850 (84 features)\n",
      "\n",
      "Training TopK-SAE with k=10...\n",
      "  Epoch [10/50], Loss: 0.020553\n",
      "  Epoch [20/50], Loss: 0.020176\n",
      "  Epoch [30/50], Loss: 0.019982\n",
      "  Epoch [40/50], Loss: 0.019903\n",
      "  Epoch [50/50], Loss: 0.019854\n",
      "  MSE: 0.019862, L0: 10.00, Explained Var: 0.7940\n",
      "  Monosemanticity: 0.6576 ± 0.0722 (139 features)\n",
      "\n",
      "Training TopK-SAE with k=20...\n",
      "  Epoch [10/50], Loss: 0.015934\n",
      "  Epoch [20/50], Loss: 0.015287\n",
      "  Epoch [30/50], Loss: 0.015202\n",
      "  Epoch [40/50], Loss: 0.015166\n",
      "  Epoch [50/50], Loss: 0.015207\n",
      "  MSE: 0.015166, L0: 20.00, Explained Var: 0.8427\n",
      "  Monosemanticity: 0.6317 ± 0.0794 (168 features)\n",
      "\n",
      "Training TopK-SAE with k=40...\n",
      "  Epoch [10/50], Loss: 0.010127\n",
      "  Epoch [20/50], Loss: 0.009719\n",
      "  Epoch [30/50], Loss: 0.009532\n",
      "  Epoch [40/50], Loss: 0.009428\n",
      "  Epoch [50/50], Loss: 0.009413\n",
      "  MSE: 0.009393, L0: 40.00, Explained Var: 0.9026\n",
      "  Monosemanticity: 0.5915 ± 0.0804 (246 features)\n",
      "\n",
      "Training TopK-SAE with k=64...\n",
      "  Epoch [10/50], Loss: 0.007758\n",
      "  Epoch [20/50], Loss: 0.006428\n",
      "  Epoch [30/50], Loss: 0.006141\n",
      "  Epoch [40/50], Loss: 0.006038\n",
      "  Epoch [50/50], Loss: 0.005964\n",
      "  MSE: 0.005985, L0: 64.00, Explained Var: 0.9379\n",
      "  Monosemanticity: 0.5833 ± 0.0686 (256 features)\n",
      "\n",
      "Training TopK-SAE with k=128...\n",
      "  Epoch [10/50], Loss: 0.007626\n",
      "  Epoch [20/50], Loss: 0.004234\n",
      "  Epoch [30/50], Loss: 0.003237\n",
      "  Epoch [40/50], Loss: 0.003118\n",
      "  Epoch [50/50], Loss: 0.003053\n",
      "  MSE: 0.003003, L0: 124.61, Explained Var: 0.9689\n",
      "  Monosemanticity: 0.5561 ± 0.0535 (256 features)\n",
      "\n",
      "================================================================================\n",
      "Processing OLIVETTI Dataset\n",
      "================================================================================\n",
      "\n",
      "Training Complete (Dense) Model...\n",
      "  Epoch [10/50], Loss: 0.021446\n",
      "  Epoch [20/50], Loss: 0.015097\n",
      "  Epoch [30/50], Loss: 0.015102\n",
      "  Epoch [40/50], Loss: 0.015092\n",
      "  Epoch [50/50], Loss: 0.015091\n",
      "  MSE: 0.015132, L0: 256.00, Explained Var: 0.4841\n",
      "  Monosemanticity: 0.9510 ± 0.0000 (256 features)\n",
      "\n",
      "Training TopK-SAE with k=5...\n",
      "  Epoch [10/50], Loss: 0.015555\n",
      "  Epoch [20/50], Loss: 0.013929\n",
      "  Epoch [30/50], Loss: 0.012804\n",
      "  Epoch [40/50], Loss: 0.011624\n",
      "  Epoch [50/50], Loss: 0.011158\n",
      "  MSE: 0.011335, L0: 5.00, Explained Var: 0.6136\n",
      "  Monosemanticity: 0.9511 ± 0.0003 (5 features)\n",
      "\n",
      "Training TopK-SAE with k=10...\n",
      "  Epoch [10/50], Loss: 0.015114\n",
      "  Epoch [20/50], Loss: 0.013149\n",
      "  Epoch [30/50], Loss: 0.011548\n",
      "  Epoch [40/50], Loss: 0.010681\n",
      "  Epoch [50/50], Loss: 0.010205\n",
      "  MSE: 0.010477, L0: 10.00, Explained Var: 0.6428\n",
      "  Monosemanticity: 0.9521 ± 0.0020 (12 features)\n",
      "\n",
      "Training TopK-SAE with k=20...\n",
      "  Epoch [10/50], Loss: 0.012632\n",
      "  Epoch [20/50], Loss: 0.011025\n",
      "  Epoch [30/50], Loss: 0.010113\n",
      "  Epoch [40/50], Loss: 0.009167\n",
      "  Epoch [50/50], Loss: 0.008680\n",
      "  MSE: 0.008816, L0: 20.00, Explained Var: 0.6994\n",
      "  Monosemanticity: 0.9515 ± 0.0028 (24 features)\n",
      "\n",
      "Training TopK-SAE with k=40...\n",
      "  Epoch [10/50], Loss: 0.014520\n",
      "  Epoch [20/50], Loss: 0.011712\n",
      "  Epoch [30/50], Loss: 0.010800\n",
      "  Epoch [40/50], Loss: 0.009913\n",
      "  Epoch [50/50], Loss: 0.008734\n",
      "  MSE: 0.008847, L0: 40.00, Explained Var: 0.6984\n",
      "  Monosemanticity: 0.9513 ± 0.0021 (49 features)\n",
      "\n",
      "Training TopK-SAE with k=64...\n",
      "  Epoch [10/50], Loss: 0.015133\n",
      "  Epoch [20/50], Loss: 0.015057\n",
      "  Epoch [30/50], Loss: 0.014315\n",
      "  Epoch [40/50], Loss: 0.013438\n",
      "  Epoch [50/50], Loss: 0.012904\n",
      "  MSE: 0.013019, L0: 64.00, Explained Var: 0.5562\n",
      "  Monosemanticity: 0.9510 ± 0.0000 (64 features)\n",
      "\n",
      "Training TopK-SAE with k=128...\n",
      "  Epoch [10/50], Loss: 0.015221\n",
      "  Epoch [20/50], Loss: 0.015149\n",
      "  Epoch [30/50], Loss: 0.015182\n",
      "  Epoch [40/50], Loss: 0.015049\n",
      "  Epoch [50/50], Loss: 0.014731\n",
      "  MSE: 0.014770, L0: 128.00, Explained Var: 0.4965\n",
      "  Monosemanticity: 0.9510 ± 0.0000 (128 features)\n",
      "\n",
      "================================================================================\n",
      "Processing LFW Dataset\n",
      "================================================================================\n",
      "\n",
      "Training Complete (Dense) Model...\n",
      "  Epoch [10/50], Loss: 0.019514\n",
      "  Epoch [20/50], Loss: 0.019512\n",
      "  Epoch [30/50], Loss: 0.019501\n",
      "  Epoch [40/50], Loss: 0.019481\n",
      "  Epoch [50/50], Loss: 0.019374\n",
      "  MSE: 0.019553, L0: 256.00, Explained Var: 0.4172\n",
      "  Monosemanticity: 0.9502 ± 0.0006 (256 features)\n",
      "\n",
      "Training TopK-SAE with k=5...\n",
      "  Epoch [10/50], Loss: 0.017124\n",
      "  Epoch [20/50], Loss: 0.014694\n",
      "  Epoch [30/50], Loss: 0.013652\n",
      "  Epoch [40/50], Loss: 0.013976\n",
      "  Epoch [50/50], Loss: 0.013563\n",
      "  MSE: 0.013852, L0: 5.00, Explained Var: 0.5871\n",
      "  Monosemanticity: 0.9374 ± 0.0124 (7 features)\n",
      "\n",
      "Training TopK-SAE with k=10...\n",
      "  Epoch [10/50], Loss: 0.014988\n",
      "  Epoch [20/50], Loss: 0.012848\n",
      "  Epoch [30/50], Loss: 0.012109\n",
      "  Epoch [40/50], Loss: 0.011904\n",
      "  Epoch [50/50], Loss: 0.011482\n",
      "  MSE: 0.011721, L0: 10.00, Explained Var: 0.6506\n",
      "  Monosemanticity: 0.9348 ± 0.0104 (17 features)\n",
      "\n",
      "Training TopK-SAE with k=20...\n",
      "  Epoch [10/50], Loss: 0.012752\n",
      "  Epoch [20/50], Loss: 0.011977\n",
      "  Epoch [30/50], Loss: 0.011037\n",
      "  Epoch [40/50], Loss: 0.010242\n",
      "  Epoch [50/50], Loss: 0.009806\n",
      "  MSE: 0.010094, L0: 20.00, Explained Var: 0.6991\n",
      "  Monosemanticity: 0.9304 ± 0.0182 (41 features)\n",
      "\n",
      "Training TopK-SAE with k=40...\n",
      "  Epoch [10/50], Loss: 0.015652\n",
      "  Epoch [20/50], Loss: 0.012581\n",
      "  Epoch [30/50], Loss: 0.011765\n",
      "  Epoch [40/50], Loss: 0.011530\n",
      "  Epoch [50/50], Loss: 0.011068\n",
      "  MSE: 0.011148, L0: 40.00, Explained Var: 0.6677\n",
      "  Monosemanticity: 0.9350 ± 0.0221 (52 features)\n",
      "\n",
      "Training TopK-SAE with k=64...\n",
      "  Epoch [10/50], Loss: 0.017855\n",
      "  Epoch [20/50], Loss: 0.014984\n",
      "  Epoch [30/50], Loss: 0.013659\n",
      "  Epoch [40/50], Loss: 0.012313\n",
      "  Epoch [50/50], Loss: 0.010907\n",
      "  MSE: 0.011085, L0: 64.00, Explained Var: 0.6696\n",
      "  Monosemanticity: 0.9450 ± 0.0071 (73 features)\n",
      "\n",
      "Training TopK-SAE with k=128...\n",
      "  Epoch [10/50], Loss: 0.019501\n",
      "  Epoch [20/50], Loss: 0.017780\n",
      "  Epoch [30/50], Loss: 0.015319\n",
      "  Epoch [40/50], Loss: 0.014065\n",
      "  Epoch [50/50], Loss: 0.012796\n",
      "  MSE: 0.012901, L0: 128.00, Explained Var: 0.6155\n",
      "  Monosemanticity: 0.9434 ± 0.0251 (141 features)\n",
      "\n",
      "✓ Results saved to 'experiment4_k_effects_results.csv'\n",
      "\n",
      "Generating Figure 1: Sparsity-Reconstruction Tradeoff...\n",
      "  ✓ Saved: figure_exp4_1_sparsity_reconstruction_tradeoff.png/pdf\n",
      "\n",
      "Generating Figure 2: Explained Variance vs Sparsity...\n",
      "  ✓ Saved: figure_exp4_2_explained_variance.png/pdf\n",
      "\n",
      "Generating Figure 3: Monosemanticity vs Sparsity...\n",
      "  ✓ Saved: figure_exp4_3_monosemanticity.png/pdf\n",
      "\n",
      "Generating Figure 4: Decoder Feature Visualizations...\n",
      "  ✓ Saved: figure_exp4_4_features_mnist.png/pdf\n",
      "  ✓ Saved: figure_exp4_4_features_olivetti.png/pdf\n",
      "  ✓ Saved: figure_exp4_4_features_lfw.png/pdf\n",
      "\n",
      "Generating Figure 5: Comprehensive Summary...\n",
      "  ✓ Saved: figure_exp4_5_comprehensive_summary.png/pdf\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 4 COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "- experiment4_k_effects_results.csv\n",
      "- figure_exp4_1_sparsity_reconstruction_tradeoff.png/pdf\n",
      "- figure_exp4_2_explained_variance.png/pdf\n",
      "- figure_exp4_3_monosemanticity.png/pdf\n",
      "- figure_exp4_4_features_mnist.png/pdf\n",
      "- figure_exp4_4_features_olivetti.png/pdf\n",
      "- figure_exp4_4_features_lfw.png/pdf\n",
      "- figure_exp4_5_comprehensive_summary.png/pdf\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
